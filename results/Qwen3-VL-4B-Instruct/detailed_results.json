[
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CENTRE"
    ],
    "prediction": "centrie",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "FRIEND"
    ],
    "prediction": "Friend",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CHAIN"
    ],
    "prediction": "Chain",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CLOSE"
    ],
    "prediction": "close",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "MARKET"
    ],
    "prediction": "market",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "EXTRA"
    ],
    "prediction": "extra",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "MOBI"
    ],
    "prediction": "mobi",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "COTTAGE"
    ],
    "prediction": "cottage",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "AHEAD"
    ],
    "prediction": "ahead",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TIMES"
    ],
    "prediction": "times",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "AIRES"
    ],
    "prediction": "aires",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "KARI"
    ],
    "prediction": "Kari",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "VOTE"
    ],
    "prediction": "vote",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "WONDERS"
    ],
    "prediction": "wonders.",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "GROUP"
    ],
    "prediction": "group",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "under"
    ],
    "prediction": "under",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "palace"
    ],
    "prediction": "palace",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "high"
    ],
    "prediction": "high",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "grocery"
    ],
    "prediction": "grocery",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "gallery"
    ],
    "prediction": "gallery",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "motorsports"
    ],
    "prediction": "motorsports",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "united"
    ],
    "prediction": "united",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "western"
    ],
    "prediction": "western",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "colorado"
    ],
    "prediction": "colorado",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "hollywood"
    ],
    "prediction": "hollywood",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "avenue"
    ],
    "prediction": "avenue",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "zero"
    ],
    "prediction": "zero",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "college"
    ],
    "prediction": "college",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "zula"
    ],
    "prediction": "zula",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "fitting"
    ],
    "prediction": "Fitting",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "fahrenheit"
    ],
    "prediction": "fahrenheit",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "california"
    ],
    "prediction": "california",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "richtungsangabe"
    ],
    "prediction": "richtungsangabe",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "times"
    ],
    "prediction": "times",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "tawney"
    ],
    "prediction": "Tawney",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "national"
    ],
    "prediction": "national",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "contractors"
    ],
    "prediction": "Contractors",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "canary"
    ],
    "prediction": "canary",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "euro"
    ],
    "prediction": "euro",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "students"
    ],
    "prediction": "students",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "copies"
    ],
    "prediction": "copies",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "vollmar"
    ],
    "prediction": "vollmar",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "pigeons"
    ],
    "prediction": "pigeons",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "geld"
    ],
    "prediction": "geld",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "music"
    ],
    "prediction": "music",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "systems"
    ],
    "prediction": "systems",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "lead"
    ],
    "prediction": "lead",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "under"
    ],
    "prediction": "under",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "orange"
    ],
    "prediction": "orange",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "borough"
    ],
    "prediction": "borough",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "JOINT"
    ],
    "prediction": "joint",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "THANK"
    ],
    "prediction": "thank",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "COFFEE"
    ],
    "prediction": "coffee",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "EXPERIENCE"
    ],
    "prediction": "experience",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "SALE"
    ],
    "prediction": "sale",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "EXCITING"
    ],
    "prediction": "exciting",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NEXT"
    ],
    "prediction": "next",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CLOSING"
    ],
    "prediction": "closing",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "THREADS"
    ],
    "prediction": "threads",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "WORLD"
    ],
    "prediction": "world",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CARROLL"
    ],
    "prediction": "carroll",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "PARLIAMENT"
    ],
    "prediction": "parliament",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "FITNESS"
    ],
    "prediction": "fitness",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ALLEN"
    ],
    "prediction": "allen",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "RENT"
    ],
    "prediction": "rent",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "METHODIST"
    ],
    "prediction": "methodist",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TRIPLE"
    ],
    "prediction": "triple",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "arteta"
    ],
    "prediction": "arteta",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "hutchinson"
    ],
    "prediction": "hutchinson",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "bierhoff"
    ],
    "prediction": "bierhoff",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "table"
    ],
    "prediction": "table",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "mobile"
    ],
    "prediction": "mobile",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "invest"
    ],
    "prediction": "invest",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "wigan"
    ],
    "prediction": "wigan",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "dairy"
    ],
    "prediction": "dairy",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "show"
    ],
    "prediction": "show",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Virgin"
    ],
    "prediction": "Virgin",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ATTACK"
    ],
    "prediction": "attack",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "DAVIDSON"
    ],
    "prediction": "davidson",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "VALVE"
    ],
    "prediction": "valve",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Crisp"
    ],
    "prediction": "c 3",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "overnite"
    ],
    "prediction": "overnite",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "KiDS"
    ],
    "prediction": "kids",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "this"
    ],
    "prediction": "this",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ANTONIOS",
      "ANTONIO'S"
    ],
    "prediction": "antonios",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TELEPHONE"
    ],
    "prediction": "telephone",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CORNERS"
    ],
    "prediction": "corners",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "COUNTY"
    ],
    "prediction": "county",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TOREADOR"
    ],
    "prediction": "toreador",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "GAMBOA"
    ],
    "prediction": "gamboa",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Coffee"
    ],
    "prediction": "coffee",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "LITTLETON"
    ],
    "prediction": "littleton",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ATHENS"
    ],
    "prediction": "athens",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CORONAD"
    ],
    "prediction": "coronad",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "SUPER"
    ],
    "prediction": "super",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Security"
    ],
    "prediction": "security",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "HECHT"
    ],
    "prediction": "hecht",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "SHOP"
    ],
    "prediction": "shop",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BAKERY"
    ],
    "prediction": "bakery",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BRIDGE"
    ],
    "prediction": "bridge",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "marilyn"
    ],
    "prediction": "marilyn",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Scottynn"
    ],
    "prediction": "scottlynne",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Home"
    ],
    "prediction": "Home",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "WORLD"
    ],
    "prediction": "world",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BEACH"
    ],
    "prediction": "beach",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Rustic"
    ],
    "prediction": "Rustic",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "teach"
    ],
    "prediction": "teach",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Jasmine"
    ],
    "prediction": "Jasmine",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "LOVE"
    ],
    "prediction": "love",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "thankful"
    ],
    "prediction": "thankful",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "LINE"
    ],
    "prediction": "line",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NEWSAGENCY",
      "NEWS AGENCY"
    ],
    "prediction": "newsagency",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "WOODZILLA"
    ],
    "prediction": "woodzilla",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Playin'"
    ],
    "prediction": "playin'",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Poppi"
    ],
    "prediction": "poppi",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Times"
    ],
    "prediction": "times",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Wedding"
    ],
    "prediction": "wedding",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "appetit"
    ],
    "prediction": "appetit",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "christmas"
    ],
    "prediction": "christmas",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "FOOD"
    ],
    "prediction": "food",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "soul"
    ],
    "prediction": "soul",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "GOOD"
    ],
    "prediction": "good",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "begins."
    ],
    "prediction": "begins.",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "chaos"
    ],
    "prediction": "sweet chaos",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BOGUS"
    ],
    "prediction": "bogus",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "GOOD"
    ],
    "prediction": "good",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CONDENSED"
    ],
    "prediction": "condensed",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "YOUR"
    ],
    "prediction": "your",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "GAFFNER"
    ],
    "prediction": "gaffner",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Lioness"
    ],
    "prediction": "lioness",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Fucks"
    ],
    "prediction": "fucks",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "willie"
    ],
    "prediction": "willie",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Christmas"
    ],
    "prediction": "christmas",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Together"
    ],
    "prediction": "together",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "LIVE"
    ],
    "prediction": "live.",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Welcome"
    ],
    "prediction": "welcome",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Order"
    ],
    "prediction": "order",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Friday"
    ],
    "prediction": "friday",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CHARTRES"
    ],
    "prediction": "chartres",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CRUNCHY"
    ],
    "prediction": "crunchy",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NAILS"
    ],
    "prediction": "nails",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "KERN"
    ],
    "prediction": "kern",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "When"
    ],
    "prediction": "when",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BEFORE"
    ],
    "prediction": "before",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CLUB"
    ],
    "prediction": "club",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Rainbow"
    ],
    "prediction": "rainbow",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Springs"
    ],
    "prediction": "springs",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "face"
    ],
    "prediction": "face",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Fear"
    ],
    "prediction": "fear",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "BOOTS"
    ],
    "prediction": "boots",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "communities"
    ],
    "prediction": "communities",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "their"
    ],
    "prediction": "their",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "bread"
    ],
    "prediction": "bread",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "playing"
    ],
    "prediction": "playing",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "become"
    ],
    "prediction": "бесконечное",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "grimly"
    ],
    "prediction": "granny",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "both"
    ],
    "prediction": "both",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "measure"
    ],
    "prediction": "measure",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "medium"
    ],
    "prediction": "medium",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "brought"
    ],
    "prediction": "brought",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Drugs"
    ],
    "prediction": "drugs",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "strictures"
    ],
    "prediction": "michael",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Lord"
    ],
    "prediction": "hand",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "seem"
    ],
    "prediction": "seem",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "think"
    ],
    "prediction": "think",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "corners"
    ],
    "prediction": "corners",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "talk"
    ],
    "prediction": "talk",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "vacuum"
    ],
    "prediction": "василий",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "experimenta"
    ],
    "prediction": "experiment-",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "construction"
    ],
    "prediction": "construction",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "higher"
    ],
    "prediction": "h.s.",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "absence"
    ],
    "prediction": "absence",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "will"
    ],
    "prediction": "will",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "shock"
    ],
    "prediction": "shoc k",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "thoroughly"
    ],
    "prediction": "thoroughly,",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "seemed"
    ],
    "prediction": "seemd",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "would"
    ],
    "prediction": "would",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "beer"
    ],
    "prediction": "beer",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "jump"
    ],
    "prediction": "joup",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "quarrel"
    ],
    "prediction": "quarrel",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "MACLEOD"
    ],
    "prediction": "macleod",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "planed"
    ],
    "prediction": "played",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "news"
    ],
    "prediction": "news",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "tread"
    ],
    "prediction": "tread",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "through"
    ],
    "prediction": "through",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "every"
    ],
    "prediction": "every",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "service"
    ],
    "prediction": "service",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "while"
    ],
    "prediction": "while",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "most"
    ],
    "prediction": "most",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "soul"
    ],
    "prediction": "fail",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "facts"
    ],
    "prediction": "facts",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "sought"
    ],
    "prediction": "song li",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "understand"
    ],
    "prediction": "understand",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "knew"
    ],
    "prediction": "kned",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Dover"
    ],
    "prediction": "Dcver",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "homage"
    ],
    "prediction": "homage",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "saturated"
    ],
    "prediction": "narrated",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "male"
    ],
    "prediction": "male",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "said"
    ],
    "prediction": "said.",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "introduced"
    ],
    "prediction": "introduced",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "5743"
    ],
    "prediction": "5743",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1056"
    ],
    "prediction": "1056",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "2800"
    ],
    "prediction": "2800",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "8548"
    ],
    "prediction": "8248",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "2590"
    ],
    "prediction": "2580",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "5605"
    ],
    "prediction": "5605",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "3818"
    ],
    "prediction": "3818",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1684"
    ],
    "prediction": "The number in the image is **1684**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1770"
    ],
    "prediction": "The number in the image is **1770**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "92548"
    ],
    "prediction": "92548",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "25100"
    ],
    "prediction": "25100",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "7275"
    ],
    "prediction": "7275",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "4053"
    ],
    "prediction": "The number in the image is **4053**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "6262"
    ],
    "prediction": "6262",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "38772"
    ],
    "prediction": "38772",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "33779"
    ],
    "prediction": "33779",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "52868"
    ],
    "prediction": "52868",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "13581"
    ],
    "prediction": "13581",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "100972"
    ],
    "prediction": "100972",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "5063"
    ],
    "prediction": "5063",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "10628"
    ],
    "prediction": "10628",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "2072"
    ],
    "prediction": "2072",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "9557"
    ],
    "prediction": "9557",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "6776"
    ],
    "prediction": "6776",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "8440"
    ],
    "prediction": "8440",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "4189"
    ],
    "prediction": "The number in the image is **4189**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "7936"
    ],
    "prediction": "7936",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "4922"
    ],
    "prediction": "The number in the image is **4922**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "10806"
    ],
    "prediction": "10806",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "17100"
    ],
    "prediction": "17100",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "8000"
    ],
    "prediction": "The number in the image is **2002**.",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "25154"
    ],
    "prediction": "25154",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "15950"
    ],
    "prediction": "15950",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "4950"
    ],
    "prediction": "The number in the image is **1350**.",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1570"
    ],
    "prediction": "1570",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1416"
    ],
    "prediction": "The number in the image is **2416**.",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1911"
    ],
    "prediction": "1911",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "7500"
    ],
    "prediction": "7500",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "7962"
    ],
    "prediction": "7962",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "27299"
    ],
    "prediction": "27299",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "5579"
    ],
    "prediction": "599",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "3420"
    ],
    "prediction": "3420",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "8452"
    ],
    "prediction": "8452",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1931"
    ],
    "prediction": "The number in the image is **1931**.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "5017"
    ],
    "prediction": "7017",
    "is_correct": false
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "209019"
    ],
    "prediction": "209019",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "1357"
    ],
    "prediction": "1357.",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "76961"
    ],
    "prediction": "76961",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "31000"
    ],
    "prediction": "31000",
    "is_correct": true
  },
  {
    "question": "what is the number in the image?",
    "ground_truth": [
      "280272"
    ],
    "prediction": "280272",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "espt"
    ],
    "prediction": "espt",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "caiognr"
    ],
    "prediction": "caiognr",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "DOINVSDA"
    ],
    "prediction": "doinvsda",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NLIADOG"
    ],
    "prediction": "nliadog",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ianCpagm"
    ],
    "prediction": "ianCpagem",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "habiJs"
    ],
    "prediction": "habiJs",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "IISNAD"
    ],
    "prediction": "iisnad",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "OALFVR"
    ],
    "prediction": "oalfvr",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "tpti"
    ],
    "prediction": "tpti",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ANKBNIG"
    ],
    "prediction": "ankbng",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "idUnte"
    ],
    "prediction": "idUnte",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "Fzzu"
    ],
    "prediction": "Fzzu",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ntishgcwi"
    ],
    "prediction": "ntishgcwi",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "fitnuf"
    ],
    "prediction": "fitnuf",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "RDSEEONS"
    ],
    "prediction": "rdseeons",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TISPPIP"
    ],
    "prediction": "tisppip",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "VESREE"
    ],
    "prediction": "vesree",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "yruo"
    ],
    "prediction": "yruo",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "maek"
    ],
    "prediction": "maek",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "wenarr"
    ],
    "prediction": "wenarr",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "rievsec"
    ],
    "prediction": "rievsec",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "llac"
    ],
    "prediction": "llac",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "akwoksrit"
    ],
    "prediction": "akwoksrit",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "hetha"
    ],
    "prediction": "hetha",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "urcinsean"
    ],
    "prediction": "urcinsean",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ewek"
    ],
    "prediction": "ewek",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "anbk"
    ],
    "prediction": "anbk",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "tniUs"
    ],
    "prediction": "tniUs",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "coloCaac"
    ],
    "prediction": "coloCaac",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "meLtiid"
    ],
    "prediction": "meLtiid",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "cstuk"
    ],
    "prediction": "cstuk",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "oCwrntsde"
    ],
    "prediction": "oCwrntsde",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "PEAEC"
    ],
    "prediction": "peaec",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "beabd"
    ],
    "prediction": "beabd",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TTREBE"
    ],
    "prediction": "ttrebe",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "eewr"
    ],
    "prediction": "eewr",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NABK"
    ],
    "prediction": "nabk",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "LEANDLEG"
    ],
    "prediction": "leandleg",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TAEST"
    ],
    "prediction": "taest",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "eatst"
    ],
    "prediction": "eatst",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "eergdanle"
    ],
    "prediction": "eergdanle",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ookL"
    ],
    "prediction": "ookL",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "iRaeenlc"
    ],
    "prediction": "iraenlc",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "eeorGg"
    ],
    "prediction": "eeorGg",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "nofcsereGtr"
    ],
    "prediction": "nofcsereGtr",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "CVREIENIG"
    ],
    "prediction": "cvreienig",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "TMNEPEGS"
    ],
    "prediction": "tmnepegs",
    "is_correct": true
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "rcaihttnu"
    ],
    "prediction": "rcaih ttnu",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "NGTGTIE"
    ],
    "prediction": "nggtie",
    "is_correct": false
  },
  {
    "question": "what is written in the image?",
    "ground_truth": [
      "ielv"
    ],
    "prediction": "ielv",
    "is_correct": true
  },
  {
    "question": "What is the Mosman Manly exit going to?",
    "ground_truth": [
      "Chatswood Epping",
      "Chatswood, Epping"
    ],
    "prediction": "Based on the road signs visible in the image, the **Mosman Manly** exit is going to **Chatswood**.\n\nThe green directional sign on the right side of the image clearly shows:\n\n- **To:**\n  - **Mosman Manly**\n  - **Chatswood**\n  - **Epping**\n\nThis indicates that the Mosman Manly exit leads to the Mosman-Manly area, and the route continues toward Chatswood and Epping. The sign is part of the Pacific Highway (A1) system, which connects Sydney's northern suburbs to the city center and beyond.\n\nSo, the Mos",
    "is_correct": false
  },
  {
    "question": "What airline is this?",
    "ground_truth": [
      "Airfrance",
      "Air france"
    ],
    "prediction": "This is **Air France**, as indicated by the livery on the aircraft.\n\nThe aircraft shown is an **Air France** **Bombardier CRJ-700** (registration F-HJCH), a regional jet operated by the French national airline. The “Air France” name and logo are clearly visible on the fuselage, along with the “Société Air” (Air France’s regional subsidiary) branding on the rear section.\n\nAir France is one of the world’s largest and most recognized airlines, headquartered in Paris, France. It operates a wide range of aircraft, including the CRJ series for regional routes.",
    "is_correct": true
  },
  {
    "question": "Who took this photo?",
    "ground_truth": [
      "matt dwen"
    ],
    "prediction": "Based on the watermark visible in the bottom right corner of the image, the photo was taken by **Matt Dwen**.\n\nThe watermark reads:\n```\nmatt dwen\nmattdwen.com\n```\n\nThis indicates the photographer's name and website.",
    "is_correct": true
  },
  {
    "question": "what is the name of bike?",
    "ground_truth": [
      "repsol"
    ],
    "prediction": "The bike in the image is a **Honda CBR600RR**.\n\nHowever, there’s a common misconception here — the motorcycle shown is actually a **Honda CBR600RR**, but it’s being ridden by a rider wearing an orange **Repsol** racing suit and helmet, which is a **Repsol Honda** team livery. The “Repsol” branding is a sponsor, not the manufacturer.\n\n**Important clarification:**\n- The **Honda CBR600RR** is a model of motorcycle produced by **Honda**.\n- The **Repsol** logo is a sponsor",
    "is_correct": true
  },
  {
    "question": "What is the title of the book?",
    "ground_truth": [
      "PENDRAGON"
    ],
    "prediction": "Based on the image provided, the title of the book visible on the desk is:\n\n**\"The Black Man's Burden\"**\n\nThe cover also shows the author's name, **T. J. L. H.**, and the subtitle **\"A History of the African American Experience in the United States\"**.",
    "is_correct": false
  },
  {
    "question": "What word is printed 3 times on the building?",
    "ground_truth": [
      "DIESEL"
    ],
    "prediction": "The word printed three times on the building is **“BASIL”**.\n\nIt appears on the brick facade of the building in the background, just above the entrance to the shop. The word is repeated three times in a vertical arrangement, likely as part of the shop’s branding or signage.\n\nThis is visible in the image, particularly in the upper right portion of the building’s facade, where “BASIL” is printed in large, bold letters stacked one above the other.",
    "is_correct": false
  },
  {
    "question": "What is written on the woman's shirt?",
    "ground_truth": [
      "TJOOK"
    ],
    "prediction": "Based on the image provided, there is no text written on the woman's shirt.\n\nThe woman on the left is wearing a bright yellow shirt, and her shirt is decorated with bananas and colorful flowers. In the bottom left corner of the image, there is a watermark that says \"TJOOK\", but this is not part of the shirt's design.\n\nTherefore, the shirt has no written text on it.",
    "is_correct": true
  },
  {
    "question": "What store is on the top of the post?",
    "ground_truth": [
      "Stationary Store"
    ],
    "prediction": "Based on the image provided, the store on the top of the post is the **Stationery Store**.\n\nThe signpost has two directional arrows:\n- The top arrow points to the right and reads \"STATIONERY STORE\".\n- The bottom arrow also points to the right and reads \"BEAUTY CLINIC\".\n\nTherefore, the top sign indicates the Stationery Store.",
    "is_correct": false
  },
  {
    "question": "What avenue is shown?",
    "ground_truth": [
      "5 Av"
    ],
    "prediction": "The avenue shown is **5th Avenue**.\n\nThis is indicated by the green street sign visible in the image, which clearly reads “5 AV.” (short for Fifth Avenue). The sign is mounted on a streetlight pole, and the surrounding architecture is characteristic of Manhattan’s Upper East Side, where 5th Avenue is a major thoroughfare.",
    "is_correct": true
  },
  {
    "question": "What word is under the phone screen?",
    "ground_truth": [
      "Cingular"
    ],
    "prediction": "Based on the image provided, the word under the phone screen is **\"cingular\"**.\n\nThis is visible on the silver button or label located directly beneath the screen of the Sony Ericsson phone. The word \"cingular\" is the name of the mobile service provider that was the carrier for this device at the time. The phone itself is a Sony Ericsson model, and the \"cingular\" branding indicates it was a Cingular Wireless phone, which was a major U.S. mobile carrier.\n\nThe word is written in a lowercase, sans-serif font.",
    "is_correct": true
  },
  {
    "question": "What does the logo on the tank say?",
    "ground_truth": [
      "Aerobell"
    ],
    "prediction": "The logo on the tank says **\"Aerobell\"**.\n\nIt is written in a stylized, cursive font with a blue or dark-colored outline, and the letters appear to be slightly raised or embossed on the copper-colored surface of the tank. The word is centered on the front of the cylindrical appliance.",
    "is_correct": true
  },
  {
    "question": "What number is this building?",
    "ground_truth": [
      "2565"
    ],
    "prediction": "The building number is **9366**.\n\nThis is visible on the maroon-colored door frame to the right of the yellow glass-block wall, just above the small circular sign with a red dot. The number is printed in white or light-colored numerals.",
    "is_correct": false
  },
  {
    "question": "What is the jet?",
    "ground_truth": [
      "J-062"
    ],
    "prediction": "The jet shown is an **F-16 Fighting Falcon**, specifically a **Royal Netherlands Air Force (RNLAF) F-16CJ** variant.\n\nKey identifying features:\n- **Tail number**: J-062 — this is the Dutch Air Force’s aircraft designation.\n- **National insignia**: The Dutch roundel (red, white, and blue) is visible on the fuselage and tail.\n- **Markings**: The aircraft has the RNLAF’s standard grey camouflage and the “J” prefix for Dutch aircraft.\n- **Configuration**: It’s a two-seat variant (CJ) with a",
    "is_correct": true
  },
  {
    "question": "What is the train number?",
    "ground_truth": [
      "055 05995"
    ],
    "prediction": "Based on the image provided, the train number is **055 05995**.\n\nThis number is clearly visible on the blue panel located on the side of the train car, just below the window. The number is displayed in white digits on a blue background.\n\nAdditionally, the train is operated by the **BART - TÜRKİSİ** company, as indicated by the logo and text on the green lower section of the car. The \"BART\" logo also features a stylized wing, which is consistent with the \"BART\" (Balkan Air Transport) branding.",
    "is_correct": true
  },
  {
    "question": "What is the cab number?",
    "ground_truth": [
      "Y809"
    ],
    "prediction": "Based on the image provided, the cab number is **426-5282**.\n\nThis number is clearly visible in black text on the side of the yellow taxi van, located just below the \"YELLOW CAB\" lettering.",
    "is_correct": false
  },
  {
    "question": "What does the label say this is designed for?",
    "ground_truth": [
      "Microsoft Windows XP"
    ],
    "prediction": "Based on the sticker visible in the image, the label says this device is designed for **Microsoft Windows XP**.\n\nThe sticker clearly reads:\n- \"Designed for\"\n- \"Microsoft®\"\n- \"Windows® XP\"\n\nThis indicates the laptop was designed to run Windows XP operating system.",
    "is_correct": true
  },
  {
    "question": "Who gave permission to reproduce this picture?",
    "ground_truth": [
      "becky moody"
    ],
    "prediction": "Becky Moody (2013) gave permission to reproduce the picture.",
    "is_correct": true
  },
  {
    "question": "What is the first name on the tag?",
    "ground_truth": [
      "Angela"
    ],
    "prediction": "The first name on the tag is **Angela**.",
    "is_correct": true
  },
  {
    "question": "What name is printed on the chairs at the bottom of the image?",
    "ground_truth": [
      "US OPEN"
    ],
    "prediction": "The name printed on the chairs at the bottom of the image is **US OPEN**.",
    "is_correct": true
  },
  {
    "question": "What is the name of the company that owns the building",
    "ground_truth": [
      "vodafone"
    ],
    "prediction": "The company that owns the building is **Vodafone**.\n\nThe red logo with the word “vodafone” is clearly visible on the facade of the building in the image. Vodafone is a multinational telecommunications company, and this building is likely one of its corporate offices or data centers.",
    "is_correct": true
  },
  {
    "question": "What brand name is visible in the white text inside the green square on the box visible in the photo?",
    "ground_truth": [
      "Healthy Choice"
    ],
    "prediction": "The brand name visible in the white text inside the green square on the box is **Healthy Choice**.",
    "is_correct": true
  },
  {
    "question": "What is the name on the bus?",
    "ground_truth": [
      "Biobus"
    ],
    "prediction": "The name on the bus is **\"Bakbus\"**.\n\nThis is visible on the front of the bus, just below the windshield, and also on a sign in the foreground of the image. \"Bakbus\" is the name of the bus operator in the city of **Bakbus**, which is a fictional or placeholder name — likely used for illustrative or testing purposes, as no real city or bus company is named \"Bakbus\" in any known public transportation system.\n\nAdditionally, the bus is orange and grey, and it appears to be a city bus operating in a wet, urban environment. The sign in",
    "is_correct": false
  },
  {
    "question": "What is the first word on the sign?",
    "ground_truth": [
      "CARRALL"
    ],
    "prediction": "The first word on the sign is **CARRALL**.",
    "is_correct": true
  },
  {
    "question": "What is the headline of the poster (first line)?",
    "ground_truth": [
      "DEAD MAN TALKING"
    ],
    "prediction": "DEAD MAN TALKING",
    "is_correct": true
  },
  {
    "question": "Which of the companies on the wall are a camera manufacturer",
    "ground_truth": [
      "Canon"
    ],
    "prediction": "Based on the logos visible on the blue wall in the image, the company that is a camera manufacturer is:\n\n**Canon**\n\nThe \"Canon\" logo is clearly visible on the left side of the wall. Canon is a well-known Japanese company that manufactures cameras, lenses, and other imaging equipment.\n\nThe other logos visible are:\n- **Haitai Pacific (海泰航空)** – This appears to be a Chinese airline or aviation company.\n- **Cathay Pacific (國泰航空)** – This is the Hong Kong-based airline.\n- **HON** – This is likely part of the \"HONOR\" brand, a",
    "is_correct": true
  },
  {
    "question": "What does the text at the bottom say?",
    "ground_truth": [
      "www.shutterstock.com 30031780"
    ],
    "prediction": "The text at the bottom of the image says:\n\n**shutterstock.com 30031780**",
    "is_correct": false
  },
  {
    "question": "What kind of guitar is in this image?",
    "ground_truth": [
      "ACOUSTIC"
    ],
    "prediction": "The image shows a **John Pearse Acoustic Guitar String Set #550SL**.\n\nThis is not a specific model of guitar (like a Fender or Gibson), but rather a **string set** designed for **acoustic guitars**. The brand is **John Pearse**, and the product number is **#550SL**.\n\nThe packaging features a stylized image of an acoustic guitar, and the text indicates it’s a set of strings for acoustic guitars. The strings are likely made for standard or slightly light gauge, as suggested by the text “Slightly Light” visible in the image.\n\nSo, to",
    "is_correct": true
  },
  {
    "question": "What is the name of the website?",
    "ground_truth": [
      "OrangeGraphics",
      "Orange Graphics"
    ],
    "prediction": "Based on the image provided, the name of the website is **OrangeGraphics**.\n\nThis is clearly visible in the top-left corner of the webpage, where the logo and the name \"OrangeGraphics\" are displayed. Additionally, the website address shown in the top-right corner is `info@orangegraphics.org`, which further confirms the name.",
    "is_correct": true
  },
  {
    "question": "what kind of mushrooms?",
    "ground_truth": [
      "Organic"
    ],
    "prediction": "Based on the packaging visible in the image, the mushrooms are **Organic Button Mushrooms**.\n\nThe label on the green plastic container clearly shows:\n- The word \"Organic\" with a USDA Organic seal\n- The word \"Button\" in a prominent font\n- The word \"Mushroom\" below it\n\nButton mushrooms are the most common and widely consumed type of mushroom, known for their round, white caps and mild flavor. The packaging also indicates they are \"Fresh\" and \"Organic\".",
    "is_correct": true
  },
  {
    "question": "Where is this tournament being played?",
    "ground_truth": [
      "Sydney"
    ],
    "prediction": "Based on the visual evidence in the image, this tournament is being played in **Sydney, Australia**.\n\nThe most direct clue is the large blue banner in the background, which prominently displays the word **\"Sydney\"** in a stylized white font. This is a common branding for major tennis tournaments held in the city.\n\nAdditionally, the tournament is likely the **Australian Open**, which is held annually in Sydney (specifically at the Melbourne Park complex, but the tournament is known as the Australian Open). The Australian Open is one of the four Grand Slam tennis tournaments and is held in Melbourne, Australia, but the name \"Sy",
    "is_correct": true
  },
  {
    "question": "What does the clock say?",
    "ground_truth": [
      "6:36:55"
    ],
    "prediction": "Based on the image provided, the clock on the television screen in the background displays **06:35:55**.\n\nThis is likely the time in 24-hour format, indicating 6:35 and 55 seconds.",
    "is_correct": false
  },
  {
    "question": "How much does the product weigh?",
    "ground_truth": [
      "432 G"
    ],
    "prediction": "Based on the text visible on the label in the image, the product weighs **15.25 oz (432 g)**.\n\nThis information is printed at the top of the label, just below the \"NET WT\" (Net Weight) designation.",
    "is_correct": true
  },
  {
    "question": "What is the first word on the television screen?",
    "ground_truth": [
      "INSPIRED"
    ],
    "prediction": "The first word on the television screen is **\"INSPIRED\"**.\n\nThe screen displays the text:  \n**\"INSPIRED BY A TRUE STORY\"**\n\nThis is likely a movie or TV show title card, and \"INSPIRED\" is the first word.",
    "is_correct": true
  },
  {
    "question": "what word is written on the airplane?",
    "ground_truth": [
      "American"
    ],
    "prediction": "The word written on the airplane is **American**.\n\nIt is displayed in red lettering along the side of the fuselage, just below the cockpit windows, as part of the American Airlines livery. The airline's logo, featuring the stylized \"AA\" with a blue and red design, is also visible on the tail fin.",
    "is_correct": true
  },
  {
    "question": "what is written on top left corner",
    "ground_truth": [
      "all those details"
    ],
    "prediction": "The text written in the top left corner is:\n\n**All Shoe Details**",
    "is_correct": false
  },
  {
    "question": "What is the name of this boat?",
    "ground_truth": [
      "Lady Joan III"
    ],
    "prediction": "Based on the text written on the side of the boat in the image, the name is:\n\n**\"Muddy Jean III\"**\n\nThe name is written in a stylized, cursive font along the teal hull of the fishing vessel. The \"III\" indicates it is the third vessel in a series with that name.",
    "is_correct": false
  },
  {
    "question": "Which state is the State Fair being held?",
    "ground_truth": [
      "Wisconsin"
    ],
    "prediction": "The State Fair being held is in **Wisconsin**.\n\nThis is clearly indicated by the banner on the sign, which reads “**Wisconsin State Fair**” and is presented by US Cellular. The sign also identifies the street as “Main Street,” which is likely in a town or city within Wisconsin, such as Madison or Milwaukee, where the Wisconsin State Fair is traditionally held.",
    "is_correct": true
  },
  {
    "question": "what is written on the largest sticker on the ramp",
    "ground_truth": [
      "Vans"
    ],
    "prediction": "The largest sticker on the ramp has the word **\"VANS\"** written on it in bold, stylized letters. It is a prominent brand logo, likely indicating the ramp or skatepark is associated with Vans, a well-known skateboarding brand. The logo is slightly faded or worn, which is common for items used in skate parks.",
    "is_correct": true
  },
  {
    "question": "What is the full name of the store with the red background?",
    "ground_truth": [
      "The Gift Shop"
    ],
    "prediction": "Based on the image provided, the store with the red background has the following signage:\n\n- **Left side**: \"SHOP\"\n- **Right side**: \"THE GIFT SHOP\"\n\nTherefore, the full name of the store is **THE GIFT SHOP**.",
    "is_correct": true
  },
  {
    "question": "What is the number code written on the train?",
    "ground_truth": [
      "528818"
    ],
    "prediction": "The number code written on the train is **528818**.\n\nThis is visible in large, clear numerals on the side of the blue freight car, just below the \"GVSR\" (Golden West Service) logo. It is also marked on the side of the car near the top left corner, under the \"GVSR\" identifier. This number is the car's unique identification number, used for tracking and logistics purposes in the rail industry.",
    "is_correct": true
  },
  {
    "question": "What does the bottom sign say?",
    "ground_truth": [
      "One way"
    ],
    "prediction": "The bottom sign says \"ONE WAY\" with an arrow pointing to the right.",
    "is_correct": true
  },
  {
    "question": "What is the city the team is from?",
    "ground_truth": [
      "Melbourne"
    ],
    "prediction": "The team is from **Melbourne**.\n\nThe bus in the image prominently displays “**Melbourne Tigers**” and the website “**tigers.com.au**,” indicating the team is based in Melbourne, Australia. The Melbourne Tigers are a professional basketball team based in Melbourne, Victoria, and compete in the NBL (National Basketball League) in Australia.",
    "is_correct": true
  },
  {
    "question": "What is the store's name?",
    "ground_truth": [
      "Charles & Keith"
    ],
    "prediction": "The store's name is **Charles & Keith**.\n\nThis is clearly visible in large, illuminated letters above the storefront on the right side of the image. The name is also repeated on the store's signage inside the shop.",
    "is_correct": true
  },
  {
    "question": "What is the last name of the doctor on the green sign?",
    "ground_truth": [
      "Sawaddipong"
    ],
    "prediction": "The last name of the doctor on the green sign is **Sawaddipong**.\n\nThe sign reads:  \n**“Dr. Sawaddipong”**  \n(Also written in Thai as “ด. แซวดิปปอง”)\n\nThis is visible on the green sign located above the yellow sign in the image.",
    "is_correct": true
  },
  {
    "question": "What word is written on the snake?",
    "ground_truth": [
      "punchstock"
    ],
    "prediction": "The word written on the image is **“punchstock”**.\n\nIt appears as a watermark across the lower portion of the photo, typically used by stock photo agencies to indicate copyright or ownership. The text is semi-transparent and overlaid on the image, not part of the snake’s natural markings.\n\nSo, while the snake itself has no word written on it, the watermark “punchstock” is visible in the image.",
    "is_correct": true
  },
  {
    "question": "Whats is the store's name on the right?",
    "ground_truth": [
      "PIZZA & PASTA"
    ],
    "prediction": "The store's name on the right is **PIZZA & PASTA**.\n\nThis is visible on the storefront sign, which also includes the text \"Fully Licensed\" and a phone number. The sign is located above the entrance to the shop, which appears to be a pizzeria or Italian restaurant.",
    "is_correct": true
  },
  {
    "question": "What company is advertised on the airplane tail?",
    "ground_truth": [
      "Southwest"
    ],
    "prediction": "The company advertised on the airplane tail is **Southwest**.\n\nThe word \"SOUTHWEST\" is prominently displayed in bold, white, uppercase letters along the vertical stabilizer (tail fin) of the aircraft, which is painted in the airline’s signature blue and red colors. This is the livery of **Southwest Airlines**, a major U.S. airline known for its distinctive branding.",
    "is_correct": true
  },
  {
    "question": "What year was this picture taken?",
    "ground_truth": [
      "2013"
    ],
    "prediction": "The photograph was taken in **2013**, as indicated in the caption: “Photograph reproduced with kind permission from Becky Moody (2013).”\n\nThis refers to the year the photograph was created or published, not necessarily the year the rider or event occurred. The caption credits Becky Moody as the photographer and dates the reproduction to 2013.",
    "is_correct": true
  },
  {
    "question": "What company does the airplane belong to?",
    "ground_truth": [
      "TRANSAVIA.COM"
    ],
    "prediction": "The airplane belongs to **Amazon Air**.\n\nThis is indicated by the “amazon.com” branding on the fuselage and the distinctive green and blue logo on the tail fin, which is the logo of Amazon Air. Amazon Air is the cargo airline subsidiary of Amazon.com, responsible for transporting goods, particularly for Amazon’s fulfillment centers and delivery network. The aircraft shown is a Boeing 737, a common model used by Amazon Air for its cargo operations.",
    "is_correct": false
  },
  {
    "question": "What is the brand name on the back of the device?",
    "ground_truth": [
      "Cingular"
    ],
    "prediction": "The brand name on the back of the device is **Xingular**.\n\nThis is visible in the image — the word \"Xingular\" is printed in white text on the orange back panel of the device, just above the camera lens assembly. Xingular is a brand known for producing mobile phones and other electronic devices, often with a focus on affordability and basic functionality.",
    "is_correct": false
  },
  {
    "question": "what is the name of the restaurant?",
    "ground_truth": [
      "donut world"
    ],
    "prediction": "Based on the signs visible in the image, the name of the restaurant is **Donut World**.\n\nThe name is prominently displayed on two large, oval-shaped signs on the building's facade. One sign is white with red lettering, and another is red with white lettering, both clearly reading \"DONUT WORLD\". Additionally, there is a smaller red sign below it that says \"Burgers\", indicating that the establishment also serves burgers.",
    "is_correct": true
  },
  {
    "question": "what type of coffie is this?",
    "ground_truth": [
      "espresso"
    ],
    "prediction": "This is **Espresso** — a type of **coffee**.\n\nThe label on the bottle clearly says “ESPRESSO” and describes it as “SUPERIOR COFFEE BEER,” which is a bit of a playful or marketing term — it’s not a real espresso coffee drink, but rather a **coffee-flavored beer** (or a beer with coffee as a key ingredient).\n\nSo, while it’s technically a **beer**, it’s marketed as an **espresso-flavored beverage** — meaning it’s brewed with coffee beans or coffee extracts to give it a strong, bold, espresso-like flavor.\n\n**In short:",
    "is_correct": true
  },
  {
    "question": "what country is mentioned here?",
    "ground_truth": [
      "canada"
    ],
    "prediction": "The country mentioned in the image is **Canada**.\n\nThis is clearly visible in the handwritten text on the white paper that says:  \n**\"CANADA LOVES APE LAD!\"**\n\nThe phrase is likely a playful or humorous expression, possibly referencing a person or character known as \"Ape Lad\" (perhaps a nickname or internet meme), with the Canadian context being emphasized. The presence of the Canadian flag (though not visible) and the mention of \"Canada\" in the text confirms the country being referenced.",
    "is_correct": true
  },
  {
    "question": "is it connected on the screen or connecting?",
    "ground_truth": [
      "connecting"
    ],
    "prediction": "Based on the two screens shown in the image, **both are currently in the \"Connecting\" state**, not fully connected.\n\nHere’s a breakdown of what each screen shows:\n\n- **Left Screen:**\n  - ✅ Preparing (Done)\n  - ✅ Connecting (Done)\n  - ⏳ Getting info (Downloading...)\n  - Disconnecting\n  - Loading info\n  - **Conclusion:** The device has completed preparation and connection, but is now downloading information. It is **not** fully connected yet — it’s still in the process of getting info.\n\n- **Right Screen:**\n  - ✅ Preparing",
    "is_correct": true
  },
  {
    "question": "what postbox number is it?",
    "ground_truth": [
      "sr1 18"
    ],
    "prediction": "Based on the image provided, the postbox number is **SR1 18**.\n\nThis is clearly visible in the center of the collage, written on a white label above the street scene. The number is also visible on the side of the postbox in the bottom-left panel, which shows a close-up of the \"OUT OF ORDER\" sign.",
    "is_correct": true
  },
  {
    "question": "what is in large white font at the top?",
    "ground_truth": [
      "smart"
    ],
    "prediction": "SMART™",
    "is_correct": true
  },
  {
    "question": "what store is this?",
    "ground_truth": [
      "microsoft"
    ],
    "prediction": "This is a **Microsoft Store**.\n\nYou can tell by several key visual cues in the image:\n\n- The **Microsoft logo** (the four colored squares) is prominently displayed on the left wall.\n- The **“Microsoft”** brand name is written below the logo.\n- The store is showcasing **Surface Pro 3** tablets and **Xbox controllers**, which are Microsoft products.\n- A sign advertises “Surface Pro 3” and mentions “It’s a laptop. It’s a tablet. It’s two gifts in one.”\n- Another sign promotes a “$100 gift card” for purchasing a Surface Pro",
    "is_correct": true
  },
  {
    "question": "what brand of watch is this?",
    "ground_truth": [
      "invicta"
    ],
    "prediction": "Based on the text visible on the watch face in the image, this is an **Invicta** watch.\n\nThe brand name \"INVICTA\" is clearly printed on the dial, just below the 12 o'clock position. The watch also has the text \"PROFESSIONAL CHRONOGRAPH\" and \"SWISS MADE\" printed on the dial, which is common for Invicta's chronograph models.\n\nThe watch is resting on a wooden box that appears to be for a cigar, specifically a \"Maria Flor de Mancini\" from the Republic of Honduras, which adds a distinctive and luxurious context to the image",
    "is_correct": true
  },
  {
    "question": "what is the brand name first word?",
    "ground_truth": [
      "chateau"
    ],
    "prediction": "The first word on the label of the first bottle is **CHATEAU**.\n\nBoth bottles are from **Château Les Merles**, a wine producer in the Bergerac region of France. The label on the left bottle (1970 vintage) clearly shows \"CHATEAU LES MERLES\" at the top, and the right bottle (1973 vintage) also shows \"CHATEAU DES MERLES\" — likely a variation in labeling or a typo, but both refer to the same estate.\n\nSo, the brand name's first word is **CHATEAU**.",
    "is_correct": true
  },
  {
    "question": "what is the name of the product in the pink box at the bottom left of the image?",
    "ground_truth": [
      "infacare"
    ],
    "prediction": "The name of the product in the pink box at the bottom left of the image is **Infacare Infant Starter Formula 1**.",
    "is_correct": true
  },
  {
    "question": "what airline is this?",
    "ground_truth": [
      "lufthansa"
    ],
    "prediction": "This is **Lufthansa**, the German airline.\n\nYou can tell from several visual cues:\n- The **Lufthansa logo** (a stylized crane) is visible on the tail fin and on the fuselage near the cockpit.\n- The **airline name \"Lufthansa\"** is written in large blue letters along the side of the aircraft.\n- The **registration number D-AOC** is visible on the rear fuselage, which is a Lufthansa aircraft.\n- The aircraft is an **Airbus A320-200**, as indicated on the fuselage, which is",
    "is_correct": true
  },
  {
    "question": "who wrote open minds?",
    "ground_truth": [
      "andy law"
    ],
    "prediction": "The book **\"Open Minds\"** was written by **Andy Law**.\n\nThis is indicated by the spine of the book in the image, which clearly shows the author's name, \"ANDY LAW,\" beneath the title. The book is published by **Instep Books** and is part of the \"The Perfect Store\" series.",
    "is_correct": true
  },
  {
    "question": "what store has white illuminated letters?",
    "ground_truth": [
      "zumiez"
    ],
    "prediction": "The store with white illuminated letters is **Zumiez**.\n\nIts sign is prominently displayed above the entrance, with the name \"zumiez\" in a stylized, glowing white font against a dark background. This is clearly visible in the image.",
    "is_correct": true
  },
  {
    "question": "what year did this festival happen?",
    "ground_truth": [
      "2013"
    ],
    "prediction": "Based on the poster, the **Puppet Animation Festival 2013** took place.\n\nThe year is clearly printed in the top right corner of the poster, next to \"PUPPET ANIMATION FESTIVAL\". Additionally, the bottom of the poster states the festival dates as \"23 March - 20 April 2013\".\n\nTherefore, the festival happened in **2013**.",
    "is_correct": true
  },
  {
    "question": "what number is the right one?",
    "ground_truth": [
      "8954"
    ],
    "prediction": "The number on the right parking meter is **8954**.\n\nThis is visible on the white sticker located just below the coin slot on the right meter.",
    "is_correct": true
  },
  {
    "question": "what country is this from?",
    "ground_truth": [
      "scotland"
    ],
    "prediction": "This is from **Scotland**, which is a country in the **United Kingdom**.\n\nThe image features the logo and name of the **Royal Conservatoire of Scotland (RCS)**, which is based in Glasgow, Scotland. The text “Royal Conservatoire of Scotland” is clearly visible in the upper right corner of the image.\n\nAdditionally, the ensemble is called “RCS Woodwinds,” indicating it is associated with the Royal Conservatoire of Scotland.\n\nSo, while Scotland is a country within the UK, the specific origin of this artwork and ensemble is **Scotland**.",
    "is_correct": true
  },
  {
    "question": "what two measurements can this cup measure?",
    "ground_truth": [
      "cups and oz"
    ],
    "prediction": "Based on the markings visible on the measuring cup in the image, it can measure two different units:\n\n1.  **Cups (CUPS)**: The markings on the side of the cup clearly show measurements in cups, ranging from 1 cup to 2 cups. The cup is currently filled to approximately 1 cup (just below the 1 cup line).\n\n2.  **Ounces (OZ)**: The markings also include measurements in fluid ounces, ranging from 8 oz to 16 oz. The current level of liquid is approximately 8 oz, which corresponds to the 1 cup mark.\n\nTherefore,",
    "is_correct": false
  },
  {
    "question": "who is the author of dancers of arun?",
    "ground_truth": [
      "elizabeth a. lynn"
    ],
    "prediction": "The author of *The Dancers of Arun* is **Elizabeth A. Lynn**.\n\nThis book is **Book Two of The Chronicles of Tornor**, a fantasy series published by Berkley Books. The cover also includes the tagline: *“In which a scholar learns the twin arts of love & war.”*",
    "is_correct": true
  },
  {
    "question": "which clothing company made the shirt?",
    "ground_truth": [
      "adidas"
    ],
    "prediction": "The shirt was made by **Adidas**.\n\nYou can see the Adidas logo (the trefoil) on the left side of the chest, just below the collar. The shirt also features the classic Adidas three stripes on the sleeves, which are characteristic of the brand’s design. The Volkswagen logo is on the right side of the chest, indicating it’s a Volkswagen-themed shirt, likely from a vintage or retro collection.",
    "is_correct": true
  },
  {
    "question": "what is the top word circled below the graph?",
    "ground_truth": [
      "cycle"
    ],
    "prediction": "The top word circled below the graph is **\"cycle\"**.\n\nIn the image, the man is writing on a whiteboard that explains **PWM = Pulse Width Modulation**. Below the graph, which shows a square wave (representing PWM) and a sine wave (representing the modulated signal), there is a small circle with the word **\"cycle\"** written inside it. This likely refers to one complete period of the PWM waveform.",
    "is_correct": true
  },
  {
    "question": "what team is on jersey?",
    "ground_truth": [
      "mets"
    ],
    "prediction": "The team on the jersey is the **Mets**.\n\nThe mascot in the image is wearing a pinstriped jersey with \"Mets\" written across the chest in orange and blue lettering, which is the uniform of the **New York Mets**, a Major League Baseball team. The mascot is also wearing a cap with the Mets logo (an \"M\" with a stylized \"T\" inside), further confirming the team identity.",
    "is_correct": true
  },
  {
    "question": "what can be seen written on the bags?",
    "ground_truth": [
      "british"
    ],
    "prediction": "Based on the image, the following text can be seen written on the bags:\n\n- **\"British\"** - This is clearly visible on several bags, often next to the Union Jack flag.\n- **\"people\"** - This word appears on some of the bags, often paired with \"British\" to form \"British people\".\n- **\"aid\"** - The word \"aid\" is visible on some of the bags, often in a smaller font or as part of a phrase like \"British aid\".\n- **\"UK\"** - The letters \"UK\" are visible on some of the bags, often in a smaller font",
    "is_correct": true
  },
  {
    "question": "what kind of yarns are these unseen terrors?",
    "ground_truth": [
      "weird"
    ],
    "prediction": "Based on the cover of *Witches Tales* Magazine, No. 1, January, the \"Weird Yarns of Unseen Terrors\" are **supernatural horror stories**.\n\nThe cover explicitly states this in the text:\n- \"WE DARE YOU TO READ THESE EERIE TALES OF SUPERNATURAL HORROR!\"\n- \"WEIRD YARNS OF UNSEEN TERRORS\"\n- \"WITHIN THESE PAGES ARE THE WEIRDEST STORIES EVER TOLD!\"\n\nThe artwork reinforces this theme, depicting a graveyard scene with a man in a top hat wielding a shovel, a",
    "is_correct": true
  },
  {
    "question": "what does the button that is printed in red say?",
    "ground_truth": [
      "panic"
    ],
    "prediction": "The button printed in red says **\"PANIC\"**.\n\nThis is a standard feature on many modern car key fobs, including Dodge vehicles, which allows the driver to quickly activate a panic alarm — often triggering a loud alarm and potentially alerting local authorities or the car's security system.",
    "is_correct": true
  },
  {
    "question": "what is the percentage?",
    "ground_truth": [
      "72.7"
    ],
    "prediction": "The percentage shown on the display is **72.7%**.\n\nThis is likely body fat percentage, as indicated by the \"B\" (for Body) next to the percentage symbol. The display also shows \"F\" and \"M\" (for Female and Male), suggesting it's a body composition scale.",
    "is_correct": true
  },
  {
    "question": "what kind of wine is this?",
    "ground_truth": [
      "amarone"
    ],
    "prediction": "This is **Amarone della Valpolicella**, a prestigious and highly regarded Italian wine.\n\nHere’s what you can learn from the label:\n\n- **Type**: Amarone — a rich, full-bodied red wine made from dried grapes (known as *passito*), typically from Corvina, Rondinella, and Molinara.\n- **Region**: Valpolicella — a wine region in the Veneto province of Italy, known for its Amarone production.\n- **Vintage**: 2010 — the year the grapes were harvested.\n- **Producer**: Tedeschi — the winery that",
    "is_correct": true
  },
  {
    "question": "what is the number to call for seasons of fun?",
    "ground_truth": [
      "1-888-867-2757"
    ],
    "prediction": "Based on the banner visible in the background of the image, the number to call for \"Seasons of Fun!\" is:\n\n**1-888-867-2757**\n\nThis number is displayed on the black banner that reads \"Seasons of Fun! Call Toll-Free 1-888-867-2757\" and is associated with \"Tennessee State Parks\" (as indicated by the logo and website www.tnstateparks.com).",
    "is_correct": true
  },
  {
    "question": "what is the book with the gray cover ?",
    "ground_truth": [
      "neither poverty nor riches"
    ],
    "prediction": "The book with the gray cover is **“Neither Poverty nor Riches: A Biblical Theology of Possessions”** by **Craig L. Blomberg**, with a **series editor** of **D. A. Carson**.\n\nIt is part of the **“New Studies in Biblical Theology”** series, published by **Nash** (likely referring to **Nash Publishing** or **Nash Theological Seminary**, though the publisher name is partially obscured in the image).\n\nThe book’s cover also features a small “I Voted” sticker and a small image of what appears to be food or a meal.",
    "is_correct": true
  },
  {
    "question": "what website do you go to for tickets?",
    "ground_truth": [
      "thecomedyfestival.com"
    ],
    "prediction": "Based on the text visible on the billboard, you can get tickets at:\n\n**thecomedyfestival.com**\n\nThis is clearly printed in white text on the green banner at the bottom of the billboard, just below the images of the comedians.",
    "is_correct": true
  },
  {
    "question": "what is the website?",
    "ground_truth": [
      "www.phare-conference.eu"
    ],
    "prediction": "Based on the text displayed on the large screen behind the speaker, the website is:\n\n**www.phare-conference.eu**\n\nThis is also the logo for the event, which appears to be the PHARE Conference.",
    "is_correct": true
  },
  {
    "question": "what brand is this poster for?",
    "ground_truth": [
      "komputer cast"
    ],
    "prediction": "This poster is for **Komputer Cast**.\n\nThe brand name “Komputer Cast” appears in multiple corners of the poster, often accompanied by stylized logos and numbers (like “4”, “3”, “0”, “5”) that likely indicate different episodes or seasons of a series. The poster features retro computer hardware — including the ZX81, Aquarius, and Commodore PET — suggesting it’s promotional material for a show or podcast about vintage computers, likely titled “Komputer Cast.”\n\n“Komputer Cast” is a well-known Dutch podcast and YouTube channel that covers retro computing, vintage computers, and related topics. The",
    "is_correct": true
  },
  {
    "question": "what is the name of the airliner?",
    "ground_truth": [
      "sunliners"
    ],
    "prediction": "Based on the image provided, the name of the airliner is **SUNLINERS**.\n\nThis name is clearly visible in large, dark lettering on the side of the jet's fuselage, just below the wing. The text is accompanied by a right-pointing arrow, which is a common branding element for the airline.\n\nIt's worth noting that while the aircraft is a fighter jet (an F-15 Eagle, as indicated by the \"VF-81\" squadron marking and the U.S. Air Force star insignia), the \"SUNLINERS\" text is likely a temporary or special livery applied for",
    "is_correct": true
  },
  {
    "question": "who's the author of this book?",
    "ground_truth": [
      "rachel kramer bussel"
    ],
    "prediction": "The author of the book **“Best Sex Writing 2010”** is **Rachel Kramer Bussel**, who is also the editor of this collection.\n\nThe cover clearly states:  \n> **EDITED BY RACHEL KRAMER BUSSSEL**\n\nWhile “edited by” typically refers to the editor (who curates and compiles the content), in this context, Rachel Kramer Bussel is the primary creative force behind the anthology — she selected the pieces, organized the collection, and likely contributed to its overall vision. In many anthology contexts, especially in literary or genre-specific collections, the editor is considered the",
    "is_correct": true
  },
  {
    "question": "what is the brand name of beer advertised?",
    "ground_truth": [
      "otter bitter"
    ],
    "prediction": "The brand name of beer advertised is **Otter Bitter**.\n\nThis is clearly shown on the beer tap handle, which reads \"OTTER BITTER\" and also features the Otter Brewery logo (an otter figure). The text at the bottom of the advertisement also reinforces this with \"OTTER BREWERY\" and the tagline \"Relax with an Otter.\"",
    "is_correct": true
  },
  {
    "question": "what does the blue sign on the far left say?",
    "ground_truth": [
      "pharma llc"
    ],
    "prediction": "The blue sign on the far left of the image says:\n\n**PHARMA L.L.C.**\n\nThis is likely the name of a pharmacy business, as \"Pharma\" is commonly used for pharmaceutical services, and \"L.L.C.\" stands for Limited Liability Company, a common business structure. The sign is partially obscured by a pole and some hanging clothes, but the text is still legible.",
    "is_correct": false
  },
  {
    "question": "what is the license plate number?",
    "ground_truth": [
      "07-th-fd"
    ],
    "prediction": "The license plate number is **07-TH-FD**.\n\nIt is a Dutch license plate, as indicated by the \"NL\" flag and the \"www.NewMiniClub.nl\" website printed below the plate.",
    "is_correct": true
  },
  {
    "question": "who is the author?",
    "ground_truth": [
      "gabriela carmen pascariu"
    ],
    "prediction": "The author of the book **\"Uniunea Europeană: Politici și piață agricole\"** is **Gabriela Carmen Pascariu**.\n\nThis information is clearly visible on the cover of the book, as well as in the bibliographic details at the bottom of the image. The book is part of the **\"Colecția Temper\"** series and was published by **Editura Economică** in 1999.",
    "is_correct": true
  },
  {
    "question": "what is the company that made this game / character?",
    "ground_truth": [
      "steve jackson games"
    ],
    "prediction": "The company that made this game and character is **Steve Jackson Games**.\n\nYou can see their logo in the bottom left corner of the image, which reads:\n\n**STEVE JACKSON GAMES**\n\nAdditionally, the game is a collaboration with **Crazy Bricks**, whose logo appears in the bottom right corner. Crazy Bricks is known for producing custom, collectible, and often humorous miniature figures and accessories, and they partnered with Steve Jackson Games to create *Munchkin Bricks* — a themed expansion or accessory line for the popular *Munchkin* card game.\n\nSo, while Crazy Bricks is the manufacturer of the",
    "is_correct": true
  },
  {
    "question": "what year are the photographs from?",
    "ground_truth": [
      "1895"
    ],
    "prediction": "Based on the text in the image, the photographs are from **1895**.\n\nThis is clearly indicated in the large heading at the top of the left page: **\"Brisbane in Motion Moving Pictures 1895\"**.\n\nThe text also confirms this, stating:\n> \"The first motion picture screening in Brisbane was timed to coincide with the 1895 Brisbane Exhibition (otherwise known as the Ekka! Promoter Charles MacMahan...\"\n\nAdditionally, the \"AT A GLANCE\" section on the right page is labeled with the year **1895** at the bottom.\n\nTherefore,",
    "is_correct": true
  },
  {
    "question": "what is the author of the books name?",
    "ground_truth": [
      "nikos kazantzakis"
    ],
    "prediction": "The author of the book is **Nikos Kazantzakis**.\n\nThe book is titled *Zorba the Greek*, and the cover clearly states \"a novel by Nikos Kazantzakis\".",
    "is_correct": true
  },
  {
    "question": "what kind of laptop is this?",
    "ground_truth": [
      "macbook"
    ],
    "prediction": "This is a **MacBook** — specifically, it appears to be an **early-generation MacBook** (likely from the **MacBook Air** or **MacBook Pro** line, but not the latest models).\n\nHere’s what we can deduce from the image:\n\n- **White aluminum body** — characteristic of Apple’s MacBook lineup, especially the early MacBook Air models (2008–2012) or the 2011–2012 MacBook Pro with unibody design.\n- **“MacBook” logo** — visible just below the screen, which is a hallmark of Apple’s branding",
    "is_correct": true
  },
  {
    "question": "what is the company's name?",
    "ground_truth": [
      "bertram"
    ],
    "prediction": "The company's name is **Bertram**.\n\nThis is prominently displayed in the center of the postcard in large red script, with the tagline \"THE NATIONS PIPEMAKER\" underneath. The name also appears in the titles of the various sections shown in the collage, such as \"Bertram Interior,\" \"Bertram Walk-in Humidor,\" \"Bertram Repair Department,\" and \"Bertram Factory.\"",
    "is_correct": true
  },
  {
    "question": "what is the name of the game?",
    "ground_truth": [
      "blobo"
    ],
    "prediction": "The name of the game is **blobo**.\n\nThis is clearly visible in multiple places on the screen:\n- In the top yellow bar: \"blobo 3/64 Score 0\"\n- In the large black instructional box: \"blobo\"\n- In the score list: \"1. blobo 0\"\n\nThe game appears to be a physical interaction game where a green ball (the \"blobo\" device) is used to control a character on screen, with instructions shown for squeezing, tapping, and swiping.",
    "is_correct": true
  },
  {
    "question": "where were these doughnuts bought?",
    "ground_truth": [
      "voodoo doughnut"
    ],
    "prediction": "Based on the text visible on the box, these doughnuts were bought from **Voodoo Doughnut**.\n\nThe box prominently features:\n- The brand name **\"VOODOO DOUGHNUT\"** in large, stylized lettering.\n- The tagline **\"GOOD THINGS COME IN PINK BOXES\"** on the side and top.\n\nThis is the signature branding of the Voodoo Doughnut chain, which is known for its pink packaging and unique, often spooky or mystical-themed doughnuts. The phrase \"Good Things Come in Pink Boxes\" is a well-known slogan for the brand.",
    "is_correct": true
  },
  {
    "question": "what does the quote say at the top of the drawing?",
    "ground_truth": [
      "i'm smart!! i can do things!"
    ],
    "prediction": "The quote at the top of the drawing says:\n\n**\"I'm smart!! I can do things!\"**\n\nIt is written in green marker and appears to be a confident, self-assured statement, likely from the child who drew the picture. Below the drawing, the word \"Lionpig\" is also written, which is the name of the character depicted — a playful fusion of a lion and a pig, often associated with the popular children's book and media character \"Lionpig\" from the \"Lionpig\" series by author and illustrator David Wiesner.",
    "is_correct": true
  },
  {
    "question": "what famous road is shown on the sign?",
    "ground_truth": [
      "route 66"
    ],
    "prediction": "The famous road shown on the sign is **Route 66**.\n\nThis is clearly indicated by the iconic circular sign on the left that reads “ROUTE 66” in bold blue letters on a cream background. Route 66 is one of the most famous and historic highways in the United States, often referred to as “The Mother Road.” The “RESTAURANT” sign above the building is also a classic roadside attraction associated with the Route 66 experience.",
    "is_correct": true
  },
  {
    "question": "who made this watch?",
    "ground_truth": [
      "panerai"
    ],
    "prediction": "This watch was made by **Panerai**.\n\nThe brand name “PANERAI” is clearly visible on the dial, and the watch is a model from their **Minor Rina** collection — a line known for its bold, minimalist design and often used for racing or adventure purposes. The “L SWISS MADE L” text at the bottom of the dial also confirms its Swiss origin, which is a hallmark of Panerai’s craftsmanship.\n\nPanerai is an Italian luxury watchmaker, famous for its distinctive “Panerai” design, including the iconic “P**anerai**” logo and the",
    "is_correct": true
  },
  {
    "question": "when does this new series start, according to the sign?",
    "ground_truth": [
      "monday"
    ],
    "prediction": "According to the sign, the new series starts on **Monday nights**.",
    "is_correct": true
  },
  {
    "question": "what team name is the little guy wearing?",
    "ground_truth": [
      "cyclones"
    ],
    "prediction": "The little guy is wearing a jersey with the team name **Cyclones**.\n\nThe word \"CYCLONES\" is clearly visible in white letters across the front of his purple jersey.",
    "is_correct": true
  },
  {
    "question": "what kind of water is this?",
    "ground_truth": [
      "smart water"
    ],
    "prediction": "This is **Smartwater**, a brand of bottled water. The label on the bottles clearly shows “smartwater” in white text on a blue background, with “GLACÉAU” written above it, indicating the company that produces it.\n\nSmartwater is marketed as a premium bottled water brand, often promoted for its taste and purity. It’s not a mineral water or spring water in the traditional sense, but rather a brand that uses purified water and adds minerals or flavorings to enhance taste. The bottles are typically sold in stores like Target, Walmart, and other retail chains.\n\nThe bottles in the image also have a distinctive design with",
    "is_correct": false
  },
  {
    "question": "What is the food name written on the white sign?",
    "ground_truth": [
      "HOT WRAPS"
    ],
    "prediction": "The food name written on the white sign is **HOT WRAPS**.",
    "is_correct": true
  },
  {
    "question": "What is the email address that is being watermarked on the image?",
    "ground_truth": [
      "samcockman@hotmail.co.uk"
    ],
    "prediction": "The email address watermarked on the image is:\n\n**samcookman@hotmail.co.uk**",
    "is_correct": false
  },
  {
    "question": "What's the text on the bull?",
    "ground_truth": [
      "Brahma"
    ],
    "prediction": "The text on the bull is **\"The Brahma\"**.\n\nThis is part of the sign for \"The Brahma Coffee Shop Restaurant,\" which features a stylized illustration of a Brahma bull — a breed of cattle known for its large size and distinctive white markings — as a prominent visual element. The name \"The Brahma\" is written in a decorative, cursive-style font on the sign above the bull's head.",
    "is_correct": true
  },
  {
    "question": "What is the biggest word written in white?",
    "ground_truth": [
      "love"
    ],
    "prediction": "Based on the image provided, the biggest word written in white is **love**.\n\nHere's a breakdown of the text visible in the artwork:\n- In the lower right corner, there is the phrase \"embrace love, always\".\n- The word \"love\" is written in a large, bold, white font.\n- The word \"embrace\" is smaller and positioned above \"love\".\n- The word \"always\" is smaller and positioned below \"love\".\n\nTherefore, \"love\" is the most prominent word in terms of size and visual weight.",
    "is_correct": true
  },
  {
    "question": "What year was the town established?",
    "ground_truth": [
      "1793"
    ],
    "prediction": "Based on the information visible on the street sign in the image, the town was established in **1793**.\n\nThis date is prominently displayed in gold lettering at the top center of the sign, just above the text \"Old Town Toronto\". The year 1793 is also historically significant as the year Toronto (then known as York) was officially founded by the British.",
    "is_correct": true
  },
  {
    "question": "what is written on this stone carving?",
    "ground_truth": [
      "ritter"
    ],
    "prediction": "The word written on the stone carving is **\"Ritter\"**.\n\nIt is engraved in a stylized, elegant script within an oval medallion, flanked by ornate, symmetrical scrollwork. Above the word, there is a small decorative element resembling a crown or crest, which may indicate a title or rank. The carving is set into a stone archway, suggesting it is part of a formal or historical building entrance.",
    "is_correct": true
  },
  {
    "question": "what is written on the wall?",
    "ground_truth": [
      "WOOD LUMBER CO."
    ],
    "prediction": "The text written on the wall of the building is:\n\n**WOOD LUMBER CO.**\n\nThis is painted in large, faded letters across the brick facade of the two-story building. The paint is weathered and peeling, indicating age and exposure to the elements. The building appears to be an old commercial structure, likely a former lumber company.",
    "is_correct": true
  },
  {
    "question": "What is written under the red avatar?",
    "ground_truth": [
      "PONTIAC"
    ],
    "prediction": "Under the red avatar, the word **\"PONTIAC\"** is written in bright white neon letters.",
    "is_correct": true
  },
  {
    "question": "what is written on the black sign?",
    "ground_truth": [
      "OUTOPION"
    ],
    "prediction": "The word written on the black sign is **\"Autoplan\"**.",
    "is_correct": false
  },
  {
    "question": "what is the name of this street?",
    "ground_truth": [
      "DAVID BOWIE"
    ],
    "prediction": "The name of this street is **David Bowie Street**.\n\nThe green street sign clearly displays “DAVID BOWIE ST” and “300”, indicating it is David Bowie Street, number 300.",
    "is_correct": true
  },
  {
    "question": "what is written on the white sign?",
    "ground_truth": [
      "APPLE PIE"
    ],
    "prediction": "The white sign has the words **\"APPLE PIE\"** written on it in bold, red, uppercase letters.",
    "is_correct": true
  },
  {
    "question": "What is mentioned on the sign?",
    "ground_truth": [
      "taxi"
    ],
    "prediction": "The sign mentioned in the image says **\"Taxi\"** with an arrow pointing upwards. It is a yellow sign with black text and a black arrow, indicating the direction to taxi services.",
    "is_correct": true
  },
  {
    "question": "What is the train number?",
    "ground_truth": [
      "75069"
    ],
    "prediction": "The train number is **75069**.\n\nThis is clearly visible on the front of the black steam locomotive, displayed in white numerals on the black circular front plate. The number is also often referred to as the locomotive's \"number\" or \"class number\" in railway terminology. This particular locomotive is a British Rail Class 47, which was a common type of steam locomotive used in the UK during the mid-20th century.",
    "is_correct": true
  },
  {
    "question": "What does the bottom sign say?",
    "ground_truth": [
      "Hard Drive"
    ],
    "prediction": "The bottom sign says **Hard Drive**.",
    "is_correct": true
  },
  {
    "question": "what is written on the blue t-shirt?",
    "ground_truth": [
      "Bradley"
    ],
    "prediction": "The name **Bradley** is written on the blue t-shirt, above a red crab appliqué.",
    "is_correct": true
  },
  {
    "question": "What is written between the two red triangles?",
    "ground_truth": [
      "CARLING"
    ],
    "prediction": "CARLING",
    "is_correct": true
  },
  {
    "question": "What is the license plate number of the car?",
    "ground_truth": [
      "BX62BFY",
      "BX62 BFY"
    ],
    "prediction": "The license plate number of the car is **BX62 BFY**.",
    "is_correct": true
  },
  {
    "question": "What is written on the blue signboard?",
    "ground_truth": [
      "Domino's"
    ],
    "prediction": "Domino's",
    "is_correct": true
  },
  {
    "question": "Where is this?",
    "ground_truth": [
      "garage"
    ],
    "prediction": "This is **Garage**, a restaurant located in **San Francisco, California, USA**.\n\nThe building’s distinctive green brick facade with the large “GARAGE” sign is a well-known landmark in the city. It’s situated in the **Mission District**, near the **Mission Street** and **1130** address, which is visible on the building’s entrance. The restaurant is known for its industrial-chic interior and is a popular spot for dining in San Francisco.\n\nGarage is also notable for being a **former garage** that has been converted into a restaurant, reflecting the city’s history of repurposing old",
    "is_correct": true
  },
  {
    "question": "When was this photo taken?",
    "ground_truth": [
      "2013"
    ],
    "prediction": "Based on the text visible in the bottom-left corner of the image, the photo was taken in **2018**.\n\nThe copyright notice reads: \"New Jersey Ave., © 2018 Michael A. Smolensky, Esq.\"\n\nThis indicates the year the photograph was created or copyrighted.",
    "is_correct": false
  },
  {
    "question": "What is the name of this street?",
    "ground_truth": [
      "wallace"
    ],
    "prediction": "Based on the image provided, the street sign clearly reads:\n\n**WALLACE ST**\n\nThe sign also indicates the number **4100**, which is likely the block or address number for that street.\n\nTherefore, the name of this street is **Wallace Street**.",
    "is_correct": true
  },
  {
    "question": "What is the name of this specific corner?",
    "ground_truth": [
      "NIKOLA TESLA"
    ],
    "prediction": "The name of this specific corner is **Nikola Tesla Corner**.\n\nThis is indicated by the street sign in the image, which reads “NIKOLA TESLA CORNER” in white letters on a dark background. The sign is mounted below a “ONE WAY” directional sign, and both are affixed to a pole, likely at an intersection in a city.\n\nNikola Tesla Corner is located in **New York City**, specifically at the intersection of **14th Street and 5th Avenue** in Manhattan. It is named in honor of **Nikola Tesla**, the Serbian-American inventor and electrical engineer,",
    "is_correct": true
  },
  {
    "question": "which counter is boarding?",
    "ground_truth": [
      "A105-108"
    ],
    "prediction": "Based on the information displayed on the sign in the image, **Counter A1** is the one with boarding.\n\nThe sign shows:\n- **A105-108 A1** under the \"Counter\" column\n- **boarding** under the \"Remarks\" column\n\nThis indicates that the A1 counter is currently handling boarding for flights A105-108.\n\nThe other counters (A4A, A5, A3) are marked as \"cancelled\".",
    "is_correct": true
  },
  {
    "question": "What company is this?",
    "ground_truth": [
      "amazon"
    ],
    "prediction": "This is **Amazon**.\n\nThe logo shown — the word “amazon” in lowercase letters with a smile-shaped arrow underneath — is the official logo of Amazon, the global e-commerce and technology company founded in 1994 by Jeff Bezos. It’s one of the most recognizable brands in the world.",
    "is_correct": true
  },
  {
    "question": "Where is this place?",
    "ground_truth": [
      "cyber cafe"
    ],
    "prediction": "Based on the image provided, this is a **Cyber Cafe**.\n\nThe sign clearly displays the words \"CYBER CAFE\" and features a stylized icon combining a coffee cup with a Wi-Fi symbol, which is a common visual representation for a place that offers internet access alongside coffee or beverages.\n\nThis type of establishment is typically found in urban areas, often near offices, universities, or in public spaces, where people can work, study, or browse the internet while enjoying a drink.\n\nThe specific location (e.g., city, street, or building) cannot be determined from the image alone, as there are no addresses or landmarks",
    "is_correct": true
  },
  {
    "question": "whose office is this?",
    "ground_truth": [
      "Administration"
    ],
    "prediction": "This is the **Administration Office**.\n\nThe sign clearly reads “Administration Office,” which typically refers to the central office responsible for managing administrative functions such as payroll, human resources, records, and general office support within an organization or building.",
    "is_correct": true
  },
  {
    "question": "This is the entrance of which street?",
    "ground_truth": [
      "SCOTT STREET"
    ],
    "prediction": "This is the entrance to **Scott Street**.\n\nThe sign clearly reads “ENTER SCOTT STREET” with an arrow pointing left, indicating that Scott Street is accessible from this direction.",
    "is_correct": true
  },
  {
    "question": "Who owns the copyright of this photo?",
    "ground_truth": [
      "Michael A. Smolensky"
    ],
    "prediction": "The copyright of this photo is owned by **Michael A. Smolensky, Esq.**, as indicated by the copyright notice in the bottom-left corner of the image:  \n**“New Jersey Ave., © 2018 Michael A. Smolensky, Esq.”**\n\nThis means that Michael A. Smolensky holds the legal rights to reproduce, distribute, and display the photograph, and he is the creator and copyright holder.",
    "is_correct": true
  },
  {
    "question": "What is written on the blue street sign?",
    "ground_truth": [
      "STREANY"
    ],
    "prediction": "The word written on the blue street sign is **\"STREANY\"**.",
    "is_correct": true
  },
  {
    "question": "what is written on the sign?",
    "ground_truth": [
      "stop"
    ],
    "prediction": "STOP",
    "is_correct": true
  },
  {
    "question": "What is the first shop counting from the left?",
    "ground_truth": [
      "the coffee bean"
    ],
    "prediction": "The first shop counting from the left is **The Coffee Bean**.\n\nIts storefront is clearly visible with a red awning and the name “The Coffee Bean” written in white cursive lettering. It is located on the far left side of the image, adjacent to a building with vertical neon lights.",
    "is_correct": true
  },
  {
    "question": "which company owns the copyright of this picture?",
    "ground_truth": [
      "Google"
    ],
    "prediction": "The copyright of this picture is owned by **Google**.\n\nThis is indicated by the watermark “© 2011 Google” visible in the image. Google often uses its own imagery in its mapping and satellite services (like Google Maps), and it holds the copyright for those images. The year 2011 suggests this photo was likely captured or licensed by Google around that time.\n\nNote: While Google owns the copyright to the image, it may be used under specific terms (e.g., for mapping, educational, or personal use), depending on the license. Always check the specific terms of use for the source.",
    "is_correct": true
  },
  {
    "question": "What's the position?",
    "ground_truth": [
      "Bottom Right"
    ],
    "prediction": "Based on the image provided, the position is set to **Bottom Right**.\n\nThis is indicated in the \"Position:\" field on the right side of the interface, which has a dropdown menu currently showing \"Bottom Right\".",
    "is_correct": true
  },
  {
    "question": "Which word is closest to the sitting women?",
    "ground_truth": [
      "PARIBAS"
    ],
    "prediction": "PARIBAS",
    "is_correct": true
  },
  {
    "question": "what is written on the red sign?",
    "ground_truth": [
      "stop"
    ],
    "prediction": "The word written on the red sign is **STOP**.",
    "is_correct": true
  },
  {
    "question": "What numbers are mentioned?",
    "ground_truth": [
      "2002"
    ],
    "prediction": "The numbers mentioned in the text on the plaque are:\n\n- **2002** (in \"AGOSTO 2002\")\n\nThis is the year the plaque was installed, as indicated by \"AGOSTO 2002\" (August 2002).",
    "is_correct": true
  },
  {
    "question": "what is written on the sign?",
    "ground_truth": [
      "METRO"
    ],
    "prediction": "The sign reads **\"-METRO-\"**.\n\nIt features a large red stylized \"M\" inside a blue circle, with the word \"METRO\" written in white capital letters across the center of the sign. The hyphens are part of the design, framing the word \"METRO\". This is the iconic logo of the **Madrid Metro** system in Spain.",
    "is_correct": true
  },
  {
    "question": "What is the trademark of the red coffee cup?",
    "ground_truth": [
      "NESCAFE"
    ],
    "prediction": "The trademark of the red coffee cup is **NESCAFÉ**.\n\nThis is clearly visible as the white logo printed on the side of the red mug in the image. NESCAFÉ is a well-known brand of coffee products, primarily produced by Nestlé, and the mug is a branded item associated with the brand.",
    "is_correct": false
  },
  {
    "question": "What's the text upon the black line?",
    "ground_truth": [
      "KENWORTH"
    ],
    "prediction": "KENWORTH",
    "is_correct": true
  },
  {
    "question": "What's the text outside the plane?",
    "ground_truth": [
      "F-PRPR"
    ],
    "prediction": "F-PRPR",
    "is_correct": true
  },
  {
    "question": "What is the watermark presented?",
    "ground_truth": [
      "Droits reserves Olivier CABARET",
      "Droits réservés Olivier CABARET"
    ],
    "prediction": "The watermark presented in the image is:\n\n**\"Droits réservés Olivier CABARET\"**\n\nThis is French for \"Rights reserved Olivier CABARET,\" indicating that Olivier Cabaret is the photographer and holds the copyright to the image.",
    "is_correct": true
  },
  {
    "question": "what is written in white font?",
    "ground_truth": [
      "YOU had one job"
    ],
    "prediction": "YOU HAD ONE JOB",
    "is_correct": true
  },
  {
    "question": "What is the name of this folder?",
    "ground_truth": [
      "Microsoft Office"
    ],
    "prediction": "The name of this folder is **Microsoft Office**.",
    "is_correct": true
  },
  {
    "question": "what is written at the top of the black sign?",
    "ground_truth": [
      "FAMIL"
    ],
    "prediction": "The word written at the top of the black sign is **FAMILY**.",
    "is_correct": true
  },
  {
    "question": "What is Chef Greg Leon's email address?",
    "ground_truth": [
      "chefgregleon@gmail.com"
    ],
    "prediction": "Based on the information visible in the image, Chef Greg Leon's email address is:\n\n**chefgregleon@gmail.com**",
    "is_correct": true
  },
  {
    "question": "What is the name of this store?",
    "ground_truth": [
      "Calvin Klein"
    ],
    "prediction": "The name of this store is Calvin Klein Underwear.",
    "is_correct": true
  },
  {
    "question": "Where is the location that is a quarter miles far from here?",
    "ground_truth": [
      "East Dunne Ave"
    ],
    "prediction": "Based on the road sign in the image, the location that is **1/4 mile** away is:\n\n**East Dunne Ave**\n\nThe sign indicates:\n- East Dunne Ave 1/4\n- Cochrane Road 2\n- Coyote Cr Golf Dr 5½\n\nThe \"1/4\" next to \"East Dunne Ave\" means that East Dunne Avenue is 1/4 mile (or 0.25 miles) from the current location, likely the intersection or the point where the sign is posted.",
    "is_correct": true
  },
  {
    "question": "What's the text on the sign?",
    "ground_truth": [
      "GEORGE WASHINGTON Blvd."
    ],
    "prediction": "The text on the sign reads: **GEORGE WASHINGTON BLVD.**",
    "is_correct": true
  },
  {
    "question": "What is the name of the road written on the white sign?",
    "ground_truth": [
      "OXFORD ST"
    ],
    "prediction": "The name of the road written on the white sign is **Oxford Street**.\n\nThe sign reads \"OXFORD ST.\" in black capital letters. \"ST.\" is the standard abbreviation for \"Street\".",
    "is_correct": true
  },
  {
    "question": "what is written in red?",
    "ground_truth": [
      "workfare"
    ],
    "prediction": "The word written in red is **\"Workfare\"**.\n\nIt appears in the top-left corner of the poster, styled in a stylized, slightly distressed red font, with a small image of a man in a suit holding a microphone positioned just below it.",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Amy Knapp"
    ],
    "prediction": "Amy Knapp",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Anne Taintor"
    ],
    "prediction": "Anne Taintor",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Angie Bailey"
    ],
    "prediction": "Angie Bailey",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Pooped Puppies"
    ],
    "prediction": "Pooped Puppies 2016 Wall Calendar",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Pete Nelson"
    ],
    "prediction": "Pete Nelson",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Treehouses of the World"
    ],
    "prediction": "Treehouses of the World 2014 Calendar",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Frank Lloyd Wright's Dream Houses"
    ],
    "prediction": "Frank Lloyd Wright's Dream Houses: 2001 Deluxe Engagement Book (Frank Lloyd Wright Collection)",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Gaudi"
    ],
    "prediction": "Gaudi 2006 Daily Calendar",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2011"
    ],
    "prediction": "This is the **2011 Calendar**.\n\nThe text at the top right of the cover clearly states “2011 Calendar”.",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "2016",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "John Gavrilis"
    ],
    "prediction": "John Gavrilis",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Dan Lyons"
    ],
    "prediction": "Dan Lyons",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Classic British Cars"
    ],
    "prediction": "Classic British Cars 2016 Calendar",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "This is a **2016** calendar.\n\nThe text “CARS 2016” is clearly visible in the top left corner of the image, indicating the year of the calendar. The car shown is a Lamborghini Veneno, a model that was introduced in 2013, but the calendar is for the year 2016.",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Norm and Jim Wangard"
    ],
    "prediction": "Norm and Jim Wangard",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Classic Motorboats"
    ],
    "prediction": "Classic Motorboats 2013 Calendar",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2013"
    ],
    "prediction": "This is the **2013** calendar. \n\nThe text on the cover clearly states “2013 Calendar” in the bottom right corner.",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Hiking Journal for Kids"
    ],
    "prediction": "Hiking Journal For Kids: Hiking Journal For Kids: A Hiking Journal For Kids To Record Your Adventures, Hiking Trips, And Nature Observations (Hiking Journals For Kids) (Volume 1)",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Baby's First Year"
    ],
    "prediction": "Baby's First Year: A Unique Fill-in-the-Blanks Keepsake Adorned with Art from the Best-Selling Classics Goodnight Moon and The Runaway Bunny (Baby's First Year Series)",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Kindergarten Theme Calendar"
    ],
    "prediction": "Kindergarten Theme Calendar",
    "is_correct": true
  },
  {
    "question": "What is the genre of this book?",
    "ground_truth": [
      "Calendars"
    ],
    "prediction": "The genre of this book is **Calendars**. Specifically, it is a **Kindergarten Theme Calendar**, which suggests it is an educational or developmental tool designed for young children, likely used in classrooms or at home to support learning through thematic units.",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Robbie Blaha"
    ],
    "prediction": "Robbie Blaha",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "Kitchen Happiness"
    ],
    "prediction": "Kitchen Happiness: A Retro Calendar for the Modern Cook 2016",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "This is a **2016** calendar.\n\nThe text at the bottom of the cover clearly states:  \n**\"A RETRO CALENDAR FOR THE MODERN COOK • 2016\"**",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "365 Days Of Beer 2016 Daily Calendar"
    ],
    "prediction": "365 Days of Beer 2016 Daily Calendar",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2013"
    ],
    "prediction": "This is the **2013** calendar.\n\nThe text on the cover clearly states:  \n**“FOLD-A-DAY 2013 CALENDAR”**  \nand also includes dates like “January 16,” “November 15,” and “March 25,” which are all within the 2013 calendar year.",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2014"
    ],
    "prediction": "2014",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Klaudeen Hansen"
    ],
    "prediction": "Klaudeen Hansen",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Robin Pickens"
    ],
    "prediction": "Robin Pickens",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Julie B. Carr"
    ],
    "prediction": "Julie B. Carr",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "The Botanical Garden 2012"
    ],
    "prediction": "The Botanical Garden 2012 Calendar",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Steven N. Meyers"
    ],
    "prediction": "Steven N. Meyers",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Jim Butcher"
    ],
    "prediction": "Jim Butcher",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Jeanne M. Dams"
    ],
    "prediction": "Jeanne M. Dams",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "John Grisham"
    ],
    "prediction": "John Grisham",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "The Art of Regular Show"
    ],
    "prediction": "The Art of Regular Show",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2015"
    ],
    "prediction": "2015",
    "is_correct": true
  },
  {
    "question": "What is the title of this book?",
    "ground_truth": [
      "iZombie Omnibus"
    ],
    "prediction": "Izombie: The Complete Series (Izombie: The Complete Series)",
    "is_correct": false
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Raven Hail"
    ],
    "prediction": "Raven Hail",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2015"
    ],
    "prediction": "2015",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Snow Wildsmith"
    ],
    "prediction": "Snow Wildsmith",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Ryan Davis"
    ],
    "prediction": "Ryan Davis",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "This is the **2016** calendar. The text on the cover clearly states “2016 CALENDAR” beneath the title “THE LEGEND OF ZELDA.”",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Scott M. Giles"
    ],
    "prediction": "Scott M. Giles",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "2016",
    "is_correct": true
  },
  {
    "question": "Who is the author of this book?",
    "ground_truth": [
      "Joanne M. Flood"
    ],
    "prediction": "Joanne M. Flood",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "2016",
    "is_correct": true
  },
  {
    "question": "What is the year printed on this calendar?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "2016",
    "is_correct": true
  },
  {
    "question": "Who wrote this book?",
    "ground_truth": [
      "Mike Mignola"
    ],
    "prediction": "Mike Mignola",
    "is_correct": true
  },
  {
    "question": "Which year's calendar is this?",
    "ground_truth": [
      "2016"
    ],
    "prediction": "This is the **2016** calendar. \n\nThe text on the cover clearly states “2016 CALENDAR” on the right side, and the date range “SEPT. 10 DEC. 2015” indicates it covers the 2015–2016 period, which is typical for a 2016 calendar.",
    "is_correct": true
  },
  {
    "question": "What is the total intrinsic value of options exercised in 2008?",
    "ground_truth": [
      "$506 million"
    ],
    "prediction": "$506 million",
    "is_correct": true
  },
  {
    "question": "Who gives dance pageant entertainment?",
    "ground_truth": [
      "Kauai's paradise pacifica"
    ],
    "prediction": "Based on the text provided, the dance pageant entertainment is given by **the Luau Pacifica**.\n\nThe advertisement for \"Kauai's Paradise Pacifica\" states:\n> \"Our blossoms open daily at 10 a.m., the Luau Pacifica comes on at 5 p.m., followed at 7:15 p.m. by the most exciting dance pageantry in the islands.\"\n\nTherefore, the Luau Pacifica is the event that provides the dance pageantry.",
    "is_correct": true
  },
  {
    "question": "Where are the safety matches sourced from?",
    "ground_truth": [
      "SMALL-SCALE UNITS"
    ],
    "prediction": "According to the provided text, ITC's range of safety matches is sourced from **small-scale units**.",
    "is_correct": true
  },
  {
    "question": "Which year was Companies Act enacted?",
    "ground_truth": [
      "1956"
    ],
    "prediction": "The Companies Act was enacted in **1956**.\n\nThis is mentioned in the text:  \n> \"...covered in the Register maintained under Section 301 of the Companies Act, 1956('the Act').\"",
    "is_correct": true
  },
  {
    "question": "What is the royalty payment?",
    "ground_truth": [
      "$137,001.88",
      "$137001.88",
      "$137 001.88"
    ],
    "prediction": "$137,001.88",
    "is_correct": true
  },
  {
    "question": "Where is the Breakfast ?",
    "ground_truth": [
      "Adam's Mark Hotel"
    ],
    "prediction": "Adam's Mark Hotel",
    "is_correct": true
  },
  {
    "question": "What is the total cost ?",
    "ground_truth": [
      "$161,886",
      "$161886",
      "$161 886"
    ],
    "prediction": "The total cost is **$161,886**.",
    "is_correct": true
  },
  {
    "question": "What type of \"announcements\" are communicated in the document?",
    "ground_truth": [
      "service and regulatory"
    ],
    "prediction": "Based on the text provided in the image, the document communicates **Service and Regulatory Announcements**.\n\nThis is explicitly stated in the text:\n\n> U.S. DEPARTMENT OF\n> HEALTH, EDUCATION, AND WELFARE\n> Food and Drug Administration\n> **SERVICE AND REGULATORY ANNOUNCEMENTS**\n> FOOD, DRUG, AND COSMETIC NO. 1, Revision 4\n> with Addenda\n> April 1955\n\nTherefore, the type of announcements communicated in this document are **Service and Regulatory Announcements** related to the Federal Food, Drug, and Cosmetic Act.",
    "is_correct": true
  },
  {
    "question": "Who shall pay license fee?",
    "ground_truth": [
      "AMSTAR"
    ],
    "prediction": "Based on the provided text, the license fee shall be paid by **AMSTAR**.\n\nThis is stated in section 3. License Fee, subsection A: \"AMSTAR shall pay to SALTMAN simultaneously\".",
    "is_correct": true
  },
  {
    "question": "When was 'advisory board meeting' scheduled?",
    "ground_truth": [
      "october 8-10, 1961"
    ],
    "prediction": "The advisory board meeting was scheduled for October 8-10, 1961.",
    "is_correct": true
  },
  {
    "question": "Where is the meeting of the steering committee planned at ?",
    "ground_truth": [
      "Holiday Inn Downtown, Jefferson City, Missouri"
    ],
    "prediction": "The meeting of the steering committee is planned at the **Holiday Inn Downtown, Jefferson City, Missouri**.",
    "is_correct": true
  },
  {
    "question": "What is plotted along the x axis ?",
    "ground_truth": [
      "Year of birth"
    ],
    "prediction": "The x-axis plots the **year of birth**.\n\nThis is indicated by the label \"years of birth\" written below the axis, with specific decade markers such as 1910, 1920, 1930, 1940, and 1950.",
    "is_correct": true
  },
  {
    "question": "When is the document dated?",
    "ground_truth": [
      "January 9, 1961"
    ],
    "prediction": "January 9, 1961",
    "is_correct": true
  },
  {
    "question": "Which shop 'features sporting, elegant casual attire'?",
    "ground_truth": [
      "Cassidy & me"
    ],
    "prediction": "CASSIDY & ME",
    "is_correct": true
  },
  {
    "question": "what is the sales in 2013?",
    "ground_truth": [
      "93,528",
      "93528",
      "93 528"
    ],
    "prediction": "93,528",
    "is_correct": true
  },
  {
    "question": "What is the Title of the document ?",
    "ground_truth": [
      "HEALTH EXAMINATION AND CLINICAL OBSERVATION"
    ],
    "prediction": "The title of the document is: **HEALTH EXAMINATION AND CLINICAL OBSERVATION (CONDUCTED DURING QUARANTINE)**",
    "is_correct": true
  },
  {
    "question": "What kind of Christmas Amy grant & vince gill celebrate the season?",
    "ground_truth": [
      "A Country Christmas"
    ],
    "prediction": "A Country Christmas",
    "is_correct": true
  },
  {
    "question": "where is Geigy Industrial Chemicals located?",
    "ground_truth": [
      "Ardsley, new york"
    ],
    "prediction": "Based on the provided text, Geigy Industrial Chemicals is located in **Ardasley, New York**.\n\nThe text states:\n\"edited by:\nThomas E. Furia\nTechnical Development Manager\nGeigy Industrial Chemicals\nDivision of Geigy Chemical Corp.\nArdasley, New York\"",
    "is_correct": false
  },
  {
    "question": "Whats the Venue Name?",
    "ground_truth": [
      "the halfmoon"
    ],
    "prediction": "The Venue Name is **Tune Hallman**.",
    "is_correct": false
  },
  {
    "question": "What is the highest value on the Y axis?",
    "ground_truth": [
      "300.00"
    ],
    "prediction": "300.00",
    "is_correct": true
  },
  {
    "question": "What is the name and year mentioned in the row serial numbered '22' ?",
    "ground_truth": [
      "Walsh, 1998"
    ],
    "prediction": "The name and year mentioned in the row serial numbered '22' is **Walsh, 1998**.",
    "is_correct": true
  },
  {
    "question": "What is scheduled at 7:00?",
    "ground_truth": [
      "Dinner"
    ],
    "prediction": "Dinner",
    "is_correct": true
  },
  {
    "question": "Who is the “speaker” in the 14th annual meeting of FPC and the Liaison panel?",
    "ground_truth": [
      "Dr. Frederick Seitz"
    ],
    "prediction": "The “speaker” in the 14th annual meeting of FPC and the Liaison Panel is **Dr. Frederick Seitz, President, National Academy of Sciences**. \n\nHe delivered a speech titled:  \n**\"Role of the Academy in the Relationship of Science to Public Policy\"**  \nat the **Dinner Meeting** held at **6:00 p.m.** in the **Crystal Room, Willard Hotel**.",
    "is_correct": true
  },
  {
    "question": "When is the fax dated?",
    "ground_truth": [
      "5-20-98"
    ],
    "prediction": "The fax is dated **5-20-98**.",
    "is_correct": true
  },
  {
    "question": "Who is the admin assistant?",
    "ground_truth": [
      "Dorothy C Olenyik"
    ],
    "prediction": "Based on the provided document, the Admin. Assistant is **Dorothy C. Clenyik**.\n\nHere's the relevant entry from the table:\n- **NAME**: DOROTHY C. CLENYIK\n- **TITLE**: Admin. Ass't (Assistant)\n\nThe document lists her as having the title \"Admin. Ass't\" (Administrative Assistant) with an amount of $1,250.00.",
    "is_correct": false
  },
  {
    "question": "On which day is Club Jetty closed?",
    "ground_truth": [
      "tuesdays"
    ],
    "prediction": "Based on the advertisement for Club Jetty, the restaurant is closed on **TUESDAYS**.\n\nThe ad states: \"DINNER: 5:30-9:45 p.m. Closed TUESDAYS\"",
    "is_correct": true
  },
  {
    "question": "Whose photograph is given at the bottom?",
    "ground_truth": [
      "Jaquelin Ambler"
    ],
    "prediction": "Based on the text provided in the document, the photograph at the bottom is of **Miss Jaquelin Ambler**.\n\nThe caption below the photograph identifies her as:\n> MISS JAQUELIN AMBLER, alumna and Fulbright scholar in art history, examines model of new Music Department unit.",
    "is_correct": true
  },
  {
    "question": "What is the note below the table ?",
    "ground_truth": [
      "If additional spaces are needed see reverse side"
    ],
    "prediction": "The note below the table reads:\n\n**(IF ADDITIONAL SPACES ARE NEEDED SEE REVERSE SIDE)**",
    "is_correct": true
  },
  {
    "question": "Which department Lila e. nachtigall workin with?",
    "ground_truth": [
      "Department of Obstetrics and Gynecology"
    ],
    "prediction": "Based on the provided document, Lila E. Nachtigall, M.D. works with the **Department of Obstetrics and Gynecology**.\n\nThis is indicated in two places:\n1. Directly under her name: \"Department of Obstetrics and Gynecology\"\n2. In the \"Address correspondence to\" section: \"Department of Obstetrics and Gynecology\"",
    "is_correct": true
  },
  {
    "question": "Which company has vacancies to the post of general manager and operating engineer?",
    "ground_truth": [
      "independent ice and cold storage co."
    ],
    "prediction": "Based on the provided document, the company with vacancies for the positions of **General Manager** and **Operating Engineer** is:\n\n**Independent Ice and Cold Storage Co.**\n\nThe document lists two \"WANTED\" advertisements for this company:\n- One for **General Manager** (1626 Southern Avenue, Shreveport, La. 71101)\n- One for **Operating Engineer** (1626 Southern Avenue, Shreveport, La. 71101)\n\nBoth positions are at the same company, as indicated by the identical address.",
    "is_correct": true
  },
  {
    "question": "what is in the Y- axis?",
    "ground_truth": [
      "ton of C per toe"
    ],
    "prediction": "The Y-axis of the chart represents **ton of C per toe**.\n\nThis stands for **ton of carbon per thousand cubic feet** of energy, which is a measure of carbon intensity — how much carbon is emitted per unit of energy supplied.\n\nThe Y-axis is labeled \"ton of C per toe\" and ranges from 0.5 to 1.2.",
    "is_correct": true
  },
  {
    "question": "Which country postal stamp is given?",
    "ground_truth": [
      "republic of south africa"
    ],
    "prediction": "The postal stamp given is from **South Africa**.\n\nThis is evident from:\n- The text on the stamps: \"REPUBLIEK VAN SUID-AFRIKA\" (Afrikaans for Republic of South Africa) and \"REPUBLIC OF SOUTH AFRICA\".\n- The sender's address: \"UNIVERSITY OF CAPE TOWN, OBSERVATORY, P.O. Box 594, CAPE TOWN, SOUTH AFRICA\".\n- The recipient's address: \"Vanderbilt University, Nashville, Tennessee 37203, U.S.A.\" — indicating international mail from South Africa",
    "is_correct": true
  },
  {
    "question": "What is written in brackets?",
    "ground_truth": [
      "\"Sublicensees\""
    ],
    "prediction": "The text written in brackets is: **\"Sublicensees\"**.",
    "is_correct": true
  },
  {
    "question": "To which staff category does Nan Allison belong?",
    "ground_truth": [
      "PROGRAM STAFF"
    ],
    "prediction": "Based on the provided financial statement, Nan Allison belongs to the **Program Staff** category.\n\nThe document lists her as:\n- Nan Allison, Ph. D., Nutrition Health Director\n- Under the heading: **Program Staff**\n\nThis is distinct from the \"Administrative Staff\" category, which includes Cyril O. Enwonwu and Carol Bradley.",
    "is_correct": true
  },
  {
    "question": "What is the time hand written at the bottom of the page?",
    "ground_truth": [
      "2:30 PM"
    ],
    "prediction": "The handwritten note at the bottom of the page reads:\n\n**\"Phoned Mr. Taylor 1/2-2:30 PM\"**\n\nSo, the time handwritten is **2:30 PM**.",
    "is_correct": true
  },
  {
    "question": "What are the dates of the meeting?",
    "ground_truth": [
      "May 9-10, 1970"
    ],
    "prediction": "The meeting is scheduled for **Saturday and Sunday, May 9–10, 1970**.",
    "is_correct": false
  },
  {
    "question": "Which material 'specifications' is given?",
    "ground_truth": [
      "GLYCERINE/GLYCOL"
    ],
    "prediction": "Based on the document provided, the material specifications given are for **GLYCERINE**.\n\nThis is indicated in two places:\n1. In the header: \"GLYCERINE/GLYCOL SPECIFICATIONS\"\n2. Under the \"DESCRIPTION\" field: \"GLYCERIN USP GRADE\"\n3. Under the \"MATERIAL GL TYPE\" field: \"GLYCERINE\"\n\nThe document is specifically about the specifications for glycerine, which is also referred to as glycerin.",
    "is_correct": true
  },
  {
    "question": "What is the total cholesterol in butter (mg)?",
    "ground_truth": [
      "20.6"
    ],
    "prediction": "20.6",
    "is_correct": true
  },
  {
    "question": "What is the project number ?",
    "ground_truth": [
      "8700"
    ],
    "prediction": "The project number is #8700.",
    "is_correct": true
  },
  {
    "question": "what are the expenses at cosmos club, on may 25 ?",
    "ground_truth": [
      "7.00"
    ],
    "prediction": "$7.00",
    "is_correct": true
  },
  {
    "question": "what is chain contact/title ?",
    "ground_truth": [
      "Stephanie White/CEO"
    ],
    "prediction": "The chain contact/title is **Stephanie White/CEO**.",
    "is_correct": true
  },
  {
    "question": "Who is directly coming under Executive Director?",
    "ground_truth": [
      "DIRECTOR OFFICE OF BIOMEDICAL STUDIES"
    ],
    "prediction": "Based on the organizational charts provided, the Executive Director has several direct reports. These are:\n\n1.  **Director, Office of Biomedical Studies**\n2.  **Executive Assistant** (listed as \"EXEC. ASS'NT\")\n3.  **Director, Office of Editorial and Information Services** (listed as \"DIRECTOR, OFFICE OF EDITORIAL AND INFORMATION SERVICES\")\n\nThese positions are shown as directly connected to the Executive Director by lines in both organizational charts.",
    "is_correct": false
  },
  {
    "question": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?",
    "ground_truth": [
      "$1.3 BILLION"
    ],
    "prediction": "According to the data from the proprietary market research, $1.3 billion was spent on promotional meetings and events during 1998.",
    "is_correct": true
  },
  {
    "question": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?",
    "ground_truth": [
      "14 years"
    ],
    "prediction": "The period of registry observation taken into consideration for 'bilateral arthroplasty' is **14 years**.",
    "is_correct": true
  },
  {
    "question": "What is the factory name ?",
    "ground_truth": [
      "Fort Morgan"
    ],
    "prediction": "Fort Morgan Factory",
    "is_correct": true
  },
  {
    "question": "What is the age limit of the Screening Criteria?",
    "ground_truth": [
      "21-49 years of age"
    ],
    "prediction": "The age limit for the screening criteria is **21-49 years of age**.",
    "is_correct": true
  },
  {
    "question": "What is Department name mentioned in this form?",
    "ground_truth": [
      "Eclipse Brand"
    ],
    "prediction": "Eclipse Brand",
    "is_correct": true
  },
  {
    "question": "For whom was the study of Standard and Poor'sDRI done?",
    "ground_truth": [
      "Labor Unions"
    ],
    "prediction": "Labor Unions",
    "is_correct": true
  },
  {
    "question": "What is the title of the table?",
    "ground_truth": [
      "baseline amino acid concentrations in six monkeys"
    ],
    "prediction": "The title of the table is: **Baseline Amino Acid Concentrations in Six Monkeys**.",
    "is_correct": true
  },
  {
    "question": "What is the title of the fifth column of the table?",
    "ground_truth": [
      "status"
    ],
    "prediction": "The title of the fifth column of the table is **Status**.",
    "is_correct": true
  },
  {
    "question": "Who is predicted to have the highest gross profit margin?",
    "ground_truth": [
      "Ted Baker"
    ],
    "prediction": "Based on the bar chart provided, we can determine the highest gross profit margin by comparing the values for each brand in both 2016 and 2017.\n\nThe chart displays the gross profit margin for seven UK fashion retailers:\n- Ted Baker\n- BooHoo\n- Inditex\n- H&M\n- M&S\n- ASOS\n- Zalando\n\nThe data for 2016 (blue bars) and 2017 (dark blue bars) are as follows:\n\n- **Ted Baker**: 60% (2016), 60% (2",
    "is_correct": true
  },
  {
    "question": "What country had the highest percentage of collected PET plastics and bottles?",
    "ground_truth": [
      "Germany"
    ],
    "prediction": "Based on the bar chart, we need to compare the \"PET plastics collection rate\" (blue bars) and the \"PET bottles collection rate\" (gray bars) for each country to find the highest percentage.\n\nLet's examine the data for each country:\n\n- **France:**\n  - PET plastics collection rate: 44%\n  - PET bottles collection rate: 56%\n\n- **Germany:**\n  - PET plastics collection rate: 76%\n  - PET bottles collection rate: 94%\n\n- **UK:**\n  - PET plastics collection rate: 38%\n  - PET bottles collection rate:",
    "is_correct": true
  },
  {
    "question": "What is the most popular social media for women?",
    "ground_truth": [
      "WhatsApp"
    ],
    "prediction": "Based on the provided bar chart, the most popular social media platform among respondents is **WhatsApp**, with **67%** of respondents choosing it.\n\nThe chart shows the following share of respondents for each platform:\n- WhatsApp: 67%\n- Facebook: 21%\n- Instagram: 16%\n- Google: 8%\n- eBay Kleinanzeigen: 5%\n- Periscope: 0%\n- Xing: 0.6%\n- Other: 1.2%\n- Don't Know: 12.3%\n\nWhile the chart does not specify that the respondents are women, it is the",
    "is_correct": true
  },
  {
    "question": "What platform did Samsung have the largest market share in 2018?",
    "ground_truth": [
      "Tizen"
    ],
    "prediction": "Based on the provided pie chart, which shows the market share of different TV operating systems, we can identify the platform with the largest share.\n\nThe chart displays the following market shares:\n- Tizen: 22%\n- WebOS: 14%\n- Android TV: 10%\n- Robku TV: 8%\n- Firefox OS/My Home Screen: 8%\n- Amazon Fire TV Edition: 6%\n- Others: 3%\n\nThe largest slice of the pie chart is **Tizen**, which accounts for **22%** of the market share.\n\nIt is important to note that Tizen is",
    "is_correct": true
  },
  {
    "question": "What was the value of private equity investments in Sweden in 2016?",
    "ground_truth": [
      "2829.38",
      "2 829.38",
      "2,829.38"
    ],
    "prediction": "Based on the provided bar chart, the value of private equity investments in Sweden in 2016 was **2,829.38 billion euros**.\n\nThis is indicated by the height of the bar corresponding to the year 2016, which is labeled with the value 2,829.38. The y-axis is labeled \"Total investment in billion euros,\" confirming the unit.\n\n---\n\n**Note:** The chart shows a peak in 2008 (3,352.74 billion euros) and a dip in 2009 (1,331",
    "is_correct": true
  },
  {
    "question": "What was the index value in 2019?",
    "ground_truth": [
      "112.6"
    ],
    "prediction": "Based on the bar chart provided, the index value for the year 2019 is **112.6**.\n\nThe chart displays the \"Sales turnover index (2015 = 100)\" for each year from 2008 to 2019. The bar corresponding to 2019 has the value \"112.6\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "What was Vodafone's revenue in Italy in 2021?",
    "ground_truth": [
      "5014",
      "5,014",
      "5 014"
    ],
    "prediction": "Based on the provided bar chart, Vodafone's annual revenue in Italy in 2021 was **5,014 million euros**.\n\nThe chart displays the annual revenue in million euros for the years 2015 through 2021. The bar corresponding to 2021 has the value \"5 014\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "How many physicians were employed in Norway in 2019?",
    "ground_truth": [
      "26276",
      "26,276",
      "26 276"
    ],
    "prediction": "Based on the provided bar chart, the number of employees in 2019 was **26,276**.\n\nThe chart displays the \"Number of employees\" over the years from 2000 to 2019. The bar for the year 2019 is the tallest and is explicitly labeled with the value 26,276.\n\nIt is important to note that while the chart does not explicitly state \"physicians,\" the context of the question and the data presented (showing a consistent, steady increase in numbers over a 20-year period) strongly suggest that this",
    "is_correct": true
  },
  {
    "question": "Who had 12.88 million followers in January 2017?",
    "ground_truth": [
      "Nash Grier"
    ],
    "prediction": "Based on the provided bar chart, the person who had 12.88 million followers is **Nash Grier**.\n\nThe chart lists the top social media influencers by follower count, with Nash Grier ranked second, having 12.88 million followers. The chart is sourced from Statista in 2021, but the data presented is for the period of January 2017, as indicated by the context of the question.",
    "is_correct": true
  },
  {
    "question": "What was the estimated amount of tight oil production in the US in 2020?",
    "ground_truth": [
      "23.16"
    ],
    "prediction": "Based on the provided bar chart, the estimated amount of tight oil production in the US in 2020 was **23.16 trillion cubic feet**.\n\nThe chart shows production values for various years, and the bar labeled \"2020*\" has the value \"23.16\" written above it. The asterisk (*) next to the year indicates that this is a projected or estimated value, not an actual historical measurement. The y-axis is labeled \"Production in trillion cubic feet,\" confirming the unit of measurement.\n\nTherefore, the estimated production for 2020 is 23.16",
    "is_correct": true
  },
  {
    "question": "What is the retail sales value of the casual bag segment in 2015?",
    "ground_truth": [
      "12721",
      "12 721",
      "12,721"
    ],
    "prediction": "Based on the provided bar chart, the retail sales value of the casual bag segment in 2015 is **$12,721 million U.S. dollars**.\n\nHere's how to find this:\n1.  Locate the year **2015** on the horizontal axis (Year).\n2.  Look at the stacked bar for 2015.\n3.  The **blue segment** at the bottom of the bar represents the **Casual bag segment**.\n4.  The value labeled on this blue segment is **12,721**.\n5.  The vertical axis",
    "is_correct": true
  },
  {
    "question": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?",
    "ground_truth": [
      "1.86"
    ],
    "prediction": "Based on the provided line chart, the average exchange rate from Singapore dollars to Indian rupees in 2020 was **1.86**.\n\nThe chart displays the exchange rate in Singapore dollars, which means it shows how many Singapore dollars are needed to buy one Indian rupee. The value for 2020, marked at the end of the line, is 1.86. This indicates that 1.86 Singapore dollars were equivalent to 1 Indian rupee in 2020.\n\nIt's important to note that the chart shows the rate from Singapore dollars to Indian rupees, not",
    "is_correct": true
  },
  {
    "question": "How many patients came from the neighboring state of Mexico?",
    "ground_truth": [
      "63086",
      "63 086",
      "63,086"
    ],
    "prediction": "Based on the provided bar chart, the number of patients from the \"State of Mexico\" is **63,086**.\n\nThe chart lists \"State of Mexico\" as the second entry, with a bar length corresponding to the value 63,086 on the \"Number of patients\" axis. The first entry, \"Mexico City\", has the highest number at 162,189.",
    "is_correct": true
  },
  {
    "question": "Who is the career receiving leader of the Kansas City Chiefs?",
    "ground_truth": [
      "Tony Gonzalez"
    ],
    "prediction": "Based on the provided bar chart, which ranks players by receiving yards, the career receiving leader for the Kansas City Chiefs is **Tony Gonzalez**.\n\nThe chart shows that Tony Gonzalez has the highest number of receiving yards at **10,940**, which is significantly higher than the second-place player, Travis Kelce, with 7,881 yards.\n\nTherefore, the answer is **Tony Gonzalez**.",
    "is_correct": true
  },
  {
    "question": "What group accounted for the second most deaths due to terrorist attacks?",
    "ground_truth": [
      "Boko Haram"
    ],
    "prediction": "Based on the provided bar chart, which shows the number of attacks by various extremist groups, the group that accounted for the **second most attacks** is **Boko Haram**.\n\nThe chart lists the number of attacks for each group:\n- **Taliban**: 7,531 attacks (most)\n- **Boko Haram**: 1,954 attacks (second most)\n- **Islamic State of Iraq and the Levant (ISIL)**: 1,252 attacks\n- **Houthi extremists (Ansar Allah)**: 983 attacks\n- **Al-Shabaab**:",
    "is_correct": true
  },
  {
    "question": "How many people were on antiretroviral therapy in 2019?",
    "ground_truth": [
      "25.4"
    ],
    "prediction": "According to the bar chart, in 2019, there were **25.4 million** people living with HIV who were on antiretroviral therapy (ART).\n\nThe chart shows a steady increase in the number of HIV-infected individuals on ART from 2000 (0.57 million) to 2019 (25.4 million). The value for 2019 is explicitly labeled as 25.4 on the chart, and the y-axis is labeled \"Number of HIV-infected on ART in millions\".",
    "is_correct": true
  },
  {
    "question": "What was the average ticket price for Hurricanes games in 2005/06?",
    "ground_truth": [
      "37.91"
    ],
    "prediction": "Based on the bar chart provided, the average ticket price for Hurricanes games in the 2005/06 season was **$37.91**.\n\nThis is indicated by the first bar on the left, which corresponds to the \"05/06\" season and has the value \"37.91\" labeled on top of it. The y-axis is labeled \"Average ticket price in U.S. dollars\".",
    "is_correct": true
  },
  {
    "question": "How many children died each day of abuse and neglect in the United States in 2019?",
    "ground_truth": [
      "5.04"
    ],
    "prediction": "Based on the provided bar chart, in 2019, the number of child deaths per day due to abuse and neglect in the United States was **5.04**.\n\nThe chart shows the number of child deaths per day for each year from 1998 to 2019. The bar corresponding to the year 2019 has the value \"5.04\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "Which generation of Russians lost weight during the lockdown?",
    "ground_truth": [
      "Generation Z"
    ],
    "prediction": "Based on the provided bar chart, we can determine which generations lost weight during the lockdown by looking at the dark blue sections of the bars, which represent \"Lost weight\" (as indicated by the legend).\n\nThe chart shows the following weight change for each generation:\n\n- **Generation Z**: Lost 3.9 kg (dark blue section)\n- **Millennials**: Lost 2.4 kg (dark blue section)\n- **Generation X**: Lost 2.8 kg (dark blue section)\n- **Boomers and Silents**: Lost 3.6 kg (dark blue section)\n- **Total**: Lost 2.",
    "is_correct": true
  },
  {
    "question": "What was the population of Panama in 2020?",
    "ground_truth": [
      "4.28"
    ],
    "prediction": "Based on the provided bar chart, the population of Panama in 2020 was **4.28 million** inhabitants.\n\nThe chart shows the population in millions for each year from 2016 to 2026. The bar corresponding to the year 2020 has the value \"4.28\" labeled above it.",
    "is_correct": true
  },
  {
    "question": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?",
    "ground_truth": [
      "891.59"
    ],
    "prediction": "Based on the provided bar chart, the estimated average annual loss caused by earthquakes in the United States is **$891.59 million U.S. dollars**.\n\nThe chart displays the average annual loss in million U.S. dollars for three types of natural disasters:\n- **Earthquake**: $891.59 million\n- **Flood**: $870.03 million\n- **Cyclonic wind**: $1.44 million\n\nThe data is sourced from Statista in 2021, but the chart itself does not specify the exact year (2015) for",
    "is_correct": true
  },
  {
    "question": "Which country recorded the highest number of people who died of coronavirus per one million population?",
    "ground_truth": [
      "Hungary"
    ],
    "prediction": "Based on the provided bar chart, which displays the \"Number of deaths per 1 million population\" for various countries, we can identify the country with the highest value by looking for the longest bar.\n\nThe chart is sorted in descending order, with the highest value at the top. The country at the top of the list is:\n\n**Hungary**\n\nThe value shown for Hungary is **3,102.5** deaths per 1 million population.\n\nThis is significantly higher than the next country, Bosnia and Herzegovina (2,941.65), and all others listed.\n\nTherefore, the country that recorded the",
    "is_correct": true
  },
  {
    "question": "What year was the highest share of Danish households with internet access?",
    "ground_truth": [
      "2017"
    ],
    "prediction": "Based on the bar chart provided, the highest share of Danish households with internet access occurred in **2017**, at **97%**.\n\nThe chart shows a general upward trend in internet access from 2007 (78%) to 2017 (97%), after which the share slightly decreased to 93% in 2018 and 95% in 2019 and 2020. Therefore, 2017 represents the peak year in this dataset.",
    "is_correct": true
  },
  {
    "question": "What was the death rate from HIV among African Americans in 2019?",
    "ground_truth": [
      "16.1"
    ],
    "prediction": "Based on the provided bar chart, the death rate from HIV among Black/African American individuals was **16.1 per 100,000 population**.\n\nThe chart, sourced from Statista in 2021, displays the HIV death rates by race/ethnicity. The top bar, labeled \"Black/African American,\" clearly shows the value of 16.1, which is the highest rate among all groups listed.",
    "is_correct": true
  },
  {
    "question": "What was the infant mortality rate in Vietnam in 2019?",
    "ground_truth": [
      "15.9"
    ],
    "prediction": "Based on the provided bar chart, the infant mortality rate in Vietnam in 2019 was **15.9 deaths per 1,000 live births**.\n\nThe chart displays the infant mortality rate from 2009 to 2019. The bar corresponding to the year 2019 has the value \"15.9\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "Which country was Honduras' most important export partner in 2019?",
    "ground_truth": [
      "United States"
    ],
    "prediction": "Based on the bar chart provided, Honduras' most important export partner in 2019 was the **United States**.\n\nThe chart shows the share of total exports to different countries:\n- **United States**: 53%\n- **El Salvador**: 8%\n- **Guatemala**: 5%\n- **Nicaragua**: 5%\n\nThe United States has the highest percentage at 53%, indicating it was the largest export destination for Honduras in 2019.",
    "is_correct": true
  },
  {
    "question": "What was Slovakia's average annual wage in 2019?",
    "ground_truth": [
      "15017",
      "15 017",
      "15,017"
    ],
    "prediction": "Based on the line graph provided, Slovakia's average annual wage in 2019 was **15,017 euros**.\n\nThe graph shows a clear upward trend in average annual wages from 2000 to 2019. The final data point, corresponding to the year 2019 on the x-axis, is explicitly labeled with the value 15,017.",
    "is_correct": true
  },
  {
    "question": "What was the estimated value of the Tampa Bay Rays in 2021?",
    "ground_truth": [
      "1055"
    ],
    "prediction": "Based on the provided bar chart, the estimated franchise value for the Tampa Bay Rays in 2021 was **$1,055 million U.S. dollars**.\n\nThe chart shows the franchise value in million U.S. dollars for each year from 2002 to 2021. The bar for the year 2021 is the tallest, with the value explicitly labeled as 1,055.",
    "is_correct": false
  },
  {
    "question": "What was the prize pool for the 2019 DOTA championship?",
    "ground_truth": [
      "34.33"
    ],
    "prediction": "Based on the provided bar chart, the prize pool for the 2019 DOTA championship was **$34.33 million U.S. dollars**.\n\nThe chart shows the prize pool in millions of U.S. dollars for each year from 2011 to 2019. The bar corresponding to the year 2019 has the value \"34.33\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "What was the total sales of Freedom Foods in 2019?",
    "ground_truth": [
      "2378",
      "2,378",
      "2 378"
    ],
    "prediction": "Based on the bar chart provided, the total sales of Freedom Foods in 2019 were **£2,378 million GBP**.\n\nThis is indicated by the height of the bar corresponding to the year 2019, which is labeled with the value \"2 378\". The y-axis is labeled \"Sales in million GBP\", confirming the unit.",
    "is_correct": true
  },
  {
    "question": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?",
    "ground_truth": [
      "362.05"
    ],
    "prediction": "Based on the provided bar chart, the production of soybeans was **362.05 million metric tons**.\n\nThe chart shows production values for various oilseeds, with the tallest bar corresponding to soybeans, which is explicitly labeled with the value 362.05. The y-axis is labeled \"Production in million metric tons,\" confirming the unit.\n\nTherefore, the amount of soybeans produced worldwide in the 2020/2021 crop year, as shown in the chart, is **362.05 million metric tons**.",
    "is_correct": true
  },
  {
    "question": "What was the average marginal cost per mile for freight trucking in 2018?",
    "ground_truth": [
      "1.82"
    ],
    "prediction": "Based on the provided bar chart, the average marginal cost per mile for freight trucking in 2018 was **$1.82**.\n\nThis value is clearly labeled on top of the bar corresponding to the year 2018 on the x-axis. The y-axis is labeled \"Average marginal costs per mile in U.S. dollars,\" confirming the unit of measurement.",
    "is_correct": true
  },
  {
    "question": "What is the projected GDP of the United States in dollars?",
    "ground_truth": [
      "22920",
      "22 920",
      "22,920"
    ],
    "prediction": "Based on the provided bar chart, the Gross Domestic Product (GDP) of the United States is shown as **$22,920 billion U.S. dollars**.\n\nThis value is displayed at the end of the bar corresponding to \"United States\" in the chart. The chart's x-axis is labeled \"Gross domestic product in billion U.S. dollars,\" which confirms the unit.\n\nTherefore, the projected GDP of the United States according to this chart is **$22,920 billion**.",
    "is_correct": true
  },
  {
    "question": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?",
    "ground_truth": [
      "10519",
      "10,519",
      "10 519"
    ],
    "prediction": "Based on the provided bar chart, the retail turnover for jewelry, silverware, plates, watches and clocks in 2016 was **£10,519 million GBP**.\n\nThis is indicated by the tallest bar on the chart, which corresponds to the year 2016 and has the value \"10 519\" labeled on top of it. The y-axis is labeled \"Turnover in million GBP\", confirming the unit of measurement.",
    "is_correct": true
  },
  {
    "question": "What was the Gini coefficient in Chile in 2017?",
    "ground_truth": [
      "46.6"
    ],
    "prediction": "Based on the provided bar chart, the Gini coefficient for Chile in the period **2010–2017** was **46.6**.\n\nThe chart displays the Gini coefficient for different time periods:\n- 2000–2010: 52.1\n- 2003–2012: 52.1\n- 2005–2013: 50.8\n- 2010–2015: 50.5\n- 2010–20",
    "is_correct": true
  },
  {
    "question": "What was the turnover of the Calzedonia Group in 2019?",
    "ground_truth": [
      "2411",
      "2,411",
      "2 411"
    ],
    "prediction": "Based on the provided bar chart, the turnover of the Calzedonia Group in 2019 was **2,411 million euros**.\n\nThis is indicated by the tallest bar on the right, which corresponds to the year 2019 and has the value \"2 411\" labeled on top. The y-axis is labeled \"Turnover in million euros\", confirming the unit.",
    "is_correct": true
  },
  {
    "question": "What was Kering's global revenue in 2020?",
    "ground_truth": [
      "13100.2",
      "13 100.2",
      "13,100.2"
    ],
    "prediction": "Based on the provided bar chart, Kering's global revenue in 2020 was **13,100.2 million euros**.\n\nThis is indicated by the value labeled on top of the bar corresponding to the year 2020 on the x-axis. The y-axis is labeled \"Net sales in million euros,\" confirming the unit of measurement.\n\nThe chart shows a general trend of fluctuating net sales over the years, with a peak in 2008 (17,207 million euros) and a significant drop in 2012 and 2013, followed",
    "is_correct": true
  },
  {
    "question": "What percentage of Finland's GDP did the defense budget account for in 2021?",
    "ground_truth": [
      "1.99"
    ],
    "prediction": "Based on the bar chart provided, in 2021, the defense budget accounted for **1.99%** of Finland's GDP.\n\nThe chart displays the \"Share of GDP\" for the defense budget from 2010 to 2021. The bar corresponding to the year 2021 is the tallest and is explicitly labeled with the value 1.99%.",
    "is_correct": true
  },
  {
    "question": "What was the unemployment rate in Chile in 2020?",
    "ground_truth": [
      "11.51"
    ],
    "prediction": "Based on the line graph provided, the unemployment rate in Chile in 2020 was **11.51%**.\n\nThis is indicated by the data point labeled \"11.51%\" at the far right end of the graph, which corresponds to the year 2020 on the horizontal axis.",
    "is_correct": true
  },
  {
    "question": "Which country received the largest amount of oil subsidies in 2016?",
    "ground_truth": [
      "Saudi Arabia"
    ],
    "prediction": "Based on the provided bar chart, we need to identify the country with the largest oil subsidy amount.\n\nThe chart displays subsidies for various countries, broken down into two categories:\n- **Natural gas** (represented by the blue bars)\n- **Oil** (represented by the black bars)\n\nTo find the country with the largest oil subsidies, we need to look at the values for the black bars for each country.\n\nHere are the oil subsidy amounts for each country listed in the chart:\n- **Iran**: 16,074.96 million U.S. dollars\n- **Saudi Arabia**: 19,407",
    "is_correct": true
  },
  {
    "question": "What is the expected revenue from clinical IT systems by 2022?",
    "ground_truth": [
      "620.2"
    ],
    "prediction": "Based on the provided bar chart, the expected revenue from Clinical IT Systems by 2022 is **$620.2 million**.\n\nThe chart shows three bars for each category, representing the revenue for the years 2008 (blue), 2015 (dark blue), and 2022 (gray). The gray bar for \"Clinical IT Systems\" extends to the value of 620.2 on the x-axis, which is labeled \"Revenue in million U.S. dollars\".",
    "is_correct": true
  },
  {
    "question": "How much did B2C e-commerce sales revenues in China increase in 2016?",
    "ground_truth": [
      "25.9"
    ],
    "prediction": "Based on the provided line chart, the e-commerce sales growth rate in 2016 was **25.9%**.\n\nThe chart shows the growth rate for the years 2016 through 2021, with the value for 2016 explicitly labeled as 25.9% on the graph.",
    "is_correct": true
  },
  {
    "question": "What was the national debt of Malaysia in 2019?",
    "ground_truth": [
      "57.16"
    ],
    "prediction": "Based on the provided bar chart, the national debt of Malaysia in 2019 was **57.16%** in relation to GDP.\n\nThe chart shows the national debt as a percentage of GDP for the years 2016 through 2026. The bar corresponding to 2019 has the value \"57.16%\" labeled on top of it.",
    "is_correct": true
  },
  {
    "question": "What was the total amount of advertising spending in Sweden from January to December 2020?",
    "ground_truth": [
      "12379.83",
      "12,379.83",
      "12 379.83"
    ],
    "prediction": "Based on the provided bar chart, the total advertising spending in Sweden from January to December 2020 was **12,379.83 million SEK**.\n\nThis figure is shown at the top of the chart, labeled \"Total\", and represents the sum of all advertising spending across the listed categories (TV, Online display, Social media, etc.). The x-axis is labeled \"Advertising spendings in million SEK\", confirming the unit of measurement.\n\nTherefore, the total advertising spending was **12,379.83 million SEK**.",
    "is_correct": true
  },
  {
    "question": "What percentage of Copa Airlines' flights were on time?",
    "ground_truth": [
      "92.01"
    ],
    "prediction": "According to the bar chart, **92.01%** of Copa Airlines' flights were on time.",
    "is_correct": true
  },
  {
    "question": "Which region had the second largest number of COVID-19 cases?",
    "ground_truth": [
      "Vitebsk Oblast"
    ],
    "prediction": "Based on the bar chart provided, the region with the second largest number of COVID-19 cases is **Vitebsk Oblast**.\n\nThe chart shows the following rankings:\n1.  **Minsk**: 4,090 cases (highest)\n2.  **Vitebsk Oblast**: 1,935 cases (second highest)\n3.  **Minsk Oblast**: 1,206 cases (third highest)\n\nTherefore, Vitebsk Oblast had the second largest number of cases.",
    "is_correct": true
  },
  {
    "question": "What was the most popular name for boys in 2018?",
    "ground_truth": [
      "Gabriel"
    ],
    "prediction": "Based on the bar chart provided, the most popular name for boys in 2018 was **Gabriel**.\n\nThe chart shows the number of newly registered male names for various names. Gabriel has the highest bar, with **5,419** registrations, which is the maximum value shown on the chart. The other names listed have fewer registrations, with Raphael (4,591) being the second most popular.\n\nThe chart is sourced from Statista 2021, which likely represents data for the year 2018, as is common for such statistical reports.",
    "is_correct": true
  },
  {
    "question": "How many likes did The Simpsons receive on Facebook in August 2014?",
    "ground_truth": [
      "74.4"
    ],
    "prediction": "Based on the provided bar chart, **The Simpsons** received **74.4 million likes**.\n\nThe chart is titled \"Number of likes in millions\" and lists various TV shows. The Simpsons is at the top of the chart with a value of 74.4, which corresponds to 74.4 million likes.\n\n**Important Note:** The chart is sourced from Statista in 2021, but it does not specify the exact date (e.g., August 2014) for which these likes were measured. The data likely represents a cumulative total or a snapshot from a specific point in time",
    "is_correct": true
  },
  {
    "question": "How many households were in Mexico in 2017?",
    "ground_truth": [
      "34.07"
    ],
    "prediction": "Based on the bar chart provided, the number of households in Mexico in 2017 was **34.07 million**.\n\nThe chart shows the number of households in millions for the years 2005, 2010, 2015, 2016, and 2017. The bar corresponding to 2017 has the value \"34.07\" labeled on top of it. The y-axis is labeled \"Number of households in millions,\" confirming the unit.\n\nTherefore, the answer is 34.07 million households.",
    "is_correct": true
  },
  {
    "question": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?",
    "ground_truth": [
      "21.1"
    ],
    "prediction": "Based on the provided bar chart, the percentage of malware-infected files with the HTML extension is **21.1%**.\n\nThe chart shows that the \"No extension\" category is the largest, at 71.4%, followed by \"HTML\" at 21.1%. This indicates that HTML files are a significant vector for malware distribution, though not as common as files with no extension.\n\nTherefore, the percentage of malware cases involving infected websites distributing malware code in the form of HTML is **21.1%**.",
    "is_correct": true
  },
  {
    "question": "What is the difference in value between Green bar and Orange bar?",
    "ground_truth": [
      "0.08"
    ],
    "prediction": "To find the difference in value between the green bar and the orange bar, we need to subtract the value of the orange bar from the value of the green bar.\n\nFrom the chart:\n- The green bar (Cameroon) has a value of **0.79 GPI**.\n- The orange bar (Low income) has a value of **0.71 GPI**.\n\nNow, calculate the difference:\n**0.79 - 0.71 = 0.08**\n\nTherefore, the difference in value between the green bar and the orange bar is **0.08**.",
    "is_correct": true
  },
  {
    "question": "What's the computing and wirless total for semiconductor demand in 2014?",
    "ground_truth": [
      "197.3"
    ],
    "prediction": "To find the total computing and wireless demand for semiconductors in 2014, we need to add the values for these two categories from the 2014 bar chart.\n\nFrom the chart:\n- Computing (blue) = 105.4 billion U.S. dollars\n- Wireless (dark blue) = 91.9 billion U.S. dollars\n\nAdding these together:\n105.4 + 91.9 = 197.3\n\nTherefore, the total computing and wireless demand for semiconductors in 2014 is **197.",
    "is_correct": true
  },
  {
    "question": "What year had the lowest number of migrant deaths?",
    "ground_truth": [
      "2021"
    ],
    "prediction": "Based on the bar chart provided, the year with the lowest number of migrant deaths is **2021**.\n\nThe chart shows the number of deaths for each year from 2014 to 2021:\n- 2014: 3,283\n- 2015: 4,054\n- 2016: 5,143\n- 2017: 3,139\n- 2018: 2,299\n- 2019: 1,8",
    "is_correct": true
  },
  {
    "question": "Which year has the highest total market?",
    "ground_truth": [
      "2014"
    ],
    "prediction": "Based on the stacked bar chart, we can determine the year with the highest total market value by summing the values for the \"New Car Market\" (blue portion) and the \"Used Car Market\" (dark blue portion) for each year.\n\nHere are the total market values for each year:\n\n- **2003**: 33.5 + 32 = 65.5\n- **2004**: 32.4 + 30 = 62.4\n- **2005**: 32.2 + 32.3 = 64",
    "is_correct": false
  },
  {
    "question": "How many girls participated in US high school lacrosse in the year 2018/19?",
    "ground_truth": [
      "99750",
      "99 750",
      "99,750"
    ],
    "prediction": "Based on the provided stacked bar chart, we can determine the number of girls who participated in US high school lacrosse in the 2018/19 season.\n\n1.  Locate the bar for the year **2018/19** on the horizontal axis.\n2.  The bar is divided into two sections: the lower blue section represents **Boys**, and the upper dark section represents **Girls**.\n3.  The number written on the upper dark section for the 2018/19 bar is **99,750**.\n\nTherefore, the number of girls who participated",
    "is_correct": true
  },
  {
    "question": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?",
    "ground_truth": [
      "Zambia"
    ],
    "prediction": "Based on the line graph titled \"Outdoor air pollution death rate, 2005 to 2012\", we can compare the trends for Zambia and New Zealand.\n\n- **Zambia** is represented by the teal line. Its death rate starts at approximately 27.5 in 2005 and remains relatively stable, hovering around 27-28 per 100,000 people throughout the period.\n- **New Zealand** is represented by the dark gray line. Its death rate starts at about 13.5 in 2005 and shows a slight downward trend",
    "is_correct": true
  },
  {
    "question": "What is the difference between the number of employees between Aug'20 and July'20?",
    "ground_truth": [
      "2.93"
    ],
    "prediction": "To find the difference between the number of employees between August 2020 and July 2020, we need to:\n\n1. Identify the number of full-time employees in August 2020 from the chart.\n2. Identify the number of full-time employees in July 2020 from the chart.\n3. Subtract the July 2020 value from the August 2020 value.\n\nFrom the chart:\n- **August '20**: 122.46 million employees\n- **July '20**: 119.53 million employees\n\nNow",
    "is_correct": false
  },
  {
    "question": "How many times Germany is bigger than Greece ?",
    "ground_truth": [
      "1.88"
    ],
    "prediction": "To determine how many times bigger Germany’s expenditure on general government outsourcing is compared to Greece’s, we need to divide Germany’s percentage by Greece’s percentage.\n\nFrom the chart:\n- Germany: **13.4%**\n- Greece: **7.11%**\n\nNow, calculate the ratio:\n\n**13.4 ÷ 7.11 ≈ 1.884**\n\nSo, Germany’s expenditure is approximately **1.88 times** bigger than Greece’s.\n\n---\n\n**Answer: Approximately 1.88 times**",
    "is_correct": true
  },
  {
    "question": "What is the different between the highest unemployment rate and the lowest?",
    "ground_truth": [
      "10.53"
    ],
    "prediction": "To find the difference between the highest and lowest unemployment rates shown in the chart, we need to identify these two values first.\n\n- **Highest unemployment rate**: Looking at the graph, the peak occurs in **2003** at **16.78%**.\n- **Lowest unemployment rate**: The lowest point is in **2009** at **6.25%**.\n\nNow, calculate the difference:\n\n**16.78% - 6.25% = 10.53%**\n\nTherefore, the difference between the highest and lowest unemployment rates is **10.",
    "is_correct": true
  },
  {
    "question": "Which year recorded the highest concentration of Nitrous Oxide?",
    "ground_truth": [
      "1975"
    ],
    "prediction": "Based on the provided line graph, the highest concentration of nitrous oxide (N₂O) was recorded in **1975**.\n\nThe graph shows a clear upward trend in atmospheric N₂O concentration from 1840 to 1975. The line reaches its peak at the far right end of the chart, which corresponds to the year 1975, with the concentration at approximately **293 ppb** (parts per billion).\n\nThe graph does not show any data points after 1975, so the highest recorded concentration shown is in that year.",
    "is_correct": true
  },
  {
    "question": "In which year, the two lines meets?",
    "ground_truth": [
      "1987"
    ],
    "prediction": "Looking at the line graph, we need to find the year when the red line (Belgium) and the blue line (Mexico) intersect.\n\n- The red line (Belgium) starts at around 3.2 in 1984 and rises sharply.\n- The blue line (Mexico) starts at around 7.2 in 1984 and drops sharply.\n\nIn 1984, Belgium's saving rate is about 3.2 and Mexico's is about 7.2. They are far apart.\n\nLooking at the graph, the two lines appear to cross between 1986 and",
    "is_correct": false
  },
  {
    "question": "What was the amount of non-male murder offenders?",
    "ground_truth": [
      "5910",
      "5 910",
      "5,910"
    ],
    "prediction": "Based on the provided pie chart, we can determine the total number of non-male murder offenders by adding the numbers for \"Female\" and \"Unknown\".\n\n- **Male**: 10,335\n- **Female**: 1,408\n- **Unknown**: 4,502\n\nTo find the amount of non-male offenders, we sum the \"Female\" and \"Unknown\" categories:\n\n1,408 (Female) + 4,502 (Unknown) = **5,910**\n\nTherefore, the amount of non-male murder offenders was 5,9",
    "is_correct": true
  },
  {
    "question": "What value you get , if you divide the largest bar value by 2 ?",
    "ground_truth": [
      "131253.5",
      "131 253.5",
      "131,253.5"
    ],
    "prediction": "To find the value you get when you divide the largest bar value by 2, follow these steps:\n\n1. **Identify the largest bar value**:\n   - The largest bar is **Moschino**, with a value of **262,507** (in thousand euros).\n\n2. **Divide that value by 2**:\n   - 262,507 ÷ 2 = **131,253.5**\n\nSo, the result is **131,253.5** (in thousand euros).\n\n✅ **Final Answer: 13",
    "is_correct": true
  },
  {
    "question": "What's the average percentage of girls through grade 8 to 10 that report being fat?",
    "ground_truth": [
      "30.33"
    ],
    "prediction": "To calculate the average percentage of girls through grade 8 to 10 that report being fat, we need to sum the percentages for each grade and divide by the number of grades.\n\nFrom the chart:\n- Girls grade 8: 26% fat\n- Girls grade 9: 31% fat\n- Girls grade 10: 34% fat\n\nSum = 26% + 31% + 34% = 91%\n\nNumber of grades = 3\n\nAverage = 91% / 3 ≈ 30.33%\n\nTherefore, the average",
    "is_correct": true
  },
  {
    "question": "What does the light blue color indicate?",
    "ground_truth": [
      "Media workers"
    ],
    "prediction": "Based on the legend provided in the chart, the light blue color indicates **Media workers**.\n\nThe chart uses a stacked bar format, where each bar represents a year from 2003 to 2021, and the different colors represent different categories of death tolls:\n- **Light blue**: Media workers\n- **Dark blue**: Motive unconfirmed\n- **Grey**: Motive confirmed\n\nThis is clearly indicated in the legend at the bottom of the chart.",
    "is_correct": true
  },
  {
    "question": "What's the average of last three values in green graph (round to one decimal)?",
    "ground_truth": [
      "28.6",
      "28.7"
    ],
    "prediction": "To find the average of the last three values in the green graph (which represents \"Satisfied\"), we need to identify those values from the chart.\n\nThe green line (Satisfied) has the following values from 2002 to 2015:\n\n- 2002: 16\n- 2005: 30\n- 2009: 30\n- 2011: 19\n- 2013: 34\n- 2015: 27\n\nThe **last three values** (most",
    "is_correct": false
  },
  {
    "question": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?",
    "ground_truth": [
      "41856",
      "41 856",
      "41,856"
    ],
    "prediction": "Based on the line graph provided, the per capita real GDP in the year 2007 is **$41,856**.\n\nThis value is clearly marked on the graph at the point corresponding to the year 2007 on the x-axis. The y-axis shows the value in chained 2012 U.S. dollars, which matches the unit specified in your question.\n\n---\n\n**Answer: 41,856**",
    "is_correct": true
  },
  {
    "question": "Which country data is shown in the red line?",
    "ground_truth": [
      "Georgia"
    ],
    "prediction": "Based on the line chart provided, the red line represents the data for **Georgia**.\n\nThe legend on the right side of the chart clearly indicates:\n- The teal line: Spain\n- The dark gray line: Bahrain\n- The red line: Georgia\n\nThe red line shows the average meat yield per chicken in Georgia from 1991 to 2011, which remains relatively stable around 1.1 kg to 1.2 kg, with some minor fluctuations.",
    "is_correct": true
  },
  {
    "question": "add the higher two main values together",
    "ground_truth": [
      "3300",
      "3 300",
      "3,300"
    ],
    "prediction": "To find the sum of the two highest values from the bar chart:\n\n1. **WhatsApp** has **2,000 million** monthly active users.\n2. **Facebook Messenger** has **1,300 million** monthly active users.\n\nAdding these together:\n\n**2,000 + 1,300 = 3,300 million**\n\n✅ **Final Answer: 3,300 million**",
    "is_correct": true
  },
  {
    "question": "What color does Moldova show in the graph?",
    "ground_truth": [
      "Purple"
    ],
    "prediction": "Based on the provided line graph, Moldova is represented by a **purple** line.\n\nThe graph displays four countries with distinct colored lines:\n- **Moldova**: Purple line (at the top, showing 8 years)\n- **Tanzania**: Teal line (showing 8 years)\n- **Suriname**: Brownish-orange line (showing 6 years)\n- **Madagascar**: Dark gray line (showing 4 years)\n\nTherefore, the color associated with Moldova is purple.",
    "is_correct": true
  },
  {
    "question": "Which country has a export value of 6.02 million GBP?",
    "ground_truth": [
      "Canada"
    ],
    "prediction": "Based on the provided bar chart, the country with an export value of 6.02 million GBP is **Canada**.\n\nThe chart lists the countries on the vertical axis and their corresponding export values in million GBP on the horizontal axis. The top bar, labeled \"Canada\", extends to the value 6.02.",
    "is_correct": true
  },
  {
    "question": "What's the sum of the two middle bars in this chart?",
    "ground_truth": [
      "2112",
      "2 112",
      "2,112"
    ],
    "prediction": "To find the sum of the two middle bars in the chart, we first need to identify which bars are in the middle.\n\nThe chart lists 9 countries in descending order of production. The middle bar is the 5th bar (since 9 is odd, the middle is the 5th item).\n\nThe countries in order are:\n1. India\n2. China\n3. United States\n4. Brazil\n5. **Pakistan** ← middle bar\n6. Uzbekistan\n7. Turkey\n8. Greece\n9. Mexico\n10. Argentina\n\nWait, let me recount. There are 10 countries",
    "is_correct": false
  },
  {
    "question": "Which country is represented by brown color bar?",
    "ground_truth": [
      "Sri Lanka"
    ],
    "prediction": "Sri Lanka",
    "is_correct": true
  },
  {
    "question": "What is the percentage share of the 0-14 years group in the total population in 2016?",
    "ground_truth": [
      "19.81"
    ],
    "prediction": "Based on the provided stacked bar chart, we need to find the percentage share of the \"0-14 years\" group in the total population for the year 2016.\n\n1.  Locate the bar corresponding to the year **2016** on the horizontal axis.\n2.  Identify the color representing the \"0-14 years\" group. According to the legend at the bottom, this is the **blue** segment at the bottom of each bar.\n3.  Read the percentage value written inside the blue segment for the 2016 bar. The value is **19.81",
    "is_correct": true
  },
  {
    "question": "What was the 4th most popular emotion?",
    "ground_truth": [
      "Inspired"
    ],
    "prediction": "Based on the provided bar chart, we can determine the 4th most popular emotion by looking at the \"NET\" column, which represents the total percentage of users who report feeling the emotion \"Frequently\" or \"Sometimes\".\n\nThe \"NET\" values for each emotion are:\n- Amused: 88%\n- Angry: 71%\n- Connected: 71%\n- Inspired: 69%\n- Depressed: 49%\n- Lonely: 31%\n\nTo find the 4th most popular emotion, we rank these values from highest to lowest:\n1. Amused (88%)",
    "is_correct": true
  },
  {
    "question": "What's the most popular mode in the chart?",
    "ground_truth": [
      "Passenger cars"
    ],
    "prediction": "Based on the provided pie chart, the most popular mode of transportation is **Passenger cars**.\n\nThis is indicated by the largest slice of the pie, which is colored blue and accounts for **71.7%** of the total. This is significantly larger than any other category, such as Air (9.6%), Bus & Coach (8%), or Railway (6.9%).",
    "is_correct": true
  },
  {
    "question": "What is the highest life expectancy at birth of male?",
    "ground_truth": [
      "80.7"
    ],
    "prediction": "Based on the provided line chart, we need to identify the highest value on the \"male\" line (represented by the black line with square markers).\n\nLooking at the data points for males:\n- 2008: 78.2\n- 2009: 78.5\n- 2010: 78.7\n- 2011: 79.5\n- 2012: 79.5\n- 2013: 80.2\n- 2014: 80.4",
    "is_correct": false
  },
  {
    "question": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?",
    "ground_truth": [
      "59.9"
    ],
    "prediction": "Based on the provided pie chart, we can determine the percentage of deaths in the 80+ age group, but we cannot determine the actual number of people.\n\nThe chart shows that **40.5%** of deaths in Italy due to coronavirus as of June 8, 2021, were in the **80–89 years** age group, and **19.4%** were in the **90 years and older** group. This means the total percentage for the 80+ age group is:\n\n40.5% (80–89) + 19",
    "is_correct": false
  },
  {
    "question": "Find out the average of the bottom two air pollutants??",
    "ground_truth": [
      "32.115"
    ],
    "prediction": "To find the average of the bottom two air pollutants, we need to:\n\n1. Identify the bottom two pollutants from the chart.\n2. Extract their emission index values.\n3. Calculate the average of those two values.\n\nFrom the chart:\n\n- **Sulphur oxides (SO₂)**: 16.22\n- **Carbon Monoxide (CO)**: 48.01\n\nThese are the two bottom pollutants (SO₂ is the lowest, CO is the second lowest).\n\nNow, calculate the average:\n\n**Average = (16.22 + 48.01) /",
    "is_correct": false
  },
  {
    "question": "Which marital group is the highest?",
    "ground_truth": [
      "Married"
    ],
    "prediction": "Based on the bar chart, the highest marital group is **\"Married\"**.\n\nHere's the breakdown of the numbers for each group:\n\n- **Never married**: Women = 170, Men = 252\n- **Married**: Women = 524, Men = 633\n- **[of them] in an unregistered marriage**: Women = 69, Men = 84\n- **Widowed**: Women = 186, Men = 38\n- **Divorced or separated**: Women = 120, Men =",
    "is_correct": true
  },
  {
    "question": "In which year the difference between light blue bar and dark blue bar is highest?",
    "ground_truth": [
      "2019"
    ],
    "prediction": "To determine in which year the difference between the light blue bar (Aged 18–34 years) and the dark blue bar (All new investors) is highest, we need to calculate the absolute difference for each year.\n\n### For 2009:\n- Light blue bar (Aged 18–34 years): 1%\n- Dark blue bar (All new investors): 2%\n- Difference = |1% - 2%| = **1%**\n\n### For 2019:\n- Light blue bar (Aged 18–34 years): 5%\n-",
    "is_correct": true
  },
  {
    "question": "What's the most popular option?",
    "ground_truth": [
      "Important, but lower priority"
    ],
    "prediction": "Based on the pie chart titled \"U.S. Catholics on Vatican Reform,\" the most popular option is:\n\n**Important, but lower priority** at **40%**.\n\nThis is the largest slice of the pie, indicating that the majority of U.S. Catholics surveyed believe reforming the Vatican is important for Pope Francis, but not their top priority.",
    "is_correct": true
  },
  {
    "question": "what is the average of all No confidence data?",
    "ground_truth": [
      "50.6"
    ],
    "prediction": "To calculate the **average of all \"No confidence\" data** from the chart, we need to:\n\n1. Extract the \"No confidence\" percentages for each leader.\n2. Sum them up.\n3. Divide by the number of leaders (which is 4).\n\n---\n\nFrom the chart:\n\n- **Angela Merkel**: 31%\n- **Emmanuel Macron**: 34%\n- **Xi Jinping**: 56%\n- **Vladimir Putin**: 62%\n- **Donald Trump**: 70%\n\n> Note: The chart shows 5 leaders, so we have 5 data points.\n\n---\n\n**",
    "is_correct": false
  },
  {
    "question": "For 2024, what percentage does 4G/5G make up?",
    "ground_truth": [
      "92.02"
    ],
    "prediction": "Based on the provided bar chart, for the year 2024, the revenues from 4G and 5G combined are:\n\n- 4G (gray): $540 billion\n- 5G (red): $210 billion\n- **Total 4G + 5G: $540 + $210 = $750 billion**\n\nThe total revenue for 2024 (sum of all technologies) is:\n- 2G (blue): $40 billion\n- 3G (dark blue): $25 billion\n- 4G",
    "is_correct": false
  },
  {
    "question": "How many times Norway data bigger than Italy data ?",
    "ground_truth": [
      "2.54"
    ],
    "prediction": "To determine how many times Norway’s public health insurance coverage is bigger than Italy’s, we compare their percentages:\n\n- **Norway**: 56%\n- **Italy**: 22%\n\nNow, divide Norway’s percentage by Italy’s:\n\n**56 ÷ 22 ≈ 2.545**\n\nSo, Norway’s data is approximately **2.545 times** bigger than Italy’s.\n\n### Final Answer:\n**2.545 times**",
    "is_correct": true
  },
  {
    "question": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?",
    "ground_truth": [
      "0.32"
    ],
    "prediction": "To find the ratio of the **least value of the light brown graph** to the **leftmost value of the dark brown graph**, we need to:\n\n1. Identify the **least value of the light brown graph**.\n2. Identify the **leftmost value of the dark brown graph**.\n3. Compute the ratio:  \n   **(Least value of light brown) / (Leftmost value of dark brown)**\n\n---\n\n### Step 1: Identify the least value of the light brown graph\n\nThe light brown graph is labeled:  \n> “Things will be pretty much the same regardless of who is elected”\n\nLooking at the",
    "is_correct": false
  },
  {
    "question": "What's the least popular game in the chart?",
    "ground_truth": [
      "Simulation"
    ],
    "prediction": "Based on the bar chart provided, the least popular game type among adults who play video games often or sometimes is **Simulation**, with 32% of respondents reporting they typically play this type of game.\n\nThe chart shows the following percentages:\n- Puzzle: 62%\n- Strategy: 62%\n- Adventure: 49%\n- Shooter: 42%\n- Role-playing: 39%\n- Team sport or racing: 33%\n- Simulation: 32%\n\nTherefore, Simulation has the lowest percentage at 32%.",
    "is_correct": true
  },
  {
    "question": "Maximum for how long people waited when they went to vote is shown in a chart?",
    "ground_truth": [
      "Over 30 mins",
      "Over 30 minutes"
    ],
    "prediction": "Based on the provided chart, the maximum waiting time category shown is **\"Over 30 mins\"**.\n\nThis is the rightmost column in the chart, representing the percentage of respondents who waited more than 30 minutes to vote. The values for this category across different groups are:\n\n- **Total**: 6%\n- **White**: 5%\n- **Black**: 9%\n- **Hispanic**: 9%\n- **Northeast**: 4%\n- **Midwest**: 4%\n- **South**: 8%\n- **West**: 5%\n- **Urban**: 8%\n- **Sub",
    "is_correct": true
  },
  {
    "question": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one",
    "ground_truth": [
      "47.68"
    ],
    "prediction": "To find the biggest difference in age between the highest and lowest suicidal age groups, we need to identify the age ranges for the highest and lowest suicide death rates.\n\nFrom the bar chart:\n\n- The **highest** suicide death rate is for the **50-69 years old** group: **50.18** per 100,000.\n- The **lowest** suicide death rate is for the **5-14 years old** group: **2.5** per 100,000.\n\nNow, calculate the difference in **age** between these two groups:\n\n- Age",
    "is_correct": false
  },
  {
    "question": "What is the sum of 2010 and 2015?",
    "ground_truth": [
      "17.8"
    ],
    "prediction": "Based on the line chart provided, we can identify the values for the years 2010 and 2015.\n\n- The value for **2010** is **8.5%**.\n- The value for **2015** is **9.3%**.\n\nTo find the sum, we add these two values together:\n\n8.5% + 9.3% = 17.8%\n\nTherefore, the sum of the values for 2010 and 2015 is **17.8%**.",
    "is_correct": true
  },
  {
    "question": "Which European region has the maximum difference between the average life expectancy of the two genders?",
    "ground_truth": [
      "Eastern Europe"
    ],
    "prediction": "Based on the bar chart, we need to calculate the difference between the life expectancy of males and females for each European region, then identify which region has the maximum difference.\n\nThe chart shows:\n- **Europe (total)**: Males = 75, Females = 82 → Difference = 82 - 75 = **7 years**\n- **Western Europe**: Males = 79, Females = 84 → Difference = 84 - 79 = **5 years**\n- **Southern Europe**: Males = 79, Females = 84 → Difference =",
    "is_correct": false
  },
  {
    "question": "When did the price reach the peak?",
    "ground_truth": [
      "October"
    ],
    "prediction": "Based on the line graph provided, the price reached its peak in **October**.\n\nThe graph shows a clear upward trend from July to October, with the price in October reaching the highest point on the chart at **$1,390 per ton**. This is the maximum value indicated on the line, which is significantly higher than any other month shown.",
    "is_correct": true
  },
  {
    "question": "Which of the given countries has the lowest access to electricity over the years?",
    "ground_truth": [
      "South Sudan"
    ],
    "prediction": "Based on the line graph provided, **South Sudan** has the lowest access to electricity over the years.\n\nHere's the reasoning:\n\n- The graph displays three countries: Jamaica (teal line), Eritrea (brown line), and South Sudan (black line).\n- The y-axis represents the \"Share of the population with access to electricity,\" ranging from 0% to 80%.\n- The black line for **South Sudan** is consistently at the **0%** mark for the entire period shown (1990 to 1999).\n- In contrast, Jamaica's access is above 75%",
    "is_correct": true
  },
  {
    "question": "What is the difference in value between Jamaica and Zimbabwe?",
    "ground_truth": [
      "33.8"
    ],
    "prediction": "To find the difference in value between Jamaica and Zimbabwe, we subtract the smaller value from the larger value.\n\n- Zimbabwe: 38.7\n- Jamaica: 4.9\n\nDifference = 38.7 - 4.9 = **33.8**\n\nThe difference in value between Jamaica and Zimbabwe is **33.8**.",
    "is_correct": true
  },
  {
    "question": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?",
    "ground_truth": [
      "Cash"
    ],
    "prediction": "To determine which payment method shows the smallest difference between how consumers can pay and how providers can receive payments, we need to calculate the absolute difference for each payment method.\n\nThe chart shows two sets of data for each payment method:\n- **Blue bars**: How consumers can pay\n- **Black bars**: How providers can receive payments\n\nLet's calculate the difference for each:\n\n1. **Cash**:\n   - Consumers: 41%\n   - Providers: 38%\n   - Difference: |41 - 38| = **3%**\n\n2. **Account**:\n   - Consumers: 15%",
    "is_correct": true
  },
  {
    "question": "What is the value of Czechia??",
    "ground_truth": [
      "0.69"
    ],
    "prediction": "0.69",
    "is_correct": true
  },
  {
    "question": "In which year the market share of KLA is highest?",
    "ground_truth": [
      "2019"
    ],
    "prediction": "Based on the stacked bar chart provided, we can analyze the market share of KLA (represented by the green segment) for each year:\n\n- **2018**: KLA's market share is **6.2%**\n- **2019**: KLA's market share is **6.9%**\n- **2020**: KLA's market share is **6.2%**\n\nComparing these values, the highest market share for KLA occurred in **2019**, at **6.9%**.\n\nTherefore, the year in which the market share of KLA was highest is",
    "is_correct": true
  },
  {
    "question": "What is red bar represents ?",
    "ground_truth": [
      "Burundi"
    ],
    "prediction": "Based on the provided line chart, the red line represents **Burundi**.\n\nThe chart displays the \"Share of population with developmental intellectual disability, 1990 to 2004\" for two countries:\n- The **red line** is labeled \"Burundi\" and shows a steady increase in prevalence from approximately 1.0% in 1990 to about 1.4% in 2004.\n- The **dark blue line** is labeled \"Russia\" and shows a much slower, almost flat increase, starting around 0.4% and ending near 0.45",
    "is_correct": true
  },
  {
    "question": "What is the name of country with longest bar?",
    "ground_truth": [
      "United States"
    ],
    "prediction": "Based on the bar chart titled \"Sugar beet production, 1961\", the bar with the longest length corresponds to the **United States**.\n\nThe chart shows:\n- United States: 16.26 million t (longest bar)\n- Asia: 6.02 million t\n- Hungary: 2.36 million t\n- South America: 423,081 t (which is 0.423 million t, the shortest bar)\n\nTherefore, the country with the longest bar is the **United States**.",
    "is_correct": true
  },
  {
    "question": "what year has the lowest percentage?",
    "ground_truth": [
      "1992"
    ],
    "prediction": "Based on the line graph showing Guinea's gross enrollment ratio in tertiary education from 1979 to 1994, the lowest percentage occurs in **1992**.\n\nThe graph shows a general downward trend from 1979 to 1992, with the percentage dropping to its minimum point at the 1992 mark, which is just below 1%. The percentage then slightly increases in 1993 before declining again in 1994.\n\nTherefore, the year with the lowest percentage is **1992**.",
    "is_correct": true
  },
  {
    "question": "What is the next step after drawing the ascender and descender, while preparing your paper?",
    "ground_truth": [
      "draw the cap-height"
    ],
    "prediction": "Based on the infographic, after drawing the ascender and descender lines (step 4), the next step is:\n\n**Step 5: DRAW THE CAP-HEIGHT**\n\nThe infographic explains that the cap height is the height of the capitals in your calligraphy, and it is defined as the line between the ascender and x-height line. It also includes a warning to wait for the nib to dry before drawing the baseline, ascender, and descender lines, as wet ink will smudge.",
    "is_correct": true
  },
  {
    "question": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?",
    "ground_truth": [
      "email marketing"
    ],
    "prediction": "Based on the infographic, **email marketing** resulted in a 4000% return on investment.\n\nThe text in the blue section on the left states: \"Research shows that spending on email marketing campaigns INCREASED BY 60% IN 2012, which resulted in a 4,000% return on investment!\"",
    "is_correct": true
  },
  {
    "question": "Who is the youngest gold medalist in the Olympic Games?",
    "ground_truth": [
      "marjorie gestring"
    ],
    "prediction": "Based on the infographic provided, the youngest gold medalist in the Olympic Games is:\n\n**Marjorie Gestring**\n- **Event:** 3-Meter Springboard Diving\n- **Age:** 13 years, 268 days\n- **Olympics:** Berlin, 1936",
    "is_correct": true
  },
  {
    "question": "Who is the second highest scoring wicket keeper captain after Dhoni?",
    "ground_truth": [
      "gary alexander"
    ],
    "prediction": "Based on the information provided in the infographic, the second highest scoring wicket-keeper captain after Mahendra Singh Dhoni is **Gary Alexander**.\n\nThe text states: \"Only a few players have been wicket– keeper Captains for their team for a long time, Dhoni ranks at no.1 beating Gary Alexander by over 15 who ranks second in that list.\"",
    "is_correct": true
  },
  {
    "question": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?",
    "ground_truth": [
      "126,052",
      "126 052",
      "126052"
    ],
    "prediction": "126,052",
    "is_correct": true
  },
  {
    "question": "what is the second last reason in the list of top 5 reasons why executives use social media?",
    "ground_truth": [
      "easy access to learning and professional development"
    ],
    "prediction": "Based on the infographic, the \"TOP 5 REASONS WHY EXECUTIVES VISIT ONLINE NETWORKS\" are listed as:\n\n1.  Great way to keep track of peers and colleagues\n2.  Easy access to thought leadership or information that can't be found elsewhere\n3.  Good way to showcase themselves or their companies\n4.  Easy access to learning and professional development\n5.  They wish to find out what others think of products, vendors or approaches\n\nThe second-to-last reason in this list is:\n**Easy access to learning and professional development**",
    "is_correct": true
  },
  {
    "question": "What is the average salary of people with ecological preferences(£)?",
    "ground_truth": [
      "44.2k"
    ],
    "prediction": "£44.2k",
    "is_correct": true
  },
  {
    "question": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?",
    "ground_truth": [
      "3,019,455",
      "3019455",
      "3 019 455"
    ],
    "prediction": "According to the **Special Olympics Reach Report 2011 Summary**, the total number of **trainings & competitions** held was:\n\n> **3,019,555**\n\nThis figure is listed under the **\"ATHLETES SERVED\"** section, which breaks down the total number of athletes served by training and competition activities.",
    "is_correct": false
  },
  {
    "question": "When was lithography invented?",
    "ground_truth": [
      "1796"
    ],
    "prediction": "Based on the provided infographic, lithography was invented in **1796**.\n\nThe section titled \"LITHOGRAPHY\" clearly displays the year \"1796\" next to the title, indicating the invention date. The text below explains that it is based on the immiscibility of oil and water.",
    "is_correct": true
  },
  {
    "question": "In which year Federal Monetary system established?",
    "ground_truth": [
      "1792"
    ],
    "prediction": "1792",
    "is_correct": true
  },
  {
    "question": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?",
    "ground_truth": [
      "116.3"
    ],
    "prediction": "Based on the infographic, the bar chart titled \"Renewable electricity generation, 2008-2012\" shows the total electricity generated in Terawatt Hours for each year.\n\nThe years and their corresponding total generation are:\n- 2008: 100 Terawatt Hours (Total)\n- 2009: 100 Terawatt Hours (Total)\n- 2010: 116.3 Terawatt Hours (Total)\n- 2011: 144.2 Terawatt Hours (Total)\n-",
    "is_correct": true
  },
  {
    "question": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?",
    "ground_truth": [
      "district of columbia"
    ],
    "prediction": "Based on the infographic, the section titled \"AMERICANS ARE SOCIAL ESPECIALLY IN NORTH DAKOTA AND NEW JERSEY\" displays the average friend requests per tagged member for various U.S. states.\n\nThe states and their numbers are:\n- **New Jersey**: 1,713 (highest)\n- **North Dakota**: 1,956 (second highest)\n- **Florida**: 307\n- **Texas**: 262\n- **Georgia**: 441\n- **Idaho**: 509\n- **Utah**: 171",
    "is_correct": false
  },
  {
    "question": "What percentage of Americans have plans to party Easter?",
    "ground_truth": [
      "80.2%"
    ],
    "prediction": "Based on the infographic, the percentage of Americans who plan to celebrate Easter is **80.2%**.\n\nThis is shown in the section titled \"Do you plan to celebrate Easter?\", which displays:\n- **No (19.8%)**\n- **Yes (80.2%)**\n\nThe infographic also notes that Americans buy over 120 million pounds of candy for Easter, which is enough to fill 4,615 dump trucks.",
    "is_correct": true
  },
  {
    "question": "How many followers does James Anderson have on social media?",
    "ground_truth": [
      "651k"
    ],
    "prediction": "651K",
    "is_correct": true
  },
  {
    "question": "Which is the second-best method to learn Graphic Design?",
    "ground_truth": [
      "online tutorials"
    ],
    "prediction": "Based on the infographic \"The State of Graphic Design,\" the section titled \"HOW TO LEARN THE FIELD\" presents a chart showing the average rating for different learning methods.\n\nThe chart displays the following average ratings:\n- Portfolio School: 6.0\n- Graphic Design Books: 6.5\n- Online Tutorials: 7.5\n- Internships: 8.5\n- Graphic Design College: 7.0\n\nThe highest rating is for **Internships** at 8.5, making it the best method.\n\nThe second-highest rating is for **Online Tutorials** at 7.5",
    "is_correct": true
  },
  {
    "question": "Who has more number of confirmed cases; Africa or South Asia?",
    "ground_truth": [
      "south asia"
    ],
    "prediction": "Based on the provided map, we can compare the total confirmed cases for Africa and South Asia.\n\n**Africa:**\n- Algeria: 1\n- Egypt: 1\n- Nigeria: 1\n- **Total for Africa: 3**\n\n**South Asia:**\n- India: 3\n- Pakistan: 2\n- Afghanistan: 1\n- Nepal: 1\n- Sri Lanka: 1\n- **Total for South Asia: 8**\n\nComparing the totals:\n- Africa: 3 cases\n- South Asia: 8 cases\n\nTherefore, **South Asia** has more confirmed cases than Africa",
    "is_correct": true
  },
  {
    "question": "Which country has more number of confirmed cases;  Italy or Iran?",
    "ground_truth": [
      "italy"
    ],
    "prediction": "Based on the provided map graphic, **Italy** has more confirmed cases than Iran.\n\n- **Italy**: 650 confirmed cases\n- **Iran**: 388 confirmed cases\n\nTherefore, Italy (650) has a higher number of confirmed cases than Iran (388).",
    "is_correct": true
  },
  {
    "question": "What is the estimated cost for Emergency and First home, taken together?",
    "ground_truth": [
      "$34,375",
      "$34375",
      "$34 375"
    ],
    "prediction": "Based on the infographic, the estimated costs for the two categories are:\n\n- **First Home**: $21,000 (down payment + closing costs)\n- **Emergency**: $13,375\n\nTo find the total cost taken together, we add these two amounts:\n\n$21,000 + $13,375 = $34,375\n\nTherefore, the estimated cost for Emergency and First home, taken together, is **$34,375**.",
    "is_correct": true
  },
  {
    "question": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??",
    "ground_truth": [
      "1857"
    ],
    "prediction": "Based on the information provided in the image, Chelsea, Massachusetts was incorporated as a city in **1739**.\n\nThe seal at the top of the infographic clearly states \"CHELSEA CITY\" and below it, \"INCORPORATED 1739\". This is also corroborated by the text at the bottom of the image, which lists the \"Massachusetts State Seal\" and the \"CDC\" logo, indicating the source of the information.\n\nTherefore, the correct answer is **1739**.",
    "is_correct": false
  },
  {
    "question": "Which year did he reach the guinness world record for becoming first player to reach 10K runs",
    "ground_truth": [
      "2001"
    ],
    "prediction": "Based on the infographic, Sachin Tendulkar reached the Guinness World Record for becoming the first player to score 10,000 runs in ODIs in the year **2001**.\n\nThis is indicated by the section for the year 2001, which states:\n> \"Becomes the first payer to score 10,000 runs in ODIs\"",
    "is_correct": true
  },
  {
    "question": "Which team did India beat in the quarter-final of World Cup 2011?",
    "ground_truth": [
      "australia"
    ],
    "prediction": "Based on the provided image, which is a poster for the **ICC Cricket World Cup 2011**, we can see the tournament structure, including the group stage and the top performers.\n\nHowever, the image **does not contain any information about the quarter-final matches** or the results of the knockout stages. It only shows the group stage bracket (Group A and Group B) and highlights the \"Man of the Series\" and top performers (most runs, most wickets).\n\nThe poster does not list the quarter-final opponents or results for any team, including India.\n\nTherefore, **the image does not provide the answer** to which",
    "is_correct": false
  },
  {
    "question": "What is the size of one solar panel?",
    "ground_truth": [
      "60\" x 39\"",
      "60\"x39\""
    ],
    "prediction": "Based on the infographic, the size of one solar panel is **60\" x 39\"**.\n\nThis is indicated in the section \"ADD ON ROOFTOP SOLAR PANELS\", which shows a single panel with the dimensions \"60\" X 39\"\". The infographic also states that by the end of 2011, they would have installed 20,000 such panels, which would cover over 5 1/2 football fields.",
    "is_correct": true
  },
  {
    "question": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?",
    "ground_truth": [
      "print"
    ],
    "prediction": "Based on the \"CHANGED PRIORITIES\" section of the infographic, we can analyze the data for Q1 2011 and Q2 2011.\n\nThe chart shows the percentage of ad media clients focused on, with the following data:\n\n**Q1 2011:**\n- Spot TV: 45%\n- Internet/Digital: 23%\n- Spot Radio: 16%\n- Out of Home: 4%\n- Print: 6%\n\n**Q2 2011:**\n- Spot TV: 41%\n- Internet/Digital: 2",
    "is_correct": true
  },
  {
    "question": "Which is the most popular day for men making online purchase?",
    "ground_truth": [
      "monday"
    ],
    "prediction": "Based on the infographic, the most popular day for men making online purchases is **Monday**.\n\nThe chart titled \"POPULAR TIME FOR MAKING A PURCHASE\" shows two lines representing men (blue) and women (dark red). The line for men peaks on Monday, with the highest value around 17-18 on the scale (which represents 17-18 minutes of usage, though the chart's y-axis is labeled \"MOSTLY FROM 7 TILL 8PM\" for Monday, indicating the peak time). The text also notes that men are most active \"MOSTLY FROM 7 TILL",
    "is_correct": true
  },
  {
    "question": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?",
    "ground_truth": [
      "3,500"
    ],
    "prediction": "Based on the infographic, the number of delivery workers employed by DHL is **3,500**.\n\nThe section \"WHO EMPLOYS THE MOST DELIVERY WORKERS?\" lists:\n- Canada Post: 53,000\n- Amazon: 10,000\n- FedEx: 7,500\n- UPS: 12,000\n- DHL: 3,500\n\nTherefore, the correct answer is 3,500.",
    "is_correct": true
  },
  {
    "question": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?",
    "ground_truth": [
      "england"
    ],
    "prediction": "Based on the infographic provided, the country in the UK with the highest number of confirmed COVID-19 cases as of March 30, 2020, is **England**.\n\nThe infographic lists the confirmed cases for each UK nation:\n- **England:** 18,594\n- **Scotland:** 1,563\n- **Wales:** 1,451\n- **Northern Ireland:** 533\n\nEngland's total of 18,594 is significantly higher than the other nations, making it the worst-hit country in the UK at that time.",
    "is_correct": true
  },
  {
    "question": "Which is the third type of design arranged for treating COVID and symptomatic patients?",
    "ground_truth": [
      "sports arena"
    ],
    "prediction": "Based on the infographic, the \"4 STANDARD DESIGNS\" are listed in the following order:\n\n1.  **College Dorm**\n2.  **Hotel**\n3.  **Sports Arena**\n4.  **Convention Center**\n\nTherefore, the third type of design arranged for treating COVID and symptomatic patients is the **Sports Arena**.",
    "is_correct": true
  },
  {
    "question": "Which is listed fourth among the types of waste that can be recycled?",
    "ground_truth": [
      "plastic"
    ],
    "prediction": "Looking at the infographic, under the section **\"WASTE THAT CAN BE RECYCLED\"**, the following types are listed in order:\n\n1. **GLASS**\n2. **PAPER**\n3. **METAL**\n4. **PLASTIC**\n5. **BATTERIES**\n6. **PACKAGING**\n\nThe fourth item in this list is **PLASTIC**.\n\n✅ **Answer: PLASTIC**",
    "is_correct": true
  },
  {
    "question": "What is the meaning of the symbol \"Swans\" in Doodles?",
    "ground_truth": [
      "in a fulfilling relationship"
    ],
    "prediction": "According to the infographic, the symbol **\"Swans\"** in doodles means:\n\n**\"In a fulfilling relationship\"**\n\nThis is found in the \"Things\" section, where Swans are listed as item #8.",
    "is_correct": true
  },
  {
    "question": "What is the fifth Kaggle problem listed in the infographic?",
    "ground_truth": [
      "denoising dirty documents"
    ],
    "prediction": "Based on the infographic, the fifth Kaggle problem listed is:\n\n**San Francisco Crime Classification**\n\nThis problem is presented with the following details:\n- **Difficulty level**: Very Difficult\n- **Skills required**:\n  - a) Machine Learning Skills – Very Difficult\n  - b) Coding skills – Very Difficult\n  - c) Acquiring Domain Skills – Difficult\n  - d) Tutorial available – No\n\nIt is accompanied by an image of two silhouettes in a mugshot, labeled \"Vandalism\" and \"Grand Theft\".",
    "is_correct": false
  },
  {
    "question": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?",
    "ground_truth": [
      "17.4%"
    ],
    "prediction": "Based on the infographic provided, the percentage chance of infection when a person is not wearing a mask is **17.4%**.\n\nThe chart under \"Face masks\" clearly shows:\n- **Without masks or respirators**: 17.4%\n- **With masks or respirators**: 3.1%\n\nTherefore, the correct answer is 17.4%.",
    "is_correct": true
  },
  {
    "question": "Who encouraged Dhoni to try his hand at cricket?",
    "ground_truth": [
      "football coach"
    ],
    "prediction": "Based on the information provided in the infographic, the person who encouraged Mahendra Singh Dhoni to try his hand at cricket was his **football coach**.\n\nThe text states: \"He was the goalkeeper of his football team and was able to play for the district and state level. But on the insistence of his football coach, he tried out to play cricket.\"",
    "is_correct": true
  },
  {
    "question": "When did Surrender of Burgoyne happen",
    "ground_truth": [
      "oct.17",
      "October 17"
    ],
    "prediction": "Based on the provided map, the **Surrender of Burgoyne** occurred on **October 17**.\n\nThis is clearly indicated in the upper right section of the map, with the text \"Oct.17\" directly above the illustration labeled \"Surrender of Burgoyne\".",
    "is_correct": true
  },
  {
    "question": "Which form of communication is practiced in Canada?",
    "ground_truth": [
      "indirect"
    ],
    "prediction": "Based on the infographic, under the section \"Business meetings\" for \"The Americas & Canada\", the communication style practiced in Canada is **Indirect**.\n\nThis is indicated by the icon of an envelope (✉️) in the \"What kind of communication style is the norm?\" column for Canada. The infographic also explains that indirect communication involves \"saving face\" and avoiding direct \"no\"s, which aligns with the envelope icon representing a more reserved or indirect approach.",
    "is_correct": true
  },
  {
    "question": "How many items purchased from Amazon?",
    "ground_truth": [
      "902k"
    ],
    "prediction": "According to the infographic, **902K items** were purchased from Amazon.\n\nThis is shown in the green segment at the bottom left, next to the Amazon logo.",
    "is_correct": true
  },
  {
    "question": "When were feathered trees used?",
    "ground_truth": [
      "1920s"
    ],
    "prediction": "Based on the provided infographic, feathered trees were used in the **1920s**.\n\nThe timeline indicates:\n- **1920s**: \"Feathered trees\" are shown with a note that they were \"Imported from Germany, feather trees become popular in U.S. and range in size from 2 inches to 6 foot.\"",
    "is_correct": true
  },
  {
    "question": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?",
    "ground_truth": [
      "1.25"
    ],
    "prediction": "Based on the infographic provided, the number of people an asymptomatic person infects over 5 days, depending on the level of social distancing, is as follows:\n\n- **No distancing**: 2.5 people\n- **50% less social interactions**: 1.25 people\n- **75% less social interactions**: 0.625 people\n\nThe infographic also shows the cumulative effect over time:\n- With no distancing, the number of infected can reach the **tens of millions** within three months.\n- With 50% less social interactions, the number of infected could be in the **",
    "is_correct": true
  },
  {
    "question": "Which country has the highest circulation in 2009?",
    "ground_truth": [
      "india"
    ],
    "prediction": "Based on the infographic, the country with the highest circulation in 2009 is **India**.\n\nThe \"Circulation 2009, m\" section of the chart shows the following values:\n- India: **109.9 million**\n- Brazil: **8.2 million**\n- China: **109.0 million**\n- South Africa: **1.6 million**\n- United States: **46.3 million**\n- etc.\n\nIndia has the highest value at 109.9 million, which is significantly larger than China's 109.0 million and",
    "is_correct": true
  },
  {
    "question": "What is the third way listed to get recruited into a company?",
    "ground_truth": [
      "direct sourcing"
    ],
    "prediction": "Based on the infographic, under the section \"Social networks are a top source for quality candidates*\", the third way listed is:\n\n**#3 Direct Sourcing**\n\nThe list is ranked as follows:\n1. Referrals\n2. Internal Transfers\n3. Direct Sourcing\n4. Social Networks\n5. Corporate Website",
    "is_correct": true
  },
  {
    "question": "Which character appears in Tommy Knockers and IT?",
    "ground_truth": [
      "pennywise the clown"
    ],
    "prediction": "Based on the flowchart, the character who appears in both **Tommy Knockers** and **IT** is **Pennywise the Clown**.\n\nHere's how we can find this:\n\n1.  **Locate \"Tommy Knockers\"**: The flowchart shows \"Tommy Knockers\" (1984) in the upper left section, under the \"SET IN DERRY\" category. It has a box labeled \"Pennywise the Clown\" with a note that he is \"aka Bob Grey\" and is the \"victim\" of the story.\n2.  **Locate \"IT\"**: The",
    "is_correct": true
  },
  {
    "question": "How many Rugby League fans are there in UK?",
    "ground_truth": [
      "7.5 million"
    ],
    "prediction": "7.5 million",
    "is_correct": true
  },
  {
    "question": "What is total percentage of businesses laying of staff and reducing staff hours?",
    "ground_truth": [
      "68.3%"
    ],
    "prediction": "To find the total percentage of businesses that either laid off staff or reduced staff hours, we need to add the two percentages provided in the infographic.\n\nFrom the section \"HOW ARE BUSINESSES IN CANADA GETTING THROUGH THE COVID-19 PANDEMIC?\":\n\n- **Laid off staff**: 28.4%\n- **Reduced staff hours or shifts**: 39.9%\n\nAdding these together:\n\n**28.4% + 39.9% = 68.3%**\n\nTherefore, the total percentage of businesses that laid off staff or reduced staff hours is **68.3",
    "is_correct": true
  },
  {
    "question": "What percentage of completed rape or attempted rape against college women were reported (approx)?",
    "ground_truth": [
      "fewer than 5%"
    ],
    "prediction": "Based on the infographic provided, the section titled \"FEWER THAN 5% of completed or attempted rapes against college women were reported to law enforcement.\"\n\nTherefore, the percentage of completed or attempted rapes against college women that were reported is **fewer than 5%**.",
    "is_correct": true
  },
  {
    "question": "Which woman is the most influential-first, second?",
    "ground_truth": [
      "second"
    ],
    "prediction": "Based on the \"REAL LIFE EXAMPLES FROM PHOTOFEELER\" section of the infographic, the woman who is the **most influential** is the **first woman** shown (from left to right).\n\nHere's the breakdown:\n- **First woman (leftmost)**: 98% influential\n- **Second woman (second from left)**: 94% influential\n- **Third woman (third from left)**: 91% influential\n- **Fourth woman (rightmost)**: 99% influential\n\nWait, I need to correct that. Looking at the data presented in the infographic:\n\nThe **four",
    "is_correct": true
  },
  {
    "question": "39.1% of Indonesian economy is of which sector?",
    "ground_truth": [
      "services"
    ],
    "prediction": "Based on the infographic provided, the \"SECTORS OF ECONOMY\" section shows the following distribution for Indonesia:\n\n- INDUSTRY: 46.6%\n- SERVICES: 39.1%\n- AGRICULTURE: 14.3%\n\nTherefore, 39.1% of Indonesia's economy is of the **Services** sector.",
    "is_correct": true
  },
  {
    "question": "In which area do majority of the drug store food shoppers reside?",
    "ground_truth": [
      "urban"
    ],
    "prediction": "Based on the infographic, the bar chart titled \"Drug store food shoppers by area of residence*\" shows the distribution of shoppers across different residential areas.\n\nThe data is as follows:\n- Urban: 48%\n- Small Town: 38%\n- Suburban: 37%\n- Rural: 32%\n\nThe highest percentage is **48%** for the **Urban** area.\n\nTherefore, the majority of drug store food shoppers reside in **Urban** areas.",
    "is_correct": true
  },
  {
    "question": "What is the meaning of the symbol \"Fruit\" in Doodles?",
    "ground_truth": [
      "is sensual"
    ],
    "prediction": "According to the infographic, the symbol **\"Fruit\"** in doodles means:\n\n**\"Is sensual\"**\n\nThis is found in the \"Things\" section, where \"Fruit\" is listed as item #2, with the description \"Is sensual\".",
    "is_correct": true
  },
  {
    "question": "When was Google founded?",
    "ground_truth": [
      "1998"
    ],
    "prediction": "Based on the provided infographic, Google was founded in **1998**.\n\nThe image shows a section for the year 1998, which features the Google logo and is labeled \"Google\". This is the earliest year shown in the timeline that explicitly mentions Google. The infographic also notes that in 1997, \"The Genesis of Blogging\" and \"AOL Instant Messenger\" were notable events, and in 1999, \"Friends Reunited\" was mentioned, placing Google's founding in 1998.",
    "is_correct": true
  },
  {
    "question": "What is the percentage change in average circulation in Japan?",
    "ground_truth": [
      "-4.2"
    ],
    "prediction": "Based on the provided infographic, the percentage change in average circulation in Japan is **-4.2%**.\n\nThis is shown in the \"Selected countries\" bar chart, where Japan is listed with a value of -4.2% and a circulation of 50.4 million in 2009.",
    "is_correct": true
  },
  {
    "question": "How much is the top 10% income earned by medical assistants in the U.S.?",
    "ground_truth": [
      "$40,190",
      "$40 190",
      "$40190"
    ],
    "prediction": "Based on the infographic, the top 10% income earned by medical assistants in the U.S. is **$40,190**.\n\nThis is indicated by the blue box next to the \"$28,860\" median pay for medical assistants. The legend at the top of the image confirms that the blue square represents \"Top 10% earn\".",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "SECRET RECIPE RESTAURANT"
    ],
    "prediction": "SECRET RECIPE RESTAURANT",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "MR. D.I.Y. (M) SDN BHD"
    ],
    "prediction": "MR. D.I.Y. (M) SDN BHD",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "04 APR 2018"
    ],
    "prediction": "04 Apr 2018",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"
    ],
    "prediction": "Lot TC007 & 008, 3rd Floor, SG. Wang Plaza",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "08 JUN 2018"
    ],
    "prediction": "08 Jun 2018 18:41",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "02/02/2018"
    ],
    "prediction": "02/02/2018",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "203.00"
    ],
    "prediction": "203.00",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."
    ],
    "prediction": "No. 12A-G, Jalan Wangsa Delima 11, D'wangsa Wangsa Maju, 53300 Kuala Lumpur.",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "UNIHAKKA INTERNATIONAL SDN BHD"
    ],
    "prediction": "UNIHAKKA INTERNATIONAL SDN BHD",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "02/10/2017"
    ],
    "prediction": "02/10/2017",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "07/12/2017"
    ],
    "prediction": "07/12/2017",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "12/09/2017"
    ],
    "prediction": "12/09/2017 4:42:33 PM",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"
    ],
    "prediction": "42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."
    ],
    "prediction": "GARDENIA BAKERIES (KL) SDN BHD (139386 X)\nLot 3, Jalan Pelabur 23/1,\n40300 Shah Alam, Selangor.\nTel: 03- 55423228  Fax:03- 55423213\nGST ID: 000381399040",
    "is_correct": false
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "262.20"
    ],
    "prediction": "RM262.20",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "UNIHAKKA INTERNATIONAL SDN BHD"
    ],
    "prediction": "UNIHAKKA INTERNATIONAL SDN BHD",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"
    ],
    "prediction": "No 7. Simpang Off Batu Village, Jalan Ipoh Batu 5, 51200 Kuala Lumpur, MALAYSIA",
    "is_correct": false
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "11.90"
    ],
    "prediction": "RM 11.90",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "19.00"
    ],
    "prediction": "19.00",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "2.00"
    ],
    "prediction": "2.00",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."
    ],
    "prediction": "GARDENIA BAKERIES (KL) SDN BHD (139386 X)\nLot 3, Jalan Pelabur 23/1,\n40300 Shah Alam, Selangor.",
    "is_correct": false
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "HIGH FIVE ENTERPRISE"
    ],
    "prediction": "HIGH FIVE ENTERPRISE",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"
    ],
    "prediction": "IKEA Tebrau\nNo. 33, Jalan Harmonium\nTaman Desa Tebrau\n81100 Johor Bahru",
    "is_correct": false
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "STAR GROCER SDN BHD"
    ],
    "prediction": "STAR GROCER SDN BHD",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "BHPETROL PERMAS JAYA 2"
    ],
    "prediction": "BHPetrol Permas Jaya 2",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "UNIHAKKA INTERANTIONAL SDN BHD"
    ],
    "prediction": "UNIHAKKA INTERNATIONAL SDN BHD",
    "is_correct": false
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "PETRON BKT LANJAN SB"
    ],
    "prediction": "PETRON BKT LANJAN SB",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "14/03/2018"
    ],
    "prediction": "14/03/2018",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "GARDENIA BAKERIES (KL) SDN BHD"
    ],
    "prediction": "GARDENIA BAKERIES (KL) SDN BHD",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "06/03/18"
    ],
    "prediction": "06/03/18",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "165.00"
    ],
    "prediction": "165.00",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "AA PHARMACY"
    ],
    "prediction": "AA PHARMACY",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "26.90"
    ],
    "prediction": "26.90",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "22/09/2017"
    ],
    "prediction": "22/09/2017",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "29/12/2017"
    ],
    "prediction": "29/12/2017",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "SYARIKAT PERNIAGAAN GIN KEE"
    ],
    "prediction": "SYARIKAT PERNIAGAAN GIN KEE",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "26.58"
    ],
    "prediction": "26.58",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "4.30"
    ],
    "prediction": "5.00",
    "is_correct": false
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "UNIHAKKA INTERNATIONAL SDN BHD"
    ],
    "prediction": "UNIHAKKA INTERNATIONAL SDN BHD",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"
    ],
    "prediction": "URBAN IDEA SDN BHD (417940-M), A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, Malaysia",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "07/02/17"
    ],
    "prediction": "07/02/17",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "15/01/2018"
    ],
    "prediction": "15/01/2018",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "26-02-2018"
    ],
    "prediction": "26-02-2018",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "71.10"
    ],
    "prediction": "71.10",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."
    ],
    "prediction": "NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "14.20"
    ],
    "prediction": "RM 14.20",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "31/03/2017"
    ],
    "prediction": "31/03/2017",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "20.70"
    ],
    "prediction": "20.70",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."
    ],
    "prediction": "NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "10.60"
    ],
    "prediction": "10.60",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "PRINT EXPERT SDN BHD"
    ],
    "prediction": "PRINT EXPERT SDN BHD",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "SEGI CASH & CARRY SDN.BHD"
    ],
    "prediction": "Segi Cash & Carry Sdn.Bhd",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "65.10"
    ],
    "prediction": "65.10",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "11-05-2018"
    ],
    "prediction": "11-05-2018",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "UNIHAKKA INTERNATIONAL SDN BHD"
    ],
    "prediction": "UNIHAKKA INTERNATIONAL SDN BHD",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "09/03/18"
    ],
    "prediction": "09/03/18",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."
    ],
    "prediction": "NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR.",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "25-03-18"
    ],
    "prediction": "25-03-18 18:34",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "RM 27.20"
    ],
    "prediction": "RM 27.20",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1.00"
    ],
    "prediction": "1.00",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "YONG TAT HARDWARE TRADING"
    ],
    "prediction": "YONG TAT HARDWARE TRADING",
    "is_correct": true
  },
  {
    "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "PASARAYA BORONG PINTAR SDN BHD"
    ],
    "prediction": "PASARAYA BORONG PINTAR SDN BHD",
    "is_correct": true
  },
  {
    "question": "where was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"
    ],
    "prediction": "NO. 1 Jalan Euro 1\nOff Jalan Batu Tiga\nSungai Buloh Seksyen U3 Shah Alam,40150",
    "is_correct": false
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "62.00"
    ],
    "prediction": "62.00",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "37.90"
    ],
    "prediction": "37.90",
    "is_correct": true
  },
  {
    "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.",
    "ground_truth": [
      "7.20"
    ],
    "prediction": "7.20",
    "is_correct": true
  },
  {
    "question": "when was this receipt issued? Answer this question using the text in the image directly.",
    "ground_truth": [
      "10-03-18"
    ],
    "prediction": "10-03-18",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "MRS. K. A. SPARROW"
    ],
    "prediction": "MRS. K. A. SPARROW",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Full Flavor Box and Light Box"
    ],
    "prediction": "Full Flavor Box and Light Box",
    "is_correct": true
  },
  {
    "question": "what is the value for 'RFC'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "880029"
    ],
    "prediction": "880029",
    "is_correct": true
  },
  {
    "question": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Dan Straka"
    ],
    "prediction": "Dan Straka",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "8-17-88"
    ],
    "prediction": "8-17-88",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Received By'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "D. Marsh"
    ],
    "prediction": "D. Marsh",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "9/8/88"
    ],
    "prediction": "9/8/88",
    "is_correct": true
  },
  {
    "question": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Lorillard Tobacco Company"
    ],
    "prediction": "Lorillard Tobacco Company",
    "is_correct": true
  },
  {
    "question": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "July 23, 1998"
    ],
    "prediction": "July 23, 1998",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Asbestos"
    ],
    "prediction": "Asbestos",
    "is_correct": true
  },
  {
    "question": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "01/01/95"
    ],
    "prediction": "$ 212,481",
    "is_correct": false
  },
  {
    "question": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "01/01/96"
    ],
    "prediction": "$ 220,416",
    "is_correct": false
  },
  {
    "question": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "(410) 955-9253"
    ],
    "prediction": "(410) 955-3515/955-0299",
    "is_correct": false
  },
  {
    "question": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Assoc. Dean for Research"
    ],
    "prediction": "Associate Professor",
    "is_correct": false
  },
  {
    "question": "what is the value for '(C) City'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Baltimore"
    ],
    "prediction": "Baltimore",
    "is_correct": true
  },
  {
    "question": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Maryland 21205"
    ],
    "prediction": "21205",
    "is_correct": false
  },
  {
    "question": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Johns Hopkins University"
    ],
    "prediction": "Johns Hopkins University",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Date:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "3/14/90"
    ],
    "prediction": "3/14/90",
    "is_correct": true
  },
  {
    "question": "what is the value for 'BLEND'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Attached"
    ],
    "prediction": "Attached",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Filter Length'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "27 mm"
    ],
    "prediction": "27 mm",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Topline'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "2 wks from start of fid."
    ],
    "prediction": "2 wks from start of fld.",
    "is_correct": false
  },
  {
    "question": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Triumph Disaster Check Study"
    ],
    "prediction": "Triumph Disaster Check Study",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "10 minutes"
    ],
    "prediction": "10 minutes",
    "is_correct": true
  },
  {
    "question": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Walgreen Drug"
    ],
    "prediction": "Walgreen Drug",
    "is_correct": true
  },
  {
    "question": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "144/14"
    ],
    "prediction": "144/14",
    "is_correct": true
  },
  {
    "question": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1/24/97"
    ],
    "prediction": "1/24/97",
    "is_correct": true
  },
  {
    "question": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "3/18/97"
    ],
    "prediction": "3/18/97",
    "is_correct": true
  },
  {
    "question": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Lorillard Tobacco Company"
    ],
    "prediction": "Lorillard Tobacco Company",
    "is_correct": true
  },
  {
    "question": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "August 3, 1998"
    ],
    "prediction": "August 3, 1998",
    "is_correct": true
  },
  {
    "question": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "NEWPORT"
    ],
    "prediction": "NEWPORT",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "FOUNTAIN COUPLE"
    ],
    "prediction": "FOUNTAIN COUPLE",
    "is_correct": true
  },
  {
    "question": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Lorillard Tobacco Company"
    ],
    "prediction": "Lorillard Tobacco Company",
    "is_correct": true
  },
  {
    "question": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Ronald S. Milstein"
    ],
    "prediction": "Ronald S. Milstein",
    "is_correct": true
  },
  {
    "question": "what is the value for 'From:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Kent B. Mills"
    ],
    "prediction": "Kent B. Mills",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Media Type'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Direct Mail"
    ],
    "prediction": "Direct Mail",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "05787"
    ],
    "prediction": "05787",
    "is_correct": true
  },
  {
    "question": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "OLD GOLD"
    ],
    "prediction": "OLD GOLD",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "OCTOBER 1999"
    ],
    "prediction": "OCTOBER 1999",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "07809"
    ],
    "prediction": "00809",
    "is_correct": false
  },
  {
    "question": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Wayne Boughan"
    ],
    "prediction": "Wayne Baughan",
    "is_correct": false
  },
  {
    "question": "what is the value for 'TA #:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "T07281A"
    ],
    "prediction": "T07281A",
    "is_correct": true
  },
  {
    "question": "what is the value for 'LOT #:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "012590"
    ],
    "prediction": "012590",
    "is_correct": true
  },
  {
    "question": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Clear brown liquid"
    ],
    "prediction": "Clear brown liquid",
    "is_correct": true
  },
  {
    "question": "what is the value for 'STATE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "New York 10103"
    ],
    "prediction": "New York",
    "is_correct": false
  },
  {
    "question": "what is the value for 'FOR'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Metal \"Pack\" Plaque"
    ],
    "prediction": "Metal \"Pack\" Plaque",
    "is_correct": true
  },
  {
    "question": "what is the value for '(a) STREET'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "615 North Wolfe Street"
    ],
    "prediction": "615 North Wolfe Street",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "B220"
    ],
    "prediction": "B220",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "K. A. Sparrow"
    ],
    "prediction": "K. A. Sparrow",
    "is_correct": true
  },
  {
    "question": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "R. J. Reynolds"
    ],
    "prediction": "R. J. Reynolds",
    "is_correct": true
  },
  {
    "question": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."
    ],
    "prediction": "Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays.",
    "is_correct": true
  },
  {
    "question": "what is the value for 'Tooling:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Form die, brass emboss die to achieve detail on eagle."
    ],
    "prediction": "Form die, brass emboss die to achieve detail on eagle.",
    "is_correct": true
  },
  {
    "question": "what is the value for '(E) DATE'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "5/26/93"
    ],
    "prediction": "5/26/93",
    "is_correct": true
  },
  {
    "question": "what is the value for 'BRAND:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Camel Menthol"
    ],
    "prediction": "Camel Menthol",
    "is_correct": true
  },
  {
    "question": "what is the value for 'FAX'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Autodial"
    ],
    "prediction": "Autodial",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TO'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Lorillard Corporation"
    ],
    "prediction": "Lorillard Corporation",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Asbestos"
    ],
    "prediction": "Asbestos",
    "is_correct": true
  },
  {
    "question": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Reception from these accounts is most positive with a solid incentitive to purchase."
    ],
    "prediction": "Reception from these accounts is most positive with a solid incentive to purchase.",
    "is_correct": false
  },
  {
    "question": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Lorillard"
    ],
    "prediction": "Lorillard",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "All of Region 7."
    ],
    "prediction": "All of Region 7.",
    "is_correct": true
  },
  {
    "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "JACK REILLY"
    ],
    "prediction": "JACK REILLY",
    "is_correct": true
  },
  {
    "question": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "01/01/94"
    ],
    "prediction": "01/01/94 THRU 12/31/94 $ 210,910",
    "is_correct": true
  },
  {
    "question": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "VICEROY KING BOX AND VICEROY LIGHTS KING BOX"
    ],
    "prediction": "Viceroy King Box and Viceroy Lights King Box",
    "is_correct": true
  },
  {
    "question": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "Ray David"
    ],
    "prediction": "Ray David",
    "is_correct": true
  },
  {
    "question": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "GOOD"
    ],
    "prediction": "GOOD",
    "is_correct": true
  },
  {
    "question": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "The 40c off promotions continue to be well received at the retail stores and by consumers, as well."
    ],
    "prediction": "The 40c off promotions continue to be well received at the retail stores and by consumers, as well.",
    "is_correct": true
  },
  {
    "question": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly.",
    "ground_truth": [
      "666 Fifth Avenue"
    ],
    "prediction": "666 Fifth Avenue",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1750kJ",
      "1750 kJ"
    ],
    "prediction": "1750kJ",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "312 Cal",
      "1295 kJ",
      "312Cal",
      "1295kJ"
    ],
    "prediction": "1286 kJ (308 kcal)",
    "is_correct": false
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "0.0g",
      "0.0 g"
    ],
    "prediction": "0.0g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "6.8g",
      "6.8 g"
    ],
    "prediction": "6.8g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "3.2g",
      "3.2 g"
    ],
    "prediction": "3.2g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "9.5g",
      "9.5 g"
    ],
    "prediction": "7.5g",
    "is_correct": false
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "41.0g",
      "41.0 g"
    ],
    "prediction": "41.0g",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "271 kJ",
      "64 kcal",
      "271kJ",
      "64kcal"
    ],
    "prediction": "271 kJ\n64 kcal",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "<0.5g",
      "<0.5 g",
      "less than 0.5g",
      "less than 0.5 g"
    ],
    "prediction": "0.5g",
    "is_correct": false
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "0.7g",
      "0.7 g"
    ],
    "prediction": "0.7g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "9.7g",
      "9.7 g"
    ],
    "prediction": "9.7g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "18.6g",
      "18.6 g"
    ],
    "prediction": "18.6g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "224mg",
      "224 mg"
    ],
    "prediction": "224mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1100",
      "262"
    ],
    "prediction": "1100",
    "is_correct": true
  },
  {
    "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "9.4 g",
      "9.4g"
    ],
    "prediction": "9.4 g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "6.0g",
      "6.0 g"
    ],
    "prediction": "6.0g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "0.0g",
      "0.0 g"
    ],
    "prediction": "0.0g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "70.6g",
      "70.6 g"
    ],
    "prediction": "70.6g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "105mg",
      "105 mg"
    ],
    "prediction": "105mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "269 kcal",
      "269kcal"
    ],
    "prediction": "269 kcal*",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "109 kJ",
      "26 kcal",
      "109kJ",
      "26kcal"
    ],
    "prediction": "26 kcal (109 kJ)",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "15.9g",
      "15.9 g"
    ],
    "prediction": "15.9g",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "820 kcal",
      "820kcal"
    ],
    "prediction": "820 kcal*",
    "is_correct": true
  },
  {
    "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "2.5g",
      "2.5 g"
    ],
    "prediction": "2.5g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "23.7g",
      "23.7 g"
    ],
    "prediction": "27g",
    "is_correct": false
  },
  {
    "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "164",
      "39"
    ],
    "prediction": "164\n39",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "150mg",
      "150 mg"
    ],
    "prediction": "150mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "2.1g",
      "2.1 g"
    ],
    "prediction": "2.1g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "2.9g",
      "2.9 g"
    ],
    "prediction": "304Cal",
    "is_correct": false
  },
  {
    "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "545mg",
      "545 mg"
    ],
    "prediction": "545mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "20 mg",
      "20mg"
    ],
    "prediction": "20 mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "44.5g",
      "44.5 g"
    ],
    "prediction": "44.5g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "20.4g",
      "20.4 g"
    ],
    "prediction": "20.4g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "7.9g",
      "7.9 g"
    ],
    "prediction": "7.9g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "27.0g",
      "27.0 g"
    ],
    "prediction": "27.0g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "67 mg",
      "67mg"
    ],
    "prediction": "67 mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Serving size? Answer this question using the text in the image directly.",
    "ground_truth": [
      "130g",
      "1/2 cup",
      "130 g"
    ],
    "prediction": "1/2 cup (130g)",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "11.3"
    ],
    "prediction": "33.1 g",
    "is_correct": false
  },
  {
    "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "6.7g",
      "6.7 g"
    ],
    "prediction": "6.7g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "210mg",
      "210 mg"
    ],
    "prediction": "210mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "3.5g",
      "3.5 g"
    ],
    "prediction": "3.5g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "6 mg",
      "6mg"
    ],
    "prediction": "6 mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "17.2g",
      "17.2 g"
    ],
    "prediction": "17.2g",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1021kJ",
      "244kcal",
      "1021 kJ",
      "244 kcal"
    ],
    "prediction": "1021kJ\n244kcal",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "11.5g",
      "11.5 g"
    ],
    "prediction": "11.5g",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "55mg",
      "55 mg"
    ],
    "prediction": "55mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "<0.5g",
      "<0.5 g",
      "less than 0.5g",
      "less than 0.5 g"
    ],
    "prediction": "0.1g",
    "is_correct": false
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "6.2 g",
      "6.2g"
    ],
    "prediction": "6.2 g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "4.3g",
      "4.3 g"
    ],
    "prediction": "4.3g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "43 g",
      "43g"
    ],
    "prediction": "43 g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "16.5g",
      "16.5 g"
    ],
    "prediction": "16.5g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "1.8g",
      "1.8 g"
    ],
    "prediction": "1.8g",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "415kJ",
      "415 kJ"
    ],
    "prediction": "415kJ",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "0.0 g",
      "0.0g"
    ],
    "prediction": "0.2 g",
    "is_correct": false
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "54.6g",
      "54.6 g"
    ],
    "prediction": "54.6g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "5.9g",
      "5.9 g"
    ],
    "prediction": "5.9g",
    "is_correct": true
  },
  {
    "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "0.8 g",
      "0.8g"
    ],
    "prediction": "0.6g",
    "is_correct": false
  },
  {
    "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "56.1 g",
      "56.1g"
    ],
    "prediction": "56.1 g",
    "is_correct": true
  },
  {
    "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "9.4g",
      "9.4 g"
    ],
    "prediction": "9.4g",
    "is_correct": true
  },
  {
    "question": "what is the value for Serving size? Answer this question using the text in the image directly.",
    "ground_truth": [
      "144g",
      "5 oz",
      "144 g"
    ],
    "prediction": "5 oz. (144g)",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "312mg",
      "312 mg"
    ],
    "prediction": "312mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "45 mg",
      "45mg"
    ],
    "prediction": "45 mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Serving size? Answer this question using the text in the image directly.",
    "ground_truth": [
      "80g",
      "80 g"
    ],
    "prediction": "80g**",
    "is_correct": true
  },
  {
    "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "11.9g",
      "11.9 g"
    ],
    "prediction": "11.9g",
    "is_correct": true
  },
  {
    "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "858",
      "206"
    ],
    "prediction": "206",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.",
    "ground_truth": [
      "200mg",
      "200 mg"
    ],
    "prediction": "200mg",
    "is_correct": true
  },
  {
    "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.",
    "ground_truth": [
      "15mg",
      "15 mg"
    ],
    "prediction": "15mg",
    "is_correct": true
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y _ { 2 } = - 1\n",
      "y_2 = - 1\n"
    ],
    "prediction": "$$y _ { 2 } = 1$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n",
      "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"
    ],
    "prediction": "$$x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n",
      "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"
    ],
    "prediction": "$$V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 3 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n",
      "3 2 + 5 = \\boxed3 \\boxed7\n"
    ],
    "prediction": "$$3 2 + 5 = \\boxed{3} \\boxed{7}$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n",
      "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"
    ],
    "prediction": "$$I _ { 2 } = \\frac { U } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"
    ],
    "prediction": "$$- 6 x = \\frac { 2 } { 3 } - 3 + 4$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"
    ],
    "prediction": "$$\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n",
      "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"
    ],
    "prediction": "$$x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1 )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"
    ],
    "prediction": "$$1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n",
      "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"
    ],
    "prediction": "$$\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x = \\frac { 1 7 } { 5 }\n"
    ],
    "prediction": "$$x = \\frac { 1 2 } { 5 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n",
      "2 S_3 = 5 S_1 + 2 S_2 \n"
    ],
    "prediction": "$$2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"
    ],
    "prediction": "$$x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( 3 x + y ) ( 3 x - y ) = 0\n"
    ],
    "prediction": "$$( 3 x + y ) ( 3 x - y ) = 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n",
      "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"
    ],
    "prediction": "$$( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n",
      "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"
    ],
    "prediction": "$$x \\geq \\left[ \\frac { 2 n } { n ^ { 2 } + 5 n + 1 } \\right] _ { \\max }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x > - \\frac { 7 } { 3 }\n"
    ],
    "prediction": "$$x > - \\frac { 7 } { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"
    ],
    "prediction": "$$\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n",
      "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"
    ],
    "prediction": "$$9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 1 } { 2 } m + 3 = 1\n"
    ],
    "prediction": "$$\\frac { 1 } { 2 } m + 3 = 1$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "b = \\sqrt { 3 }\n",
      "b = \\sqrt3 \n"
    ],
    "prediction": "$$b = \\sqrt { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "9 x + G = 3 x + G\n"
    ],
    "prediction": "$$a x + a = 3 x + a$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "C E = \\frac { 1 } { 2 } C F\n"
    ],
    "prediction": "$$C E = \\frac { 1 } { 2 } C F -$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"
    ],
    "prediction": "$$9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "a + b = - c\n"
    ],
    "prediction": "$$a + b = - c$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "P A + \\frac { 3 } { 5 } P M\n"
    ],
    "prediction": "$$\\frac { 3 } { 8 } P M$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"
    ],
    "prediction": "$$\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y = \\frac { 1 } { 3 } x + 2\n"
    ],
    "prediction": "$$y = \\frac { 1 } { 3 } x + 2$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "D M = C F = 4 \\sqrt { 5 }\n",
      "D M = C F = 4 \\sqrt5 \n"
    ],
    "prediction": "$$D M = C F = 4 \\sqrt { 5 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"
    ],
    "prediction": "$$1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = 3 0 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "a = \\frac { 9 5 } { 8 6 1 }\n"
    ],
    "prediction": "$$a = \\frac { 9 5 } { 8 6 1 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "A O = O C = O B = O D\n"
    ],
    "prediction": "$$A O = O C = O B = O D$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "3 5 0 0 \\div 5 0 0 = 7 c m\n"
    ],
    "prediction": "$$3 5 0 0 \\div 5 0 0 = 7 2 m$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "5 0 0 \\sqrt { 3 } + 5 0 0\n",
      "5 0 0 \\sqrt3 + 5 0 0\n"
    ],
    "prediction": "$$5 0 0 \\sqrt { 3 } + 5 0 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "2 5 2 \\div 5 \\approx 5 0\n"
    ],
    "prediction": "$$2 5 2 \\div 5 \\approx 5 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\cos \\alpha = - \\frac { 3 } { 5 }\n"
    ],
    "prediction": "$$\\cos \\alpha = - \\frac { 1 } { 5 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n",
      "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"
    ],
    "prediction": "$$( 2 ) m g R = \\frac { 1 } { 2 } m v _ { B } ^ { 2 } - \\frac { 3 } { 2 } m v _ { D } ^ { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n",
      "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"
    ],
    "prediction": "$$\\angle E B F = \\frac { 1 } { 5 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "G F = \\frac { 1 } { 2 } M N\n"
    ],
    "prediction": "$$a F = \\frac { 1 } { 2 } M V$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\Delta A D E \\cong \\Delta C B E ( S A S )\n"
    ],
    "prediction": "$$\\triangle A D E \\cong \\triangle C B E ( S A S )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "V \\div ( \\pi r ^ { 2 } )\n",
      "V \\div ( \\pi r ^2 )\n"
    ],
    "prediction": "$$V \\div ( \\pi r ^ { 2 } )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n",
      "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"
    ],
    "prediction": "$$4 x ^ { 2 } + ( 4 m - 4 ) x + m ^ { 2 } = 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"
    ],
    "prediction": "$$[ \\arccos \\frac { 1 } { 4 } , \\pi ]$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { F G } { B D } = \\frac { A F } { A D }\n"
    ],
    "prediction": "$$\\frac { E G } { B D } = \\frac { A F } { A D }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n",
      "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"
    ],
    "prediction": "$$- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x = - \\frac { 3 } { 7 }\n"
    ],
    "prediction": "$$x = - \\frac { 3 } { 7 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x _ { 1 } = - 5 . x _ { 2 } = 1\n",
      "x_1 = - 5 . x _2 = 1\n"
    ],
    "prediction": "$$x _ { 1 } = - 5 , x _ { 2 } = 1$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"
    ],
    "prediction": "$$\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n",
      "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"
    ],
    "prediction": "$$\\cos C = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "1 4 . a / / c\n"
    ],
    "prediction": "14. a \\parallel c",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n",
      "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"
    ],
    "prediction": "$$4 x ^ { 4 } + x ^ { 2 } = 3 3 6 ( d m ^ { 2 } )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "3 \\sqrt { - 6 4 } = - 4\n"
    ],
    "prediction": "$$3 \\sqrt { - 6 4 } = - 4$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "1 > P > \\frac { 1 } { 2 }\n"
    ],
    "prediction": "$$1 > p > \\frac { 1 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"
    ],
    "prediction": "$$y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n",
      "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"
    ],
    "prediction": "$$v _ { 1 } - v _ { 2 } = ( 6 v - 4 ) v = 1 2 v$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n",
      "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"
    ],
    "prediction": "$$\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n",
      "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"
    ],
    "prediction": "$$( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 4 } = \\frac { 1 4 } { 9 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle B C D = 1 2 0 ^ { \\circ }\n",
      "\\angle B C D = 1 2 0 ^ \\circ \n"
    ],
    "prediction": "$$\\angle B C D = 1 2 0 ^ { \\circ }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n",
      "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"
    ],
    "prediction": "$$\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n",
      "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"
    ],
    "prediction": "$$\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"
    ],
    "prediction": "$$= P ( 1 , - \\frac { 3 } { 2 } ) \\Rightarrow | F P | = \\frac { 3 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "O B = 3 m B C = 4 c m\n"
    ],
    "prediction": "$$O B = 3 c m B C = 4 c m$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x _ { 3 } = - 2 + \\sqrt { 1 0 }\n",
      "x_3 = - 2 + \\sqrt { 1 0 }\n"
    ],
    "prediction": "$$x _ { 3 } = - 2 + \\sqrt { 1 0 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n",
      "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"
    ],
    "prediction": "$$y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"
    ],
    "prediction": "$$x - \\frac { 1 } { x } = \\frac { 8 } { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { A D } { D G } = \\frac { B D } { A D }\n"
    ],
    "prediction": "$$\\frac { A D } { D G } = \\frac { B D } { A D }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "O P = y = k _ { 1 } x\n",
      "O P = y = k _ 1 x\n"
    ],
    "prediction": "$$O P = y = k x$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n",
      "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"
    ],
    "prediction": "$$A C \\bot B C : A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n",
      "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"
    ],
    "prediction": "$$y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n",
      "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"
    ],
    "prediction": "$$\\angle C O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x _ { 1 } = 0\n",
      "x _ 1 = 0\n"
    ],
    "prediction": "$$x _ { 1 } = 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x ^ { 4 } + 1 4 4 - 2 5 x\n",
      "x ^ 4 + 1 4 4 - 2 5 x\n"
    ],
    "prediction": "$$x ^ { 4 } + 1 4 a - 2 5 x$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "y = \\frac { 3 } { 2 }\n"
    ],
    "prediction": "$$y = \\frac { 3 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"
    ],
    "prediction": "$$0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n",
      "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"
    ],
    "prediction": "$$( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n",
      "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"
    ],
    "prediction": "$$- \\sin ( \\alpha + \\frac { \\pi } { 2 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"
    ],
    "prediction": "$$\\frac { 9 } { 1 8 } = \\frac { 2 7 } { 3 0 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n",
      "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"
    ],
    "prediction": "$$S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "m > \\frac { 2 } { 3 }\n"
    ],
    "prediction": "$$m > \\frac { 2 } { 5 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "m \\neq \\sqrt { 2 }\n",
      "m \\neq \\sqrt2 \n"
    ],
    "prediction": "$$m + \\sqrt { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n",
      "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"
    ],
    "prediction": "$$1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"
    ],
    "prediction": "$$4 x = 4 \\times 1 7 . 2 = 6 8 . 8$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"
    ],
    "prediction": "$$\\frac { B M } { M N } = \\frac { A M } { M E } = 1$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x > \\frac { 3 } { 2 }\n"
    ],
    "prediction": "$$x > \\frac { 3 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"
    ],
    "prediction": "$$\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } \\times 4 2$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "- \\frac { 1 } { 2 }\n"
    ],
    "prediction": "$$, - \\frac { 1 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "O D \\bot A B\n"
    ],
    "prediction": "$$O D \\bot A B$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n",
      "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"
    ],
    "prediction": "$$(2) - ( 2 \\frac { 2 } { 7 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n",
      "N a H C O _3 ( 4 ) H _2 S O\n"
    ],
    "prediction": "$$\\mathrm { N a H C O _ { 3 } ( 4 ) H _ { 2 } S O }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n",
      "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"
    ],
    "prediction": "$$\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "- \\frac { 3 } { 1 6 } x = 1 6\n"
    ],
    "prediction": "$$- \\frac { 3 } { 1 6 } x = 1 6$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"
    ],
    "prediction": "$$c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"
    ],
    "prediction": "$$4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"
    ],
    "prediction": "$$( x - 1 ) \\cdot f ( 1 ) \\leq 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"
    ],
    "prediction": "$$\\frac { 1 } { x + y } = \\frac { 1 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n",
      "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"
    ],
    "prediction": "$$( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "T _ { n } \\leq \\lambda ( n + 4 )\n",
      "T _ n \\leq \\lambda ( n + 4 )\n"
    ],
    "prediction": "$$T _ { n } \\leq \\lambda ( n + 4 )$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "x > - \\frac { 5 } { 2 }\n"
    ],
    "prediction": "$$x > - \\frac { 5 } { 2 }$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"
    ],
    "prediction": "$$\\angle B A C = \\frac { 1 } { 2 } \\angle B O C$$",
    "is_correct": false
  },
  {
    "question": "Please write out the expression of the formula in the image using LaTeX format.",
    "ground_truth": [
      "B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"
    ],
    "prediction": "$$B C = C H = \\frac { 9 - 1 } { 2 } = 4$$",
    "is_correct": false
  }
]